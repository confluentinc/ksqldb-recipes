{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ksqlDB Recipes This is a Work in Progress! Click in the left nav bar to see one of the recipes. Note: these recipes will be launched on Confluent Developer https://developer.confluent.io.","title":"ksqlDB Recipes"},{"location":"#welcome-to-ksqldb-recipes","text":"This is a Work in Progress! Click in the left nav bar to see one of the recipes. Note: these recipes will be launched on Confluent Developer https://developer.confluent.io.","title":"Welcome to ksqlDB Recipes"},{"location":"customer-360/clickstream/","text":"Clickstream Data Analysis What is it? Analyzing clickstream data enables businesses to understand the behavior of its online users, for example: User activity over a given time frame: how many webpages are users viewing Requests that end in error, over a given threshold (e.g., 404 HTTP codes) Where the requests are coming from geographically in a given window of time How long users are interacting with the site (user sessions) Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data. Read the data in Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. The recipes are shown with some specific connector types and configuration, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment, see documentation for details. This recipe creates simulated data with the Datagen connector. -- Stream of users CREATE SOURCE CONNECTOR datagen_clickstream_users WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_users', 'quickstart' = 'clickstream_users', 'maxInterval' = '10', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of per-user session information CREATE SOURCE CONNECTOR datagen_clickstream WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream', 'quickstart' = 'clickstream', 'maxInterval' = '30', 'format' = 'json'); Optional: to simulate a real-world scenario where user sessions aren't just always open but do close after some time, you can pause and resume the DATAGEN_CLICKSTREAM connector. Run stream processing app Now you can process the data in a variety of ways, by enriching the clickstream data with user information, analyze errors, aggregate data into windows of time, etc. --------------------------------------------------------------------------------------------------- -- Register sources: --------------------------------------------------------------------------------------------------- -- stream of user clicks: CREATE STREAM clickstream ( _time bigint, time varchar, ip varchar, request varchar, status int, userid int, bytes bigint, agent varchar ) with ( kafka_topic = 'clickstream', value_format = 'json' ); -- users lookup table: CREATE TABLE WEB_USERS ( user_id int primary key, registered_At BIGINT, username varchar, first_name varchar, last_name varchar, city varchar, level varchar ) with ( kafka_topic = 'clickstream_users', value_format = 'json' ); --------------------------------------------------------------------------------------------------- -- Build materialized stream views: --------------------------------------------------------------------------------------------------- -- enrich click-stream with more user information: CREATE STREAM USER_CLICKSTREAM AS SELECT userid, u.username, ip, u.city, request, status, bytes FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id; --------------------------------------------------------------------------------------------------- -- Build materialized table views: --------------------------------------------------------------------------------------------------- -- Per-userId tables ----------------------------------------------------------------------------- -- Table of events per minute for each user: CREATE table events_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS events FROM clickstream window TUMBLING (size 60 second) GROUP BY userid; -- Table of html pages per minute for each user: CREATE TABLE pages_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS pages FROM clickstream WINDOW HOPPING (size 60 second, advance by 5 second) WHERE request like '%html%' GROUP BY userid; -- Per-username tables ---------------------------------------------------------------------------- -- User sessions table - 30 seconds of inactivity expires the session -- Table counts number of events within the session CREATE TABLE CLICK_USER_SESSIONS AS SELECT username as K, AS_VALUE(username) as username, WINDOWEND as EVENT_TS, count(*) AS events FROM USER_CLICKSTREAM window SESSION (30 second) GROUP BY username; -- Per-status tables ------------------------------------------------------------------------------ -- number of errors per min, using 'HAVING' Filter to show ERROR codes > 400 where count > 5 CREATE TABLE ERRORS_PER_MIN_ALERT AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 20 second) WHERE status > 400 GROUP BY status HAVING count(*) > 5 AND count(*) is not NULL; -- Enriched user details table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE USER_IP_ACTIVITY WITH (key_format='json') AS SELECT username as k1, ip as k2, city as k3, AS_VALUE(username) as username, WINDOWSTART as EVENT_TS, AS_VALUE(ip) as ip, AS_VALUE(city) as city, COUNT(*) AS count FROM USER_CLICKSTREAM WINDOW TUMBLING (size 60 second) GROUP BY username, ip, city HAVING COUNT(*) > 1; Write the data out After processing the data, send it to Elasticsearch. -- Send data to Elasticsearch CREATE SINK CONNECTOR analyzed_clickstream WITH ( 'connector.class' = 'ElasticsearchSink', 'name' = 'recipe-elasticsearch-analyzed_clickstream', 'input.data.format' = 'JSON', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'USER_IP_ACTIVITY, ENRICHED_ERROR_CODES_COUNT', 'connection.url' = '<elasticsearch-URI>', 'connection.user' = '<elasticsearch-username>', 'connection.password' = '<elasticsearch-password>', 'type.name' = 'type.name=kafkaconnect', 'key.ignore' = 'true', 'schema.ignore' = 'true' ); Full ksqlDB Statements Here is a complete view of all the recipe steps explained above. -- Stream of users CREATE SOURCE CONNECTOR datagen_clickstream_users WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_users', 'quickstart' = 'clickstream_users', 'maxInterval' = '10', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of per-user session information CREATE SOURCE CONNECTOR datagen_clickstream WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream', 'quickstart' = 'clickstream', 'maxInterval' = '30', 'format' = 'json'); --------------------------------------------------------------------------------------------------- -- Register sources: --------------------------------------------------------------------------------------------------- -- stream of user clicks: CREATE STREAM clickstream ( _time bigint, time varchar, ip varchar, request varchar, status int, userid int, bytes bigint, agent varchar ) with ( kafka_topic = 'clickstream', value_format = 'json' ); -- users lookup table: CREATE TABLE WEB_USERS ( user_id int primary key, registered_At BIGINT, username varchar, first_name varchar, last_name varchar, city varchar, level varchar ) with ( kafka_topic = 'clickstream_users', value_format = 'json' ); --------------------------------------------------------------------------------------------------- -- Build materialized stream views: --------------------------------------------------------------------------------------------------- -- enrich click-stream with more user information: CREATE STREAM USER_CLICKSTREAM AS SELECT userid, u.username, ip, u.city, request, status, bytes FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id; --------------------------------------------------------------------------------------------------- -- Build materialized table views: --------------------------------------------------------------------------------------------------- -- Per-userId tables ----------------------------------------------------------------------------- -- Table of events per minute for each user: CREATE table events_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS events FROM clickstream window TUMBLING (size 60 second) GROUP BY userid; -- Table of html pages per minute for each user: CREATE TABLE pages_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS pages FROM clickstream WINDOW HOPPING (size 60 second, advance by 5 second) WHERE request like '%html%' GROUP BY userid; -- Per-username tables ---------------------------------------------------------------------------- -- User sessions table - 30 seconds of inactivity expires the session -- Table counts number of events within the session CREATE TABLE CLICK_USER_SESSIONS AS SELECT username as K, AS_VALUE(username) as username, WINDOWEND as EVENT_TS, count(*) AS events FROM USER_CLICKSTREAM window SESSION (30 second) GROUP BY username; -- Per-status tables ------------------------------------------------------------------------------ -- number of errors per min, using 'HAVING' Filter to show ERROR codes > 400 where count > 5 CREATE TABLE ERRORS_PER_MIN_ALERT AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 20 second) WHERE status > 400 GROUP BY status HAVING count(*) > 5 AND count(*) is not NULL; -- Enriched user details table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE USER_IP_ACTIVITY WITH (key_format='json') AS SELECT username as k1, ip as k2, city as k3, AS_VALUE(username) as username, WINDOWSTART as EVENT_TS, AS_VALUE(ip) as ip, AS_VALUE(city) as city, COUNT(*) AS count FROM USER_CLICKSTREAM WINDOW TUMBLING (size 60 second) GROUP BY username, ip, city HAVING COUNT(*) > 1; -- Send data to Elasticsearch CREATE SINK CONNECTOR analyzed_clickstream WITH ( 'connector.class' = 'ElasticsearchSink', 'name' = 'recipe-elasticsearch-analyzed_clickstream', 'input.data.format' = 'JSON', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'USER_IP_ACTIVITY, ENRICHED_ERROR_CODES_COUNT', 'connection.url' = '<elasticsearch-URI>', 'connection.user' = '<elasticsearch-username>', 'connection.password' = '<elasticsearch-password>', 'type.name' = 'type.name=kafkaconnect', 'key.ignore' = 'true', 'schema.ignore' = 'true' );","title":"Clickstream Data Analysis"},{"location":"customer-360/clickstream/#clickstream-data-analysis","text":"","title":"Clickstream Data Analysis"},{"location":"customer-360/clickstream/#what-is-it","text":"Analyzing clickstream data enables businesses to understand the behavior of its online users, for example: User activity over a given time frame: how many webpages are users viewing Requests that end in error, over a given threshold (e.g., 404 HTTP codes) Where the requests are coming from geographically in a given window of time How long users are interacting with the site (user sessions)","title":"What is it?"},{"location":"customer-360/clickstream/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"customer-360/clickstream/#step-by-step","text":"","title":"Step-by-step"},{"location":"customer-360/clickstream/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data.","title":"Setup your Environment"},{"location":"customer-360/clickstream/#read-the-data-in","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. The recipes are shown with some specific connector types and configuration, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment, see documentation for details. This recipe creates simulated data with the Datagen connector. -- Stream of users CREATE SOURCE CONNECTOR datagen_clickstream_users WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_users', 'quickstart' = 'clickstream_users', 'maxInterval' = '10', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of per-user session information CREATE SOURCE CONNECTOR datagen_clickstream WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream', 'quickstart' = 'clickstream', 'maxInterval' = '30', 'format' = 'json'); Optional: to simulate a real-world scenario where user sessions aren't just always open but do close after some time, you can pause and resume the DATAGEN_CLICKSTREAM connector.","title":"Read the data in"},{"location":"customer-360/clickstream/#run-stream-processing-app","text":"Now you can process the data in a variety of ways, by enriching the clickstream data with user information, analyze errors, aggregate data into windows of time, etc. --------------------------------------------------------------------------------------------------- -- Register sources: --------------------------------------------------------------------------------------------------- -- stream of user clicks: CREATE STREAM clickstream ( _time bigint, time varchar, ip varchar, request varchar, status int, userid int, bytes bigint, agent varchar ) with ( kafka_topic = 'clickstream', value_format = 'json' ); -- users lookup table: CREATE TABLE WEB_USERS ( user_id int primary key, registered_At BIGINT, username varchar, first_name varchar, last_name varchar, city varchar, level varchar ) with ( kafka_topic = 'clickstream_users', value_format = 'json' ); --------------------------------------------------------------------------------------------------- -- Build materialized stream views: --------------------------------------------------------------------------------------------------- -- enrich click-stream with more user information: CREATE STREAM USER_CLICKSTREAM AS SELECT userid, u.username, ip, u.city, request, status, bytes FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id; --------------------------------------------------------------------------------------------------- -- Build materialized table views: --------------------------------------------------------------------------------------------------- -- Per-userId tables ----------------------------------------------------------------------------- -- Table of events per minute for each user: CREATE table events_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS events FROM clickstream window TUMBLING (size 60 second) GROUP BY userid; -- Table of html pages per minute for each user: CREATE TABLE pages_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS pages FROM clickstream WINDOW HOPPING (size 60 second, advance by 5 second) WHERE request like '%html%' GROUP BY userid; -- Per-username tables ---------------------------------------------------------------------------- -- User sessions table - 30 seconds of inactivity expires the session -- Table counts number of events within the session CREATE TABLE CLICK_USER_SESSIONS AS SELECT username as K, AS_VALUE(username) as username, WINDOWEND as EVENT_TS, count(*) AS events FROM USER_CLICKSTREAM window SESSION (30 second) GROUP BY username; -- Per-status tables ------------------------------------------------------------------------------ -- number of errors per min, using 'HAVING' Filter to show ERROR codes > 400 where count > 5 CREATE TABLE ERRORS_PER_MIN_ALERT AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 20 second) WHERE status > 400 GROUP BY status HAVING count(*) > 5 AND count(*) is not NULL; -- Enriched user details table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE USER_IP_ACTIVITY WITH (key_format='json') AS SELECT username as k1, ip as k2, city as k3, AS_VALUE(username) as username, WINDOWSTART as EVENT_TS, AS_VALUE(ip) as ip, AS_VALUE(city) as city, COUNT(*) AS count FROM USER_CLICKSTREAM WINDOW TUMBLING (size 60 second) GROUP BY username, ip, city HAVING COUNT(*) > 1;","title":"Run stream processing app"},{"location":"customer-360/clickstream/#write-the-data-out","text":"After processing the data, send it to Elasticsearch. -- Send data to Elasticsearch CREATE SINK CONNECTOR analyzed_clickstream WITH ( 'connector.class' = 'ElasticsearchSink', 'name' = 'recipe-elasticsearch-analyzed_clickstream', 'input.data.format' = 'JSON', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'USER_IP_ACTIVITY, ENRICHED_ERROR_CODES_COUNT', 'connection.url' = '<elasticsearch-URI>', 'connection.user' = '<elasticsearch-username>', 'connection.password' = '<elasticsearch-password>', 'type.name' = 'type.name=kafkaconnect', 'key.ignore' = 'true', 'schema.ignore' = 'true' );","title":"Write the data out"},{"location":"customer-360/clickstream/#full-ksqldb-statements","text":"Here is a complete view of all the recipe steps explained above. -- Stream of users CREATE SOURCE CONNECTOR datagen_clickstream_users WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_users', 'quickstart' = 'clickstream_users', 'maxInterval' = '10', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of per-user session information CREATE SOURCE CONNECTOR datagen_clickstream WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream', 'quickstart' = 'clickstream', 'maxInterval' = '30', 'format' = 'json'); --------------------------------------------------------------------------------------------------- -- Register sources: --------------------------------------------------------------------------------------------------- -- stream of user clicks: CREATE STREAM clickstream ( _time bigint, time varchar, ip varchar, request varchar, status int, userid int, bytes bigint, agent varchar ) with ( kafka_topic = 'clickstream', value_format = 'json' ); -- users lookup table: CREATE TABLE WEB_USERS ( user_id int primary key, registered_At BIGINT, username varchar, first_name varchar, last_name varchar, city varchar, level varchar ) with ( kafka_topic = 'clickstream_users', value_format = 'json' ); --------------------------------------------------------------------------------------------------- -- Build materialized stream views: --------------------------------------------------------------------------------------------------- -- enrich click-stream with more user information: CREATE STREAM USER_CLICKSTREAM AS SELECT userid, u.username, ip, u.city, request, status, bytes FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id; --------------------------------------------------------------------------------------------------- -- Build materialized table views: --------------------------------------------------------------------------------------------------- -- Per-userId tables ----------------------------------------------------------------------------- -- Table of events per minute for each user: CREATE table events_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS events FROM clickstream window TUMBLING (size 60 second) GROUP BY userid; -- Table of html pages per minute for each user: CREATE TABLE pages_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS pages FROM clickstream WINDOW HOPPING (size 60 second, advance by 5 second) WHERE request like '%html%' GROUP BY userid; -- Per-username tables ---------------------------------------------------------------------------- -- User sessions table - 30 seconds of inactivity expires the session -- Table counts number of events within the session CREATE TABLE CLICK_USER_SESSIONS AS SELECT username as K, AS_VALUE(username) as username, WINDOWEND as EVENT_TS, count(*) AS events FROM USER_CLICKSTREAM window SESSION (30 second) GROUP BY username; -- Per-status tables ------------------------------------------------------------------------------ -- number of errors per min, using 'HAVING' Filter to show ERROR codes > 400 where count > 5 CREATE TABLE ERRORS_PER_MIN_ALERT AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 20 second) WHERE status > 400 GROUP BY status HAVING count(*) > 5 AND count(*) is not NULL; -- Enriched user details table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE USER_IP_ACTIVITY WITH (key_format='json') AS SELECT username as k1, ip as k2, city as k3, AS_VALUE(username) as username, WINDOWSTART as EVENT_TS, AS_VALUE(ip) as ip, AS_VALUE(city) as city, COUNT(*) AS count FROM USER_CLICKSTREAM WINDOW TUMBLING (size 60 second) GROUP BY username, ip, city HAVING COUNT(*) > 1; -- Send data to Elasticsearch CREATE SINK CONNECTOR analyzed_clickstream WITH ( 'connector.class' = 'ElasticsearchSink', 'name' = 'recipe-elasticsearch-analyzed_clickstream', 'input.data.format' = 'JSON', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'USER_IP_ACTIVITY, ENRICHED_ERROR_CODES_COUNT', 'connection.url' = '<elasticsearch-URI>', 'connection.user' = '<elasticsearch-username>', 'connection.password' = '<elasticsearch-password>', 'type.name' = 'type.name=kafkaconnect', 'key.ignore' = 'true', 'schema.ignore' = 'true' );","title":"Full ksqlDB Statements"},{"location":"fin-serv/credit-card-activity/","text":"Detecting Unusual Credit Card Activity What is it? In banking, fraud can involve using stolen credit cards, forging checks, misleading accounting practices, etc. This recipe analyzes total credit card spend, and if it's more than the average credit card usage of a customer, the account will be flagged as a possible case of credit card theft. Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data. Read the data in Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. The recipes are shown with some specific connector types and configuration, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment, see documentation for details. -- Stream of transactions CREATE SOURCE CONNECTOR transactions WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-transactions', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'TRANSACTIONS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! You can manually insert values into the source streams in the next section. Run stream processing app Now you can process the data in a variety of ways. -- Register the stream of transactions CREATE STREAM TRANSACTIONS_RAW (ACCOUNT_ID VARCHAR, TIMESTAMP VARCHAR, CARD_TYPE VARCHAR, AMOUNT DOUBLE, IP_ADDRESS VARCHAR, TRANSACTION_ID VARCHAR) WITH (KAFKA_TOPIC='transactions', VALUE_FORMAT='JSON'); -- Repartition the stream on account_id in order to ensure that all the streams and tables are co-partitioned, which means that input records on both sides of the join have the same configuration settings for partitions. CREATE STREAM TRANSACTIONS_SOURCE WITH (VALUE_FORMAT='AVRO') AS SELECT * FROM TRANSACTIONS_RAW PARTITION BY ACCOUNT_ID; -- Register the existing stream of customer data CREATE STREAM CUST_RAW_STREAM (ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, AVG_CREDIT_SPEND DOUBLE) WITH (KAFKA_TOPIC='customers', VALUE_FORMAT='JSON'); -- Repartition the customer data stream by account_id to prepare for the join CREATE STREAM CUSTOMER_REKEYED WITH (VALUE_FORMAT='AVRO') AS SELECT * FROM CUST_RAW_STREAM PARTITION BY ID; -- Register the partitioned customer data topic as a table used for the join with the incoming stream of transactions: CREATE TABLE customer WITH (KAFKA_TOPIC='CUSTOMER_REKEYED', VALUE_FORMAT='AVRO', KEY='ID'); -- Join the transactions to customer information: CREATE STREAM TRANSACTIONS_ENRICHED AS SELECT T.ACCOUNT_ID, T.CARD_TYPE, T.AMOUNT, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME, C.AVG_CREDIT_SPEND FROM TRANSACTIONS_SOURCE T INNER JOIN CUSTOMER C ON T.ACCOUNT_ID = C.ID; -- Aggregate the stream of transactions for each account ID using a two-hour tumbling window, and filter for accounts in which the total spend in a two-hour period is greater than the customer\u2019s average: CREATE TABLE POSSIBLE_STOLEN_CARD AS SELECT TIMESTAMPTOSTRING(WindowStart(), 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_START, T.ACCOUNT_ID, T.CARD_TYPE, SUM(T.AMOUNT) AS TOTAL_CREDIT_SPEND, T.FULL_NAME, MAX(T.AVG_CREDIT_SPEND) AS AVG_CREDIT_SPEND FROM TRANSACTIONS_ENRICHED T WINDOW TUMBLING (SIZE 2 HOURS) GROUP BY T.ACCOUNT_ID, T.CARD_TYPE, T.FULL_NAME HAVING SUM(T.AMOUNT) > MAX(T.AVG_CREDIT_SPEND) ; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO ...TODO... Full ksqlDB Statements Here is a complete view of all the recipe steps explained above. -- Stream of transactions CREATE SOURCE CONNECTOR transactions WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-transactions', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'TRANSACTIONS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Register the stream of transactions CREATE STREAM TRANSACTIONS_RAW (ACCOUNT_ID VARCHAR, TIMESTAMP VARCHAR, CARD_TYPE VARCHAR, AMOUNT DOUBLE, IP_ADDRESS VARCHAR, TRANSACTION_ID VARCHAR) WITH (KAFKA_TOPIC='transactions', VALUE_FORMAT='JSON'); -- Repartition the stream on account_id in order to ensure that all the streams and tables are co-partitioned, which means that input records on both sides of the join have the same configuration settings for partitions. CREATE STREAM TRANSACTIONS_SOURCE WITH (VALUE_FORMAT='AVRO') AS SELECT * FROM TRANSACTIONS_RAW PARTITION BY ACCOUNT_ID; -- Register the existing stream of customer data CREATE STREAM CUST_RAW_STREAM (ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, AVG_CREDIT_SPEND DOUBLE) WITH (KAFKA_TOPIC='customers', VALUE_FORMAT='JSON'); -- Repartition the customer data stream by account_id to prepare for the join CREATE STREAM CUSTOMER_REKEYED WITH (VALUE_FORMAT='AVRO') AS SELECT * FROM CUST_RAW_STREAM PARTITION BY ID; -- Register the partitioned customer data topic as a table used for the join with the incoming stream of transactions: CREATE TABLE customer WITH (KAFKA_TOPIC='CUSTOMER_REKEYED', VALUE_FORMAT='AVRO', KEY='ID'); -- Join the transactions to customer information: CREATE STREAM TRANSACTIONS_ENRICHED AS SELECT T.ACCOUNT_ID, T.CARD_TYPE, T.AMOUNT, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME, C.AVG_CREDIT_SPEND FROM TRANSACTIONS_SOURCE T INNER JOIN CUSTOMER C ON T.ACCOUNT_ID = C.ID; -- Aggregate the stream of transactions for each account ID using a two-hour tumbling window, and filter for accounts in which the total spend in a two-hour period is greater than the customer\u2019s average: CREATE TABLE POSSIBLE_STOLEN_CARD AS SELECT TIMESTAMPTOSTRING(WindowStart(), 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_START, T.ACCOUNT_ID, T.CARD_TYPE, SUM(T.AMOUNT) AS TOTAL_CREDIT_SPEND, T.FULL_NAME, MAX(T.AVG_CREDIT_SPEND) AS AVG_CREDIT_SPEND FROM TRANSACTIONS_ENRICHED T WINDOW TUMBLING (SIZE 2 HOURS) GROUP BY T.ACCOUNT_ID, T.CARD_TYPE, T.FULL_NAME HAVING SUM(T.AMOUNT) > MAX(T.AVG_CREDIT_SPEND) ;","title":"Detecting Unusual Credit Card Activity"},{"location":"fin-serv/credit-card-activity/#detecting-unusual-credit-card-activity","text":"","title":"Detecting Unusual Credit Card Activity"},{"location":"fin-serv/credit-card-activity/#what-is-it","text":"In banking, fraud can involve using stolen credit cards, forging checks, misleading accounting practices, etc. This recipe analyzes total credit card spend, and if it's more than the average credit card usage of a customer, the account will be flagged as a possible case of credit card theft.","title":"What is it?"},{"location":"fin-serv/credit-card-activity/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"fin-serv/credit-card-activity/#step-by-step","text":"","title":"Step-by-step"},{"location":"fin-serv/credit-card-activity/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data.","title":"Setup your Environment"},{"location":"fin-serv/credit-card-activity/#read-the-data-in","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. The recipes are shown with some specific connector types and configuration, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment, see documentation for details. -- Stream of transactions CREATE SOURCE CONNECTOR transactions WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-transactions', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'TRANSACTIONS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! You can manually insert values into the source streams in the next section.","title":"Read the data in"},{"location":"fin-serv/credit-card-activity/#run-stream-processing-app","text":"Now you can process the data in a variety of ways. -- Register the stream of transactions CREATE STREAM TRANSACTIONS_RAW (ACCOUNT_ID VARCHAR, TIMESTAMP VARCHAR, CARD_TYPE VARCHAR, AMOUNT DOUBLE, IP_ADDRESS VARCHAR, TRANSACTION_ID VARCHAR) WITH (KAFKA_TOPIC='transactions', VALUE_FORMAT='JSON'); -- Repartition the stream on account_id in order to ensure that all the streams and tables are co-partitioned, which means that input records on both sides of the join have the same configuration settings for partitions. CREATE STREAM TRANSACTIONS_SOURCE WITH (VALUE_FORMAT='AVRO') AS SELECT * FROM TRANSACTIONS_RAW PARTITION BY ACCOUNT_ID; -- Register the existing stream of customer data CREATE STREAM CUST_RAW_STREAM (ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, AVG_CREDIT_SPEND DOUBLE) WITH (KAFKA_TOPIC='customers', VALUE_FORMAT='JSON'); -- Repartition the customer data stream by account_id to prepare for the join CREATE STREAM CUSTOMER_REKEYED WITH (VALUE_FORMAT='AVRO') AS SELECT * FROM CUST_RAW_STREAM PARTITION BY ID; -- Register the partitioned customer data topic as a table used for the join with the incoming stream of transactions: CREATE TABLE customer WITH (KAFKA_TOPIC='CUSTOMER_REKEYED', VALUE_FORMAT='AVRO', KEY='ID'); -- Join the transactions to customer information: CREATE STREAM TRANSACTIONS_ENRICHED AS SELECT T.ACCOUNT_ID, T.CARD_TYPE, T.AMOUNT, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME, C.AVG_CREDIT_SPEND FROM TRANSACTIONS_SOURCE T INNER JOIN CUSTOMER C ON T.ACCOUNT_ID = C.ID; -- Aggregate the stream of transactions for each account ID using a two-hour tumbling window, and filter for accounts in which the total spend in a two-hour period is greater than the customer\u2019s average: CREATE TABLE POSSIBLE_STOLEN_CARD AS SELECT TIMESTAMPTOSTRING(WindowStart(), 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_START, T.ACCOUNT_ID, T.CARD_TYPE, SUM(T.AMOUNT) AS TOTAL_CREDIT_SPEND, T.FULL_NAME, MAX(T.AVG_CREDIT_SPEND) AS AVG_CREDIT_SPEND FROM TRANSACTIONS_ENRICHED T WINDOW TUMBLING (SIZE 2 HOURS) GROUP BY T.ACCOUNT_ID, T.CARD_TYPE, T.FULL_NAME HAVING SUM(T.AMOUNT) > MAX(T.AVG_CREDIT_SPEND) ; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO ...TODO...","title":"Run stream processing app"},{"location":"fin-serv/credit-card-activity/#full-ksqldb-statements","text":"Here is a complete view of all the recipe steps explained above. -- Stream of transactions CREATE SOURCE CONNECTOR transactions WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-transactions', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'TRANSACTIONS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Register the stream of transactions CREATE STREAM TRANSACTIONS_RAW (ACCOUNT_ID VARCHAR, TIMESTAMP VARCHAR, CARD_TYPE VARCHAR, AMOUNT DOUBLE, IP_ADDRESS VARCHAR, TRANSACTION_ID VARCHAR) WITH (KAFKA_TOPIC='transactions', VALUE_FORMAT='JSON'); -- Repartition the stream on account_id in order to ensure that all the streams and tables are co-partitioned, which means that input records on both sides of the join have the same configuration settings for partitions. CREATE STREAM TRANSACTIONS_SOURCE WITH (VALUE_FORMAT='AVRO') AS SELECT * FROM TRANSACTIONS_RAW PARTITION BY ACCOUNT_ID; -- Register the existing stream of customer data CREATE STREAM CUST_RAW_STREAM (ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, AVG_CREDIT_SPEND DOUBLE) WITH (KAFKA_TOPIC='customers', VALUE_FORMAT='JSON'); -- Repartition the customer data stream by account_id to prepare for the join CREATE STREAM CUSTOMER_REKEYED WITH (VALUE_FORMAT='AVRO') AS SELECT * FROM CUST_RAW_STREAM PARTITION BY ID; -- Register the partitioned customer data topic as a table used for the join with the incoming stream of transactions: CREATE TABLE customer WITH (KAFKA_TOPIC='CUSTOMER_REKEYED', VALUE_FORMAT='AVRO', KEY='ID'); -- Join the transactions to customer information: CREATE STREAM TRANSACTIONS_ENRICHED AS SELECT T.ACCOUNT_ID, T.CARD_TYPE, T.AMOUNT, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME, C.AVG_CREDIT_SPEND FROM TRANSACTIONS_SOURCE T INNER JOIN CUSTOMER C ON T.ACCOUNT_ID = C.ID; -- Aggregate the stream of transactions for each account ID using a two-hour tumbling window, and filter for accounts in which the total spend in a two-hour period is greater than the customer\u2019s average: CREATE TABLE POSSIBLE_STOLEN_CARD AS SELECT TIMESTAMPTOSTRING(WindowStart(), 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_START, T.ACCOUNT_ID, T.CARD_TYPE, SUM(T.AMOUNT) AS TOTAL_CREDIT_SPEND, T.FULL_NAME, MAX(T.AVG_CREDIT_SPEND) AS AVG_CREDIT_SPEND FROM TRANSACTIONS_ENRICHED T WINDOW TUMBLING (SIZE 2 HOURS) GROUP BY T.ACCOUNT_ID, T.CARD_TYPE, T.FULL_NAME HAVING SUM(T.AMOUNT) > MAX(T.AVG_CREDIT_SPEND) ;","title":"Full ksqlDB Statements"},{"location":"fin-serv/denormalization/","text":"Change Data Capture (CDC) for Orders What is it? If you have transactional events for orders in a marketplace, you can stream the Change Data Capture (CDC) and denormalize it. Denormalization is a well-established pattern for performance, because most times querying a single table of data will perform better than querying across multiple at runtime. Then open it up to be consumed by downstream applications in your business, or stream it out to another destination. This recipe demonstrates this principle by streaming from a SQL Server, denormalizing the data, and writing to Snowflake. Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data. Read the data in Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. The recipes are shown with some specific connector types and configuration, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment, see documentation for details. Change Data Capture (CDC) for orders is being written to a SQL Server database, and there is an Oracle database with customer data. -- Stream of orders CREATE SOURCE CONNECTOR orders WITH ( 'connector.class' = 'SqlServerCdcSource', 'name' = 'recipe-sqlservercdc-orders', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'database.hostname' = '<db-name>', 'database.port' = '1433', 'database.user' = '<database-username>', 'database.password' = '<database-password>', 'database.dbname' = 'database-name', 'database.server.name' = 'sql', 'table.include.list' ='<table_name>', 'snapshot.mode' = 'initial', 'output.data.format' = 'JSON', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topic.prefix' = 'oracle_', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! You can manually insert values into the source streams in the next section. Run stream processing app This streams the user orders and denormalizes the data by joining facts (orders) with the dimension (customer). -- stream of user orders: CREATE STREAM orders ( ...TODO... ) with ( kafka_topic = 'orders', value_format = 'json' ); -- Register the existing stream of customer data CREATE STREAM CUST_RAW_STREAM (ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, COMPANY VARCHAR, STREET_ADDRESS VARCHAR, CITY VARCHAR, COUNTRY VARCHAR) WITH (KAFKA_TOPIC='customers', VALUE_FORMAT='JSON'); -- Register the customer data topic as a table CREATE TABLE customer WITH (KAFKA_TOPIC='CUST_RAW_STREAM', VALUE_FORMAT='JSON', KEY='ID'); -- Denormalize data: joining facts (orders) with the dimension (customer) CREATE STREAM ORDERS_ENRICHED AS SELECT O.order_id AS order_id, O.item AS item, O.order_total_usd AS order_total_usd, C.first_name || ' ' || C.last_name AS full_name, C.email AS email, C.company AS company, C.street_address AS street_address, C.city AS city, C.country AS country FROM ORDERS O LEFT JOIN CUSTOMERS C ON O.customer_id = C.id; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO ...TODO... Write the data out Any downstream application or database can receive the denormalized data. -- Send data to Snowflake CREATE SINK CONNECTOR orders_enriched WITH ( 'connector.class' = 'SnowflakeSink', 'name' = 'recipe-snowflake-analyzed_clickstream', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'ORDERS_ENRICHED', 'input.data.format' = 'JSON', 'snowflake.url.name' = 'https=//wm83168.us-central1.gcp.snowflakecomputing.com=443', 'snowflake.user.name' = '<login-username>', 'snowflake.private.key' = '<private-key>', 'snowflake.database.name' = '<database-name>', 'snowflake.schema.name' = '<schema-name>', 'tasks.max' = '1'); Full ksqlDB Statements Here is a complete view of all the recipe steps explained above. -- Stream of orders CREATE SOURCE CONNECTOR orders WITH ( 'connector.class' = 'SqlServerCdcSource', 'name' = 'recipe-sqlservercdc-orders', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'database.hostname' = '<db-name>', 'database.port' = '1433', 'database.user' = '<database-username>', 'database.password' = '<database-password>', 'database.dbname' = 'database-name', 'database.server.name' = 'sql', 'table.include.list' ='<table_name>', 'snapshot.mode' = 'initial', 'output.data.format' = 'JSON', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topic.prefix' = 'oracle_', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- stream of user orders: CREATE STREAM orders ( ...TODO... ) with ( kafka_topic = 'orders', value_format = 'json' ); -- Register the existing stream of customer data CREATE STREAM CUST_RAW_STREAM (ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, COMPANY VARCHAR, STREET_ADDRESS VARCHAR, CITY VARCHAR, COUNTRY VARCHAR) WITH (KAFKA_TOPIC='customers', VALUE_FORMAT='JSON'); -- Register the customer data topic as a table CREATE TABLE customer WITH (KAFKA_TOPIC='CUST_RAW_STREAM', VALUE_FORMAT='JSON', KEY='ID'); -- Denormalize data: joining facts (orders) with the dimension (customer) CREATE STREAM ORDERS_ENRICHED AS SELECT O.order_id AS order_id, O.item AS item, O.order_total_usd AS order_total_usd, C.first_name || ' ' || C.last_name AS full_name, C.email AS email, C.company AS company, C.street_address AS street_address, C.city AS city, C.country AS country FROM ORDERS O LEFT JOIN CUSTOMERS C ON O.customer_id = C.id; -- Send data to Snowflake CREATE SINK CONNECTOR orders_enriched WITH ( 'connector.class' = 'SnowflakeSink', 'name' = 'recipe-snowflake-analyzed_clickstream', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'ORDERS_ENRICHED', 'input.data.format' = 'JSON', 'snowflake.url.name' = 'https=//wm83168.us-central1.gcp.snowflakecomputing.com=443', 'snowflake.user.name' = '<login-username>', 'snowflake.private.key' = '<private-key>', 'snowflake.database.name' = '<database-name>', 'snowflake.schema.name' = '<schema-name>', 'tasks.max' = '1');","title":"Change Data Capture (CDC) for Orders"},{"location":"fin-serv/denormalization/#change-data-capture-cdc-for-orders","text":"","title":"Change Data Capture (CDC) for Orders"},{"location":"fin-serv/denormalization/#what-is-it","text":"If you have transactional events for orders in a marketplace, you can stream the Change Data Capture (CDC) and denormalize it. Denormalization is a well-established pattern for performance, because most times querying a single table of data will perform better than querying across multiple at runtime. Then open it up to be consumed by downstream applications in your business, or stream it out to another destination. This recipe demonstrates this principle by streaming from a SQL Server, denormalizing the data, and writing to Snowflake.","title":"What is it?"},{"location":"fin-serv/denormalization/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"fin-serv/denormalization/#step-by-step","text":"","title":"Step-by-step"},{"location":"fin-serv/denormalization/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data.","title":"Setup your Environment"},{"location":"fin-serv/denormalization/#read-the-data-in","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. The recipes are shown with some specific connector types and configuration, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment, see documentation for details. Change Data Capture (CDC) for orders is being written to a SQL Server database, and there is an Oracle database with customer data. -- Stream of orders CREATE SOURCE CONNECTOR orders WITH ( 'connector.class' = 'SqlServerCdcSource', 'name' = 'recipe-sqlservercdc-orders', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'database.hostname' = '<db-name>', 'database.port' = '1433', 'database.user' = '<database-username>', 'database.password' = '<database-password>', 'database.dbname' = 'database-name', 'database.server.name' = 'sql', 'table.include.list' ='<table_name>', 'snapshot.mode' = 'initial', 'output.data.format' = 'JSON', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topic.prefix' = 'oracle_', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! You can manually insert values into the source streams in the next section.","title":"Read the data in"},{"location":"fin-serv/denormalization/#run-stream-processing-app","text":"This streams the user orders and denormalizes the data by joining facts (orders) with the dimension (customer). -- stream of user orders: CREATE STREAM orders ( ...TODO... ) with ( kafka_topic = 'orders', value_format = 'json' ); -- Register the existing stream of customer data CREATE STREAM CUST_RAW_STREAM (ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, COMPANY VARCHAR, STREET_ADDRESS VARCHAR, CITY VARCHAR, COUNTRY VARCHAR) WITH (KAFKA_TOPIC='customers', VALUE_FORMAT='JSON'); -- Register the customer data topic as a table CREATE TABLE customer WITH (KAFKA_TOPIC='CUST_RAW_STREAM', VALUE_FORMAT='JSON', KEY='ID'); -- Denormalize data: joining facts (orders) with the dimension (customer) CREATE STREAM ORDERS_ENRICHED AS SELECT O.order_id AS order_id, O.item AS item, O.order_total_usd AS order_total_usd, C.first_name || ' ' || C.last_name AS full_name, C.email AS email, C.company AS company, C.street_address AS street_address, C.city AS city, C.country AS country FROM ORDERS O LEFT JOIN CUSTOMERS C ON O.customer_id = C.id; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO ...TODO...","title":"Run stream processing app"},{"location":"fin-serv/denormalization/#write-the-data-out","text":"Any downstream application or database can receive the denormalized data. -- Send data to Snowflake CREATE SINK CONNECTOR orders_enriched WITH ( 'connector.class' = 'SnowflakeSink', 'name' = 'recipe-snowflake-analyzed_clickstream', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'ORDERS_ENRICHED', 'input.data.format' = 'JSON', 'snowflake.url.name' = 'https=//wm83168.us-central1.gcp.snowflakecomputing.com=443', 'snowflake.user.name' = '<login-username>', 'snowflake.private.key' = '<private-key>', 'snowflake.database.name' = '<database-name>', 'snowflake.schema.name' = '<schema-name>', 'tasks.max' = '1');","title":"Write the data out"},{"location":"fin-serv/denormalization/#full-ksqldb-statements","text":"Here is a complete view of all the recipe steps explained above. -- Stream of orders CREATE SOURCE CONNECTOR orders WITH ( 'connector.class' = 'SqlServerCdcSource', 'name' = 'recipe-sqlservercdc-orders', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'database.hostname' = '<db-name>', 'database.port' = '1433', 'database.user' = '<database-username>', 'database.password' = '<database-password>', 'database.dbname' = 'database-name', 'database.server.name' = 'sql', 'table.include.list' ='<table_name>', 'snapshot.mode' = 'initial', 'output.data.format' = 'JSON', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topic.prefix' = 'oracle_', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- stream of user orders: CREATE STREAM orders ( ...TODO... ) with ( kafka_topic = 'orders', value_format = 'json' ); -- Register the existing stream of customer data CREATE STREAM CUST_RAW_STREAM (ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, COMPANY VARCHAR, STREET_ADDRESS VARCHAR, CITY VARCHAR, COUNTRY VARCHAR) WITH (KAFKA_TOPIC='customers', VALUE_FORMAT='JSON'); -- Register the customer data topic as a table CREATE TABLE customer WITH (KAFKA_TOPIC='CUST_RAW_STREAM', VALUE_FORMAT='JSON', KEY='ID'); -- Denormalize data: joining facts (orders) with the dimension (customer) CREATE STREAM ORDERS_ENRICHED AS SELECT O.order_id AS order_id, O.item AS item, O.order_total_usd AS order_total_usd, C.first_name || ' ' || C.last_name AS full_name, C.email AS email, C.company AS company, C.street_address AS street_address, C.city AS city, C.country AS country FROM ORDERS O LEFT JOIN CUSTOMERS C ON O.customer_id = C.id; -- Send data to Snowflake CREATE SINK CONNECTOR orders_enriched WITH ( 'connector.class' = 'SnowflakeSink', 'name' = 'recipe-snowflake-analyzed_clickstream', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'ORDERS_ENRICHED', 'input.data.format' = 'JSON', 'snowflake.url.name' = 'https=//wm83168.us-central1.gcp.snowflakecomputing.com=443', 'snowflake.user.name' = '<login-username>', 'snowflake.private.key' = '<private-key>', 'snowflake.database.name' = '<database-name>', 'snowflake.schema.name' = '<schema-name>', 'tasks.max' = '1');","title":"Full ksqlDB Statements"},{"location":"retail/fleet_management/","text":"Fleet Management What is it? More and more fleet management relies on knowing real-time information on vehicles, their locations, and vehicle telemetry. This enables businesses to improve route efficiency, fuel efficiency, automate service schedules, etc. TODO--add diagram Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data. Read the data in Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. The recipes are shown with some specific connector types and configuration, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment, see documentation for details. TODO -- Stream of fleet descriptions CREATE SOURCE CONNECTOR fleet_description WITH ( 'connector.class' = 'MongoDbAtlasSource', 'name' = 'recipe-mongodb-fleet_description', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<database-host-address>', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'database' = '<database-name>', 'collection' = '<database-collection-name>', 'poll.await.time.ms' = '5000', 'poll.max.batch.size' = '1000', 'copy.existing' = 'true', 'output.data.format' = 'JSON' 'tasks.max' = '1'); -- Stream of current location of each vehicle in the fleet CREATE SOURCE CONNECTOR fleet_location WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-fleet_location', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'fleet_location', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! You can manually insert values into the source streams in the next section. Run stream processing app TODO --------------------------------------------------------------------------------------------------- -- Register sources: --------------------------------------------------------------------------------------------------- -- stream of locations: CREATE STREAM locations ( vehicle_id int, latitude float, longitude float, timestamp varchar ) with ( kafka_topic = 'locations', value_format = 'json' ); -- fleet lookup table: CREATE TABLE fleet ( vehicle_id int, driver_id int, license bigint ) with ( kafka_topic = 'descriptions', value_format = 'json' ); -- enrich fleet location stream with more fleet information: CREATE STREAM fleet_location_enhanced AS SELECT vehicle_id, latitude, longitude, timestamp, f.driver_id, f.license FROM locations l LEFT JOIN fleet f ON l.vehicle_id = f.vehicle_id; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO location (vehicle_id, latitude, longitude, timestamp) VALUES (5401, 16.7587537, 96.2482149, '2021-09-23T10:50:00.000Z'); INSERT INTO location (vehicle_id, latitude, longitude, timestamp) VALUES (5401, 16.004175, 120.7806412, '2021-09-27T06:39:00.000Z'); INSERT INTO location (vehicle_id, latitude, longitude, timestamp) VALUES (5402, 32.755613, 22.6377432, '2021-09-25T20:22:00.000Z'); INSERT INTO description (vehicle_id, driver_id, license_plate) VALUES (5401, 847383, 8852693196); INSERT INTO description (vehicle_id, driver_id, license_plate) VALUES (5402, 922947, 1255144201); INSERT INTO description (vehicle_id, driver_id, license_plate) VALUES (5403, 435309, 2132311746); Full ksqlDB Statements Here is a complete view of all the recipe steps explained above. -- Stream of fleet descriptions CREATE SOURCE CONNECTOR fleet_description WITH ( 'connector.class' = 'MongoDbAtlasSource', 'name' = 'recipe-mongodb-fleet_description', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<database-host-address>', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'database' = '<database-name>', 'collection' = '<database-collection-name>', 'poll.await.time.ms' = '5000', 'poll.max.batch.size' = '1000', 'copy.existing' = 'true', 'output.data.format' = 'JSON' 'tasks.max' = '1'); -- Stream of current location of each vehicle in the fleet CREATE SOURCE CONNECTOR fleet_location WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-fleet_location', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'fleet_location', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); --------------------------------------------------------------------------------------------------- -- Register sources: --------------------------------------------------------------------------------------------------- -- stream of locations: CREATE STREAM locations ( vehicle_id int, latitude float, longitude float, timestamp varchar ) with ( kafka_topic = 'locations', value_format = 'json' ); -- fleet lookup table: CREATE TABLE fleet ( vehicle_id int, driver_id int, license bigint ) with ( kafka_topic = 'descriptions', value_format = 'json' ); -- enrich fleet location stream with more fleet information: CREATE STREAM fleet_location_enhanced AS SELECT vehicle_id, latitude, longitude, timestamp, f.driver_id, f.license FROM locations l LEFT JOIN fleet f ON l.vehicle_id = f.vehicle_id;","title":"Fleet Management"},{"location":"retail/fleet_management/#fleet-management","text":"","title":"Fleet Management"},{"location":"retail/fleet_management/#what-is-it","text":"More and more fleet management relies on knowing real-time information on vehicles, their locations, and vehicle telemetry. This enables businesses to improve route efficiency, fuel efficiency, automate service schedules, etc. TODO--add diagram","title":"What is it?"},{"location":"retail/fleet_management/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"retail/fleet_management/#step-by-step","text":"","title":"Step-by-step"},{"location":"retail/fleet_management/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data.","title":"Setup your Environment"},{"location":"retail/fleet_management/#read-the-data-in","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. The recipes are shown with some specific connector types and configuration, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment, see documentation for details. TODO -- Stream of fleet descriptions CREATE SOURCE CONNECTOR fleet_description WITH ( 'connector.class' = 'MongoDbAtlasSource', 'name' = 'recipe-mongodb-fleet_description', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<database-host-address>', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'database' = '<database-name>', 'collection' = '<database-collection-name>', 'poll.await.time.ms' = '5000', 'poll.max.batch.size' = '1000', 'copy.existing' = 'true', 'output.data.format' = 'JSON' 'tasks.max' = '1'); -- Stream of current location of each vehicle in the fleet CREATE SOURCE CONNECTOR fleet_location WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-fleet_location', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'fleet_location', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! You can manually insert values into the source streams in the next section.","title":"Read the data in"},{"location":"retail/fleet_management/#run-stream-processing-app","text":"TODO --------------------------------------------------------------------------------------------------- -- Register sources: --------------------------------------------------------------------------------------------------- -- stream of locations: CREATE STREAM locations ( vehicle_id int, latitude float, longitude float, timestamp varchar ) with ( kafka_topic = 'locations', value_format = 'json' ); -- fleet lookup table: CREATE TABLE fleet ( vehicle_id int, driver_id int, license bigint ) with ( kafka_topic = 'descriptions', value_format = 'json' ); -- enrich fleet location stream with more fleet information: CREATE STREAM fleet_location_enhanced AS SELECT vehicle_id, latitude, longitude, timestamp, f.driver_id, f.license FROM locations l LEFT JOIN fleet f ON l.vehicle_id = f.vehicle_id; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO location (vehicle_id, latitude, longitude, timestamp) VALUES (5401, 16.7587537, 96.2482149, '2021-09-23T10:50:00.000Z'); INSERT INTO location (vehicle_id, latitude, longitude, timestamp) VALUES (5401, 16.004175, 120.7806412, '2021-09-27T06:39:00.000Z'); INSERT INTO location (vehicle_id, latitude, longitude, timestamp) VALUES (5402, 32.755613, 22.6377432, '2021-09-25T20:22:00.000Z'); INSERT INTO description (vehicle_id, driver_id, license_plate) VALUES (5401, 847383, 8852693196); INSERT INTO description (vehicle_id, driver_id, license_plate) VALUES (5402, 922947, 1255144201); INSERT INTO description (vehicle_id, driver_id, license_plate) VALUES (5403, 435309, 2132311746);","title":"Run stream processing app"},{"location":"retail/fleet_management/#full-ksqldb-statements","text":"Here is a complete view of all the recipe steps explained above. -- Stream of fleet descriptions CREATE SOURCE CONNECTOR fleet_description WITH ( 'connector.class' = 'MongoDbAtlasSource', 'name' = 'recipe-mongodb-fleet_description', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<database-host-address>', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'database' = '<database-name>', 'collection' = '<database-collection-name>', 'poll.await.time.ms' = '5000', 'poll.max.batch.size' = '1000', 'copy.existing' = 'true', 'output.data.format' = 'JSON' 'tasks.max' = '1'); -- Stream of current location of each vehicle in the fleet CREATE SOURCE CONNECTOR fleet_location WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-fleet_location', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'fleet_location', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); --------------------------------------------------------------------------------------------------- -- Register sources: --------------------------------------------------------------------------------------------------- -- stream of locations: CREATE STREAM locations ( vehicle_id int, latitude float, longitude float, timestamp varchar ) with ( kafka_topic = 'locations', value_format = 'json' ); -- fleet lookup table: CREATE TABLE fleet ( vehicle_id int, driver_id int, license bigint ) with ( kafka_topic = 'descriptions', value_format = 'json' ); -- enrich fleet location stream with more fleet information: CREATE STREAM fleet_location_enhanced AS SELECT vehicle_id, latitude, longitude, timestamp, f.driver_id, f.license FROM locations l LEFT JOIN fleet f ON l.vehicle_id = f.vehicle_id;","title":"Full ksqlDB Statements"},{"location":"retail/inventory/","text":"Real-time Inventory What is it? Having an up-to-date view of inventory on every item is essential in today's online marketplaces. This helps businesses maintain the right amount of inventory\u2014not too much and not too little\u2014so that they can meet demand while minimizing costs. This recipe demonstrates how to update inventory in real time and always have an up-to-date snapshot of your stock. Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-Step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data. Read the data in Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. The recipes are shown with some specific connector types and configuration, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment, see documentation for details. For this recipe, we are interested in knowing each event for an item that affects its quantity. This creates a stream of events, where each event results in the addition or removal of inventory. CREATE SOURCE CONNECTOR inventory WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-inventory', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = '<table>', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! You can manually insert values into the source streams in the next section. Run stream processing app Create a ksqlDB TABLE , which is a mutable, partitioned collection that models change over time that represents what is true as of \"now\". --------------------------------------------------------------------------------------------------- -- Create stream of inventory --------------------------------------------------------------------------------------------------- CREATE STREAM inventory_stream ( id STRING key, item STRING, quantity INTEGER, price DOUBLE) with (VALUE_FORMAT='json', KAFKA_TOPIC='inventory'); --------------------------------------------------------------------------------------------------- -- Create stateful table with up-to-date information of inventory availability --------------------------------------------------------------------------------------------------- CREATE TABLE inventory_stream_table WITH (kafka_topic='inventory_table') AS SELECT item, SUM(quantity) AS item_quantity FROM inventory_stream GROUP BY item EMIT CHANGES; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('1', 'Apple Magic Mouse 2', 10, 99.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('2', 'iPhoneX', 25, 999.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('3', 'MacBookPro13', 100, 1799.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('4', 'iPad4', 20, 340.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('5', 'Apple Pencil', 10, 79.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('5', 'PhoneX', 10, 899.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('4', 'iPad4', -20, 399.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('3', 'MacBookPro13', 10, 1899.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('4', 'iPad4', 20, 399.00); Full ksqlDB Statements Here is a complete view of all the recipe steps explained above. CREATE SOURCE CONNECTOR inventory WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-inventory', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = '<table>', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); --------------------------------------------------------------------------------------------------- -- Create stream of inventory --------------------------------------------------------------------------------------------------- CREATE STREAM inventory_stream ( id STRING key, item STRING, quantity INTEGER, price DOUBLE) with (VALUE_FORMAT='json', KAFKA_TOPIC='inventory'); --------------------------------------------------------------------------------------------------- -- Create stateful table with up-to-date information of inventory availability --------------------------------------------------------------------------------------------------- CREATE TABLE inventory_stream_table WITH (kafka_topic='inventory_table') AS SELECT item, SUM(quantity) AS item_quantity FROM inventory_stream GROUP BY item EMIT CHANGES;","title":"Real-time Inventory"},{"location":"retail/inventory/#real-time-inventory","text":"","title":"Real-time Inventory"},{"location":"retail/inventory/#what-is-it","text":"Having an up-to-date view of inventory on every item is essential in today's online marketplaces. This helps businesses maintain the right amount of inventory\u2014not too much and not too little\u2014so that they can meet demand while minimizing costs. This recipe demonstrates how to update inventory in real time and always have an up-to-date snapshot of your stock.","title":"What is it?"},{"location":"retail/inventory/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"retail/inventory/#step-by-step","text":"","title":"Step-by-Step"},{"location":"retail/inventory/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data.","title":"Setup your Environment"},{"location":"retail/inventory/#read-the-data-in","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. The recipes are shown with some specific connector types and configuration, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment, see documentation for details. For this recipe, we are interested in knowing each event for an item that affects its quantity. This creates a stream of events, where each event results in the addition or removal of inventory. CREATE SOURCE CONNECTOR inventory WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-inventory', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = '<table>', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! You can manually insert values into the source streams in the next section.","title":"Read the data in"},{"location":"retail/inventory/#run-stream-processing-app","text":"Create a ksqlDB TABLE , which is a mutable, partitioned collection that models change over time that represents what is true as of \"now\". --------------------------------------------------------------------------------------------------- -- Create stream of inventory --------------------------------------------------------------------------------------------------- CREATE STREAM inventory_stream ( id STRING key, item STRING, quantity INTEGER, price DOUBLE) with (VALUE_FORMAT='json', KAFKA_TOPIC='inventory'); --------------------------------------------------------------------------------------------------- -- Create stateful table with up-to-date information of inventory availability --------------------------------------------------------------------------------------------------- CREATE TABLE inventory_stream_table WITH (kafka_topic='inventory_table') AS SELECT item, SUM(quantity) AS item_quantity FROM inventory_stream GROUP BY item EMIT CHANGES; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('1', 'Apple Magic Mouse 2', 10, 99.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('2', 'iPhoneX', 25, 999.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('3', 'MacBookPro13', 100, 1799.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('4', 'iPad4', 20, 340.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('5', 'Apple Pencil', 10, 79.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('5', 'PhoneX', 10, 899.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('4', 'iPad4', -20, 399.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('3', 'MacBookPro13', 10, 1899.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('4', 'iPad4', 20, 399.00);","title":"Run stream processing app"},{"location":"retail/inventory/#full-ksqldb-statements","text":"Here is a complete view of all the recipe steps explained above. CREATE SOURCE CONNECTOR inventory WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-inventory', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = '<table>', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); --------------------------------------------------------------------------------------------------- -- Create stream of inventory --------------------------------------------------------------------------------------------------- CREATE STREAM inventory_stream ( id STRING key, item STRING, quantity INTEGER, price DOUBLE) with (VALUE_FORMAT='json', KAFKA_TOPIC='inventory'); --------------------------------------------------------------------------------------------------- -- Create stateful table with up-to-date information of inventory availability --------------------------------------------------------------------------------------------------- CREATE TABLE inventory_stream_table WITH (kafka_topic='inventory_table') AS SELECT item, SUM(quantity) AS item_quantity FROM inventory_stream GROUP BY item EMIT CHANGES;","title":"Full ksqlDB Statements"},{"location":"security/ssh-attack/","text":"Detecting and Analyzing SSH Attacks What is it? There are lots of ways SSH can be abused but one of the most straightforward ways to detect a problem is to monitor for rejected logins. This recipe processes Syslog data to detect bad logins and streams out those pairs of usernames and IP addresses. With ksqlDB, you can filter and react to events in real time rather than performing historical analysis of Syslog data from cold storage. Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-Step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data. Read the data in This recipe is a great demonstration on how to run a self-managed connector, to push syslog data into Confluent Cloud into a Kafka topic called syslog . Create a file called Dockerfile to bundle a connect worker with kafka-connect-syslog : FROM confluentinc/cp-server-connect-base:6.2.0 ENV CONNECT_PLUGIN_PATH=\"/usr/share/java,/usr/share/confluent-hub-components\" USER root COPY --chown=appuser:appuser include/etc/confluent/docker /etc/confluent/docker USER appuser ARG CONNECTOR_OWNER=confluentinc ARG CONNECTOR_NAME ARG CONNECTOR_VERSION RUN confluent-hub install --no-prompt confluent-hub install confluentinc/kafka-connect-syslog:1.3.4 CMD [\"/etc/confluent/docker/run\"] Build the custom Docker image with the command: docker build \\ -t localbuild/connect_distributed_with_syslog:1.3.4 \\ -f Dockerfile . Create a file called docker-compose.yml with the following content, substituting your Confluent Cloud connection information: --- version: '2' services: connect: image: localbuild/connect_distributed_with_syslog:1.3.4 hostname: connect container_name: connect ports: - \"8083:8083\" environment: CONNECT_BOOTSTRAP_SERVERS: $BOOTSTRAP_SERVERS CONNECT_REST_PORT: 8083 CONNECT_GROUP_ID: \"connect\" CONNECT_CONFIG_STORAGE_TOPIC: recipe-connect-configs CONNECT_OFFSET_STORAGE_TOPIC: recipe-connect-offsets CONNECT_STATUS_STORAGE_TOPIC: recipe-connect-status CONNECT_REPLICATION_FACTOR: 3 CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3 CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3 CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3 CONNECT_KEY_CONVERTER: \"org.apache.kafka.connect.storage.StringConverter\" CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\" CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"true\" CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: $SCHEMA_REGISTRY_URL CONNECT_VALUE_CONVERTER_BASIC_AUTH_CREDENTIALS_SOURCE: $BASIC_AUTH_CREDENTIALS_SOURCE CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_BASIC_AUTH_USER_INFO: $SCHEMA_REGISTRY_BASIC_AUTH_USER_INFO CONNECT_REST_ADVERTISED_HOST_NAME: \"connect\" CONNECT_PLUGIN_PATH: \"/usr/share/java,/usr/share/confluent-hub-components\" CONNECT_LOG4J_ROOT_LOGLEVEL: INFO CONNECT_LOG4J_LOGGERS: org.reflections=ERROR CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-6.2.0.jar # Connect worker CONNECT_SECURITY_PROTOCOL: SASL_SSL CONNECT_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_SASL_MECHANISM: PLAIN # Connect producer CONNECT_PRODUCER_SECURITY_PROTOCOL: SASL_SSL CONNECT_PRODUCER_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_PRODUCER_SASL_MECHANISM: PLAIN # Connect consumer CONNECT_CONSUMER_SECURITY_PROTOCOL: SASL_SSL CONNECT_CONSUMER_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_CONSUMER_SASL_MECHANISM: PLAIN Run the container with: docker-compose up -d Now you should have Syslog messages being written to the topic syslog in Confluent Cloud. Run stream processing app Process the syslog events by flagging events with invalid users, stripping out all the other unnecessary fields, and creating just a stream of relevant information. There are many ways to customize the resulting stream to fit the business needs: this example also demonstrates how to enrich the stream with a new field FACILITY_DESCRIPTION with human-readable content. -- Define Kafka parameters DEFINE topic = 'syslog'; -- Extract relevant fields from log messages CREATE OR REPLACE STREAM `syslog` WITH ( KAFKA_TOPIC = '${topic}', VALUE_FORMAT = 'avro' ); CREATE STREAM `by_facility` AS SELECT TIMESTAMPTOSTRING(TIMESTAMP, 'yyyy-MM-dd HH:mm:ss') AS SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS FROM `syslog` PARTITION BY FACILITY EMIT CHANGES; -- Flag events with invalid users, and enrich with a new field 'FACILITY_DESCRIPTION' CREATE STREAM `invalid_users` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS, CASE WHEN FACILITY = 0 THEN 'kernel messages' WHEN FACILITY = 1 THEN 'user-level messages' WHEN FACILITY = 2 THEN 'mail system' WHEN FACILITY = 3 THEN 'system daemons' WHEN FACILITY = 4 THEN 'security/authorization messages' WHEN FACILITY = 5 THEN 'messages generated internally by syslogd' WHEN FACILITY = 6 THEN 'line printer subsystem' ELSE '<unknown>' END AS FACILITY_DESCRIPTION FROM `by_facility` WHERE MESSAGE LIKE 'Invalid user%' EMIT CHANGES; -- Create actionable stream of SSH attacks, enriched with user and IP CREATE STREAM `ssh_attacks` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY_DESCRIPTION, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[1] AS ATTACK_USER, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[2] AS ATTACK_IP FROM `invalid_users` EMIT CHANGES; Full ksqlDB Statements Here is a complete view of all the recipe steps explained above. Run the Syslog source connector locally, then proceed with ksqlDB to process the Syslog messages. -- Define Kafka parameters DEFINE topic = 'syslog'; -- Extract relevant fields from log messages CREATE OR REPLACE STREAM `syslog` WITH ( KAFKA_TOPIC = '${topic}', VALUE_FORMAT = 'avro' ); CREATE STREAM `by_facility` AS SELECT TIMESTAMPTOSTRING(TIMESTAMP, 'yyyy-MM-dd HH:mm:ss') AS SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS FROM `syslog` PARTITION BY FACILITY EMIT CHANGES; -- Flag events with invalid users, and enrich with a new field 'FACILITY_DESCRIPTION' CREATE STREAM `invalid_users` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS, CASE WHEN FACILITY = 0 THEN 'kernel messages' WHEN FACILITY = 1 THEN 'user-level messages' WHEN FACILITY = 2 THEN 'mail system' WHEN FACILITY = 3 THEN 'system daemons' WHEN FACILITY = 4 THEN 'security/authorization messages' WHEN FACILITY = 5 THEN 'messages generated internally by syslogd' WHEN FACILITY = 6 THEN 'line printer subsystem' ELSE '<unknown>' END AS FACILITY_DESCRIPTION FROM `by_facility` WHERE MESSAGE LIKE 'Invalid user%' EMIT CHANGES; -- Create actionable stream of SSH attacks, enriched with user and IP CREATE STREAM `ssh_attacks` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY_DESCRIPTION, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[1] AS ATTACK_USER, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[2] AS ATTACK_IP FROM `invalid_users` EMIT CHANGES;","title":"Detecting and Analyzing SSH Attacks"},{"location":"security/ssh-attack/#detecting-and-analyzing-ssh-attacks","text":"","title":"Detecting and Analyzing SSH Attacks"},{"location":"security/ssh-attack/#what-is-it","text":"There are lots of ways SSH can be abused but one of the most straightforward ways to detect a problem is to monitor for rejected logins. This recipe processes Syslog data to detect bad logins and streams out those pairs of usernames and IP addresses. With ksqlDB, you can filter and react to events in real time rather than performing historical analysis of Syslog data from cold storage.","title":"What is it?"},{"location":"security/ssh-attack/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"security/ssh-attack/#step-by-step","text":"","title":"Step-by-Step"},{"location":"security/ssh-attack/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data.","title":"Setup your Environment"},{"location":"security/ssh-attack/#read-the-data-in","text":"This recipe is a great demonstration on how to run a self-managed connector, to push syslog data into Confluent Cloud into a Kafka topic called syslog . Create a file called Dockerfile to bundle a connect worker with kafka-connect-syslog : FROM confluentinc/cp-server-connect-base:6.2.0 ENV CONNECT_PLUGIN_PATH=\"/usr/share/java,/usr/share/confluent-hub-components\" USER root COPY --chown=appuser:appuser include/etc/confluent/docker /etc/confluent/docker USER appuser ARG CONNECTOR_OWNER=confluentinc ARG CONNECTOR_NAME ARG CONNECTOR_VERSION RUN confluent-hub install --no-prompt confluent-hub install confluentinc/kafka-connect-syslog:1.3.4 CMD [\"/etc/confluent/docker/run\"] Build the custom Docker image with the command: docker build \\ -t localbuild/connect_distributed_with_syslog:1.3.4 \\ -f Dockerfile . Create a file called docker-compose.yml with the following content, substituting your Confluent Cloud connection information: --- version: '2' services: connect: image: localbuild/connect_distributed_with_syslog:1.3.4 hostname: connect container_name: connect ports: - \"8083:8083\" environment: CONNECT_BOOTSTRAP_SERVERS: $BOOTSTRAP_SERVERS CONNECT_REST_PORT: 8083 CONNECT_GROUP_ID: \"connect\" CONNECT_CONFIG_STORAGE_TOPIC: recipe-connect-configs CONNECT_OFFSET_STORAGE_TOPIC: recipe-connect-offsets CONNECT_STATUS_STORAGE_TOPIC: recipe-connect-status CONNECT_REPLICATION_FACTOR: 3 CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3 CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3 CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3 CONNECT_KEY_CONVERTER: \"org.apache.kafka.connect.storage.StringConverter\" CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\" CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"true\" CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: $SCHEMA_REGISTRY_URL CONNECT_VALUE_CONVERTER_BASIC_AUTH_CREDENTIALS_SOURCE: $BASIC_AUTH_CREDENTIALS_SOURCE CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_BASIC_AUTH_USER_INFO: $SCHEMA_REGISTRY_BASIC_AUTH_USER_INFO CONNECT_REST_ADVERTISED_HOST_NAME: \"connect\" CONNECT_PLUGIN_PATH: \"/usr/share/java,/usr/share/confluent-hub-components\" CONNECT_LOG4J_ROOT_LOGLEVEL: INFO CONNECT_LOG4J_LOGGERS: org.reflections=ERROR CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-6.2.0.jar # Connect worker CONNECT_SECURITY_PROTOCOL: SASL_SSL CONNECT_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_SASL_MECHANISM: PLAIN # Connect producer CONNECT_PRODUCER_SECURITY_PROTOCOL: SASL_SSL CONNECT_PRODUCER_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_PRODUCER_SASL_MECHANISM: PLAIN # Connect consumer CONNECT_CONSUMER_SECURITY_PROTOCOL: SASL_SSL CONNECT_CONSUMER_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_CONSUMER_SASL_MECHANISM: PLAIN Run the container with: docker-compose up -d Now you should have Syslog messages being written to the topic syslog in Confluent Cloud.","title":"Read the data in"},{"location":"security/ssh-attack/#run-stream-processing-app","text":"Process the syslog events by flagging events with invalid users, stripping out all the other unnecessary fields, and creating just a stream of relevant information. There are many ways to customize the resulting stream to fit the business needs: this example also demonstrates how to enrich the stream with a new field FACILITY_DESCRIPTION with human-readable content. -- Define Kafka parameters DEFINE topic = 'syslog'; -- Extract relevant fields from log messages CREATE OR REPLACE STREAM `syslog` WITH ( KAFKA_TOPIC = '${topic}', VALUE_FORMAT = 'avro' ); CREATE STREAM `by_facility` AS SELECT TIMESTAMPTOSTRING(TIMESTAMP, 'yyyy-MM-dd HH:mm:ss') AS SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS FROM `syslog` PARTITION BY FACILITY EMIT CHANGES; -- Flag events with invalid users, and enrich with a new field 'FACILITY_DESCRIPTION' CREATE STREAM `invalid_users` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS, CASE WHEN FACILITY = 0 THEN 'kernel messages' WHEN FACILITY = 1 THEN 'user-level messages' WHEN FACILITY = 2 THEN 'mail system' WHEN FACILITY = 3 THEN 'system daemons' WHEN FACILITY = 4 THEN 'security/authorization messages' WHEN FACILITY = 5 THEN 'messages generated internally by syslogd' WHEN FACILITY = 6 THEN 'line printer subsystem' ELSE '<unknown>' END AS FACILITY_DESCRIPTION FROM `by_facility` WHERE MESSAGE LIKE 'Invalid user%' EMIT CHANGES; -- Create actionable stream of SSH attacks, enriched with user and IP CREATE STREAM `ssh_attacks` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY_DESCRIPTION, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[1] AS ATTACK_USER, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[2] AS ATTACK_IP FROM `invalid_users` EMIT CHANGES;","title":"Run stream processing app"},{"location":"security/ssh-attack/#full-ksqldb-statements","text":"Here is a complete view of all the recipe steps explained above. Run the Syslog source connector locally, then proceed with ksqlDB to process the Syslog messages. -- Define Kafka parameters DEFINE topic = 'syslog'; -- Extract relevant fields from log messages CREATE OR REPLACE STREAM `syslog` WITH ( KAFKA_TOPIC = '${topic}', VALUE_FORMAT = 'avro' ); CREATE STREAM `by_facility` AS SELECT TIMESTAMPTOSTRING(TIMESTAMP, 'yyyy-MM-dd HH:mm:ss') AS SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS FROM `syslog` PARTITION BY FACILITY EMIT CHANGES; -- Flag events with invalid users, and enrich with a new field 'FACILITY_DESCRIPTION' CREATE STREAM `invalid_users` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS, CASE WHEN FACILITY = 0 THEN 'kernel messages' WHEN FACILITY = 1 THEN 'user-level messages' WHEN FACILITY = 2 THEN 'mail system' WHEN FACILITY = 3 THEN 'system daemons' WHEN FACILITY = 4 THEN 'security/authorization messages' WHEN FACILITY = 5 THEN 'messages generated internally by syslogd' WHEN FACILITY = 6 THEN 'line printer subsystem' ELSE '<unknown>' END AS FACILITY_DESCRIPTION FROM `by_facility` WHERE MESSAGE LIKE 'Invalid user%' EMIT CHANGES; -- Create actionable stream of SSH attacks, enriched with user and IP CREATE STREAM `ssh_attacks` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY_DESCRIPTION, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[1] AS ATTACK_USER, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[2] AS ATTACK_IP FROM `invalid_users` EMIT CHANGES;","title":"Full ksqlDB Statements"},{"location":"shared/ccloud_launch/","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Ccloud launch"},{"location":"shared/ccloud_setup/","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data.","title":"Ccloud setup"},{"location":"shared/code_summary/","text":"Here is a complete view of all the recipe steps explained above.","title":"Code summary"},{"location":"shared/connect/","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. The recipes are shown with some specific connector types and configuration, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment, see documentation for details.","title":"Connect"},{"location":"shared/manual_cue/","text":"In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements:","title":"Manual cue"},{"location":"shared/manual_insert/","text":"","title":"Manual insert"}]}
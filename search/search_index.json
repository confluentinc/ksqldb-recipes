{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ksqlDB Recipes Work in Progress! Click in the left nav bar to see one of the recipes","title":"ksqlDB Recipes"},{"location":"#welcome-to-ksqldb-recipes","text":"Work in Progress! Click in the left nav bar to see one of the recipes","title":"Welcome to ksqlDB Recipes"},{"location":"ai-ml/","text":"Description These recipes ...","title":"Description"},{"location":"ai-ml/#description","text":"These recipes ...","title":"Description"},{"location":"customer-360/","text":"Description These recipes ...","title":"Description"},{"location":"customer-360/#description","text":"These recipes ...","title":"Description"},{"location":"customer-360/clickstream/","text":"Clickstream Data Analysis What is it? Analyzing clickstream data enables businesses to understand the behavior of its online users, for example: User activity over a given time frame: how many webpages are users viewing Requests that end in error, over a given threshold (e.g., 404 HTTP codes) Where the requests are coming from geographically in a given window of time How long users are interacting with the site (user sessions) Cut to the code -- Stream of HTTP codes CREATE SOURCE CONNECTOR datagen_clickstream_codes WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_codes', 'quickstart' = 'clickstream_codes', 'maxInterval' = '20', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of users CREATE SOURCE CONNECTOR datagen_clickstream_users WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_users', 'quickstart' = 'clickstream_users', 'maxInterval' = '10', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of per-user session information CREATE SOURCE CONNECTOR datagen_clickstream WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream', 'quickstart' = 'clickstream', 'maxInterval' = '30', 'format' = 'json'); --------------------------------------------------------------------------------------------------- -- Register sources: --------------------------------------------------------------------------------------------------- -- stream of user clicks: CREATE STREAM clickstream ( _time bigint, time varchar, ip varchar, request varchar, status int, userid int, bytes bigint, agent varchar ) with ( kafka_topic = 'clickstream', value_format = 'json' ); -- error code lookup table: CREATE TABLE clickstream_codes ( code int primary key, definition varchar ) with ( kafka_topic = 'clickstream_codes', value_format = 'json' ); -- users lookup table: CREATE TABLE WEB_USERS ( user_id int primary key, registered_At BIGINT, username varchar, first_name varchar, last_name varchar, city varchar, level varchar ) with ( kafka_topic = 'clickstream_users', value_format = 'json' ); --------------------------------------------------------------------------------------------------- -- Build materialized stream views: --------------------------------------------------------------------------------------------------- -- enrich click-stream with more user information: CREATE STREAM USER_CLICKSTREAM AS SELECT userid, u.username, ip, u.city, request, status, bytes FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id; -- Enrich click-stream with more information about error codes: CREATE STREAM ENRICHED_ERROR_CODES AS SELECT code, definition FROM clickstream LEFT JOIN clickstream_codes ON clickstream.status = clickstream_codes.code; --------------------------------------------------------------------------------------------------- -- Build materialized table views: --------------------------------------------------------------------------------------------------- -- Per-userId tables ----------------------------------------------------------------------------- -- Table of events per minute for each user: CREATE table events_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS events FROM clickstream window TUMBLING (size 60 second) GROUP BY userid; -- Table of html pages per minute for each user: CREATE TABLE pages_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS pages FROM clickstream WINDOW HOPPING (size 60 second, advance by 5 second) WHERE request like '%html%' GROUP BY userid; -- Per-username tables ---------------------------------------------------------------------------- -- User sessions table - 30 seconds of inactivity expires the session -- Table counts number of events within the session CREATE TABLE CLICK_USER_SESSIONS AS SELECT username as K, AS_VALUE(username) as username, WINDOWEND as EVENT_TS, count(*) AS events FROM USER_CLICKSTREAM window SESSION (30 second) GROUP BY username; -- Per-status tables ------------------------------------------------------------------------------ -- number of errors per min, using 'HAVING' Filter to show ERROR codes > 400 where count > 5 CREATE TABLE ERRORS_PER_MIN_ALERT AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 20 second) WHERE status > 400 GROUP BY status HAVING count(*) > 5 AND count(*) is not NULL; -- number of errors per min CREATE TABLE ERRORS_PER_MIN AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 5 second) WHERE status > 400 GROUP BY status; -- Per-error code tables -------------------------------------------------------------------------- -- Enriched error codes table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE ENRICHED_ERROR_CODES_COUNT WITH (key_format='json') AS SELECT code as k1, definition as K2, AS_VALUE(code) as code, WINDOWSTART as EVENT_TS, AS_VALUE(definition) as definition, COUNT(*) AS count FROM ENRICHED_ERROR_CODES WINDOW TUMBLING (size 30 second) GROUP BY code, definition HAVING COUNT(*) > 1; -- Enriched user details table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE USER_IP_ACTIVITY WITH (key_format='json') AS SELECT username as k1, ip as k2, city as k3, AS_VALUE(username) as username, WINDOWSTART as EVENT_TS, AS_VALUE(ip) as ip, AS_VALUE(city) as city, COUNT(*) AS count FROM USER_CLICKSTREAM WINDOW TUMBLING (size 60 second) GROUP BY username, ip, city HAVING COUNT(*) > 1; -- Send data to Elasticsearch CREATE SINK CONNECTOR analyzed_clickstream WITH ( 'connector.class' = 'ElasticsearchSink', 'name' = 'elasticsearch-connector', 'input.data.format' = 'JSON', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'USER_IP_ACTIVITY, ENRICHED_ERROR_CODES_COUNT', 'connection.url' = '<elasticsearch-URI>', 'connection.user' = '<elasticsearch-username>', 'connection.password' = '<elasticsearch-password>', 'type.name' = 'type.name=kafkaconnect', 'key.ignore' = 'true', 'schema.ignore' = 'true' ); Launch Step-by-Step Setup your Environment Sign up for Confluent Cloud , a fully-managed Apache Kafka service. After you log in to Confluent Cloud, click on Add cloud environment and name the environment learn-kafka . Using a new environment keeps your learning resources separate from your other Confluent Cloud resources. From the Billing & payment section in the Menu, apply the promo code C50INTEG to receive an additional $50 free usage on Confluent Cloud ( details ). Click on LEARN and follow the instructions to launch a Kafka cluster and to enable Schema Registry. Read the data in This recipe creates simulated data with the Datagen connector. -- Stream of HTTP codes CREATE SOURCE CONNECTOR datagen_clickstream_codes WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_codes', 'quickstart' = 'clickstream_codes', 'maxInterval' = '20', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of users CREATE SOURCE CONNECTOR datagen_clickstream_users WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_users', 'quickstart' = 'clickstream_users', 'maxInterval' = '10', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of per-user session information CREATE SOURCE CONNECTOR datagen_clickstream WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream', 'quickstart' = 'clickstream', 'maxInterval' = '30', 'format' = 'json'); Optional: to simulate a real-world scenario where user sessions aren't just always open but do close after some time, you can pause and resume the DATAGEN_CLICKSTREAM connector. Run stream processing app Now you can process the data in a variety of ways, by enriching the clickstream data with user information, analyze errors, aggregate data into windows of time, etc. --------------------------------------------------------------------------------------------------- -- Register sources: --------------------------------------------------------------------------------------------------- -- stream of user clicks: CREATE STREAM clickstream ( _time bigint, time varchar, ip varchar, request varchar, status int, userid int, bytes bigint, agent varchar ) with ( kafka_topic = 'clickstream', value_format = 'json' ); -- error code lookup table: CREATE TABLE clickstream_codes ( code int primary key, definition varchar ) with ( kafka_topic = 'clickstream_codes', value_format = 'json' ); -- users lookup table: CREATE TABLE WEB_USERS ( user_id int primary key, registered_At BIGINT, username varchar, first_name varchar, last_name varchar, city varchar, level varchar ) with ( kafka_topic = 'clickstream_users', value_format = 'json' ); --------------------------------------------------------------------------------------------------- -- Build materialized stream views: --------------------------------------------------------------------------------------------------- -- enrich click-stream with more user information: CREATE STREAM USER_CLICKSTREAM AS SELECT userid, u.username, ip, u.city, request, status, bytes FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id; -- Enrich click-stream with more information about error codes: CREATE STREAM ENRICHED_ERROR_CODES AS SELECT code, definition FROM clickstream LEFT JOIN clickstream_codes ON clickstream.status = clickstream_codes.code; --------------------------------------------------------------------------------------------------- -- Build materialized table views: --------------------------------------------------------------------------------------------------- -- Per-userId tables ----------------------------------------------------------------------------- -- Table of events per minute for each user: CREATE table events_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS events FROM clickstream window TUMBLING (size 60 second) GROUP BY userid; -- Table of html pages per minute for each user: CREATE TABLE pages_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS pages FROM clickstream WINDOW HOPPING (size 60 second, advance by 5 second) WHERE request like '%html%' GROUP BY userid; -- Per-username tables ---------------------------------------------------------------------------- -- User sessions table - 30 seconds of inactivity expires the session -- Table counts number of events within the session CREATE TABLE CLICK_USER_SESSIONS AS SELECT username as K, AS_VALUE(username) as username, WINDOWEND as EVENT_TS, count(*) AS events FROM USER_CLICKSTREAM window SESSION (30 second) GROUP BY username; -- Per-status tables ------------------------------------------------------------------------------ -- number of errors per min, using 'HAVING' Filter to show ERROR codes > 400 where count > 5 CREATE TABLE ERRORS_PER_MIN_ALERT AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 20 second) WHERE status > 400 GROUP BY status HAVING count(*) > 5 AND count(*) is not NULL; -- number of errors per min CREATE TABLE ERRORS_PER_MIN AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 5 second) WHERE status > 400 GROUP BY status; -- Per-error code tables -------------------------------------------------------------------------- -- Enriched error codes table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE ENRICHED_ERROR_CODES_COUNT WITH (key_format='json') AS SELECT code as k1, definition as K2, AS_VALUE(code) as code, WINDOWSTART as EVENT_TS, AS_VALUE(definition) as definition, COUNT(*) AS count FROM ENRICHED_ERROR_CODES WINDOW TUMBLING (size 30 second) GROUP BY code, definition HAVING COUNT(*) > 1; -- Enriched user details table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE USER_IP_ACTIVITY WITH (key_format='json') AS SELECT username as k1, ip as k2, city as k3, AS_VALUE(username) as username, WINDOWSTART as EVENT_TS, AS_VALUE(ip) as ip, AS_VALUE(city) as city, COUNT(*) AS count FROM USER_CLICKSTREAM WINDOW TUMBLING (size 60 second) GROUP BY username, ip, city HAVING COUNT(*) > 1; Write the data out After processing the data, send it to Elasticsearch. -- Send data to Elasticsearch CREATE SINK CONNECTOR analyzed_clickstream WITH ( 'connector.class' = 'ElasticsearchSink', 'name' = 'elasticsearch-connector', 'input.data.format' = 'JSON', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'USER_IP_ACTIVITY, ENRICHED_ERROR_CODES_COUNT', 'connection.url' = '<elasticsearch-URI>', 'connection.user' = '<elasticsearch-username>', 'connection.password' = '<elasticsearch-password>', 'type.name' = 'type.name=kafkaconnect', 'key.ignore' = 'true', 'schema.ignore' = 'true' );","title":"Clickstream Data Analysis"},{"location":"customer-360/clickstream/#clickstream-data-analysis","text":"","title":"Clickstream Data Analysis"},{"location":"customer-360/clickstream/#what-is-it","text":"Analyzing clickstream data enables businesses to understand the behavior of its online users, for example: User activity over a given time frame: how many webpages are users viewing Requests that end in error, over a given threshold (e.g., 404 HTTP codes) Where the requests are coming from geographically in a given window of time How long users are interacting with the site (user sessions)","title":"What is it?"},{"location":"customer-360/clickstream/#cut-to-the-code","text":"-- Stream of HTTP codes CREATE SOURCE CONNECTOR datagen_clickstream_codes WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_codes', 'quickstart' = 'clickstream_codes', 'maxInterval' = '20', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of users CREATE SOURCE CONNECTOR datagen_clickstream_users WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_users', 'quickstart' = 'clickstream_users', 'maxInterval' = '10', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of per-user session information CREATE SOURCE CONNECTOR datagen_clickstream WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream', 'quickstart' = 'clickstream', 'maxInterval' = '30', 'format' = 'json'); --------------------------------------------------------------------------------------------------- -- Register sources: --------------------------------------------------------------------------------------------------- -- stream of user clicks: CREATE STREAM clickstream ( _time bigint, time varchar, ip varchar, request varchar, status int, userid int, bytes bigint, agent varchar ) with ( kafka_topic = 'clickstream', value_format = 'json' ); -- error code lookup table: CREATE TABLE clickstream_codes ( code int primary key, definition varchar ) with ( kafka_topic = 'clickstream_codes', value_format = 'json' ); -- users lookup table: CREATE TABLE WEB_USERS ( user_id int primary key, registered_At BIGINT, username varchar, first_name varchar, last_name varchar, city varchar, level varchar ) with ( kafka_topic = 'clickstream_users', value_format = 'json' ); --------------------------------------------------------------------------------------------------- -- Build materialized stream views: --------------------------------------------------------------------------------------------------- -- enrich click-stream with more user information: CREATE STREAM USER_CLICKSTREAM AS SELECT userid, u.username, ip, u.city, request, status, bytes FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id; -- Enrich click-stream with more information about error codes: CREATE STREAM ENRICHED_ERROR_CODES AS SELECT code, definition FROM clickstream LEFT JOIN clickstream_codes ON clickstream.status = clickstream_codes.code; --------------------------------------------------------------------------------------------------- -- Build materialized table views: --------------------------------------------------------------------------------------------------- -- Per-userId tables ----------------------------------------------------------------------------- -- Table of events per minute for each user: CREATE table events_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS events FROM clickstream window TUMBLING (size 60 second) GROUP BY userid; -- Table of html pages per minute for each user: CREATE TABLE pages_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS pages FROM clickstream WINDOW HOPPING (size 60 second, advance by 5 second) WHERE request like '%html%' GROUP BY userid; -- Per-username tables ---------------------------------------------------------------------------- -- User sessions table - 30 seconds of inactivity expires the session -- Table counts number of events within the session CREATE TABLE CLICK_USER_SESSIONS AS SELECT username as K, AS_VALUE(username) as username, WINDOWEND as EVENT_TS, count(*) AS events FROM USER_CLICKSTREAM window SESSION (30 second) GROUP BY username; -- Per-status tables ------------------------------------------------------------------------------ -- number of errors per min, using 'HAVING' Filter to show ERROR codes > 400 where count > 5 CREATE TABLE ERRORS_PER_MIN_ALERT AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 20 second) WHERE status > 400 GROUP BY status HAVING count(*) > 5 AND count(*) is not NULL; -- number of errors per min CREATE TABLE ERRORS_PER_MIN AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 5 second) WHERE status > 400 GROUP BY status; -- Per-error code tables -------------------------------------------------------------------------- -- Enriched error codes table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE ENRICHED_ERROR_CODES_COUNT WITH (key_format='json') AS SELECT code as k1, definition as K2, AS_VALUE(code) as code, WINDOWSTART as EVENT_TS, AS_VALUE(definition) as definition, COUNT(*) AS count FROM ENRICHED_ERROR_CODES WINDOW TUMBLING (size 30 second) GROUP BY code, definition HAVING COUNT(*) > 1; -- Enriched user details table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE USER_IP_ACTIVITY WITH (key_format='json') AS SELECT username as k1, ip as k2, city as k3, AS_VALUE(username) as username, WINDOWSTART as EVENT_TS, AS_VALUE(ip) as ip, AS_VALUE(city) as city, COUNT(*) AS count FROM USER_CLICKSTREAM WINDOW TUMBLING (size 60 second) GROUP BY username, ip, city HAVING COUNT(*) > 1; -- Send data to Elasticsearch CREATE SINK CONNECTOR analyzed_clickstream WITH ( 'connector.class' = 'ElasticsearchSink', 'name' = 'elasticsearch-connector', 'input.data.format' = 'JSON', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'USER_IP_ACTIVITY, ENRICHED_ERROR_CODES_COUNT', 'connection.url' = '<elasticsearch-URI>', 'connection.user' = '<elasticsearch-username>', 'connection.password' = '<elasticsearch-password>', 'type.name' = 'type.name=kafkaconnect', 'key.ignore' = 'true', 'schema.ignore' = 'true' );","title":"Cut to the code"},{"location":"customer-360/clickstream/#launch-step-by-step","text":"","title":"Launch Step-by-Step"},{"location":"customer-360/clickstream/#setup-your-environment","text":"Sign up for Confluent Cloud , a fully-managed Apache Kafka service. After you log in to Confluent Cloud, click on Add cloud environment and name the environment learn-kafka . Using a new environment keeps your learning resources separate from your other Confluent Cloud resources. From the Billing & payment section in the Menu, apply the promo code C50INTEG to receive an additional $50 free usage on Confluent Cloud ( details ). Click on LEARN and follow the instructions to launch a Kafka cluster and to enable Schema Registry.","title":"Setup your Environment"},{"location":"customer-360/clickstream/#read-the-data-in","text":"This recipe creates simulated data with the Datagen connector. -- Stream of HTTP codes CREATE SOURCE CONNECTOR datagen_clickstream_codes WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_codes', 'quickstart' = 'clickstream_codes', 'maxInterval' = '20', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of users CREATE SOURCE CONNECTOR datagen_clickstream_users WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_users', 'quickstart' = 'clickstream_users', 'maxInterval' = '10', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of per-user session information CREATE SOURCE CONNECTOR datagen_clickstream WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream', 'quickstart' = 'clickstream', 'maxInterval' = '30', 'format' = 'json'); Optional: to simulate a real-world scenario where user sessions aren't just always open but do close after some time, you can pause and resume the DATAGEN_CLICKSTREAM connector.","title":"Read the data in"},{"location":"customer-360/clickstream/#run-stream-processing-app","text":"Now you can process the data in a variety of ways, by enriching the clickstream data with user information, analyze errors, aggregate data into windows of time, etc. --------------------------------------------------------------------------------------------------- -- Register sources: --------------------------------------------------------------------------------------------------- -- stream of user clicks: CREATE STREAM clickstream ( _time bigint, time varchar, ip varchar, request varchar, status int, userid int, bytes bigint, agent varchar ) with ( kafka_topic = 'clickstream', value_format = 'json' ); -- error code lookup table: CREATE TABLE clickstream_codes ( code int primary key, definition varchar ) with ( kafka_topic = 'clickstream_codes', value_format = 'json' ); -- users lookup table: CREATE TABLE WEB_USERS ( user_id int primary key, registered_At BIGINT, username varchar, first_name varchar, last_name varchar, city varchar, level varchar ) with ( kafka_topic = 'clickstream_users', value_format = 'json' ); --------------------------------------------------------------------------------------------------- -- Build materialized stream views: --------------------------------------------------------------------------------------------------- -- enrich click-stream with more user information: CREATE STREAM USER_CLICKSTREAM AS SELECT userid, u.username, ip, u.city, request, status, bytes FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id; -- Enrich click-stream with more information about error codes: CREATE STREAM ENRICHED_ERROR_CODES AS SELECT code, definition FROM clickstream LEFT JOIN clickstream_codes ON clickstream.status = clickstream_codes.code; --------------------------------------------------------------------------------------------------- -- Build materialized table views: --------------------------------------------------------------------------------------------------- -- Per-userId tables ----------------------------------------------------------------------------- -- Table of events per minute for each user: CREATE table events_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS events FROM clickstream window TUMBLING (size 60 second) GROUP BY userid; -- Table of html pages per minute for each user: CREATE TABLE pages_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS pages FROM clickstream WINDOW HOPPING (size 60 second, advance by 5 second) WHERE request like '%html%' GROUP BY userid; -- Per-username tables ---------------------------------------------------------------------------- -- User sessions table - 30 seconds of inactivity expires the session -- Table counts number of events within the session CREATE TABLE CLICK_USER_SESSIONS AS SELECT username as K, AS_VALUE(username) as username, WINDOWEND as EVENT_TS, count(*) AS events FROM USER_CLICKSTREAM window SESSION (30 second) GROUP BY username; -- Per-status tables ------------------------------------------------------------------------------ -- number of errors per min, using 'HAVING' Filter to show ERROR codes > 400 where count > 5 CREATE TABLE ERRORS_PER_MIN_ALERT AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 20 second) WHERE status > 400 GROUP BY status HAVING count(*) > 5 AND count(*) is not NULL; -- number of errors per min CREATE TABLE ERRORS_PER_MIN AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 5 second) WHERE status > 400 GROUP BY status; -- Per-error code tables -------------------------------------------------------------------------- -- Enriched error codes table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE ENRICHED_ERROR_CODES_COUNT WITH (key_format='json') AS SELECT code as k1, definition as K2, AS_VALUE(code) as code, WINDOWSTART as EVENT_TS, AS_VALUE(definition) as definition, COUNT(*) AS count FROM ENRICHED_ERROR_CODES WINDOW TUMBLING (size 30 second) GROUP BY code, definition HAVING COUNT(*) > 1; -- Enriched user details table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE USER_IP_ACTIVITY WITH (key_format='json') AS SELECT username as k1, ip as k2, city as k3, AS_VALUE(username) as username, WINDOWSTART as EVENT_TS, AS_VALUE(ip) as ip, AS_VALUE(city) as city, COUNT(*) AS count FROM USER_CLICKSTREAM WINDOW TUMBLING (size 60 second) GROUP BY username, ip, city HAVING COUNT(*) > 1;","title":"Run stream processing app"},{"location":"customer-360/clickstream/#write-the-data-out","text":"After processing the data, send it to Elasticsearch. -- Send data to Elasticsearch CREATE SINK CONNECTOR analyzed_clickstream WITH ( 'connector.class' = 'ElasticsearchSink', 'name' = 'elasticsearch-connector', 'input.data.format' = 'JSON', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'USER_IP_ACTIVITY, ENRICHED_ERROR_CODES_COUNT', 'connection.url' = '<elasticsearch-URI>', 'connection.user' = '<elasticsearch-username>', 'connection.password' = '<elasticsearch-password>', 'type.name' = 'type.name=kafkaconnect', 'key.ignore' = 'true', 'schema.ignore' = 'true' );","title":"Write the data out"},{"location":"fin-serv/","text":"Description These recipes ...","title":"Description"},{"location":"fin-serv/#description","text":"These recipes ...","title":"Description"},{"location":"healthcare/","text":"Description These recipes ...","title":"Description"},{"location":"healthcare/#description","text":"These recipes ...","title":"Description"},{"location":"retail/","text":"Description These recipes ...","title":"Description"},{"location":"retail/#description","text":"These recipes ...","title":"Description"},{"location":"retail/inventory/","text":"Real-time Inventory What is it? TODO Cut to the code -- Create stream of inventory CREATE STREAM inventory_stream ( id STRING key, item STRING, qty INTEGER, price DOUBLE, balance INTEGER) with (VALUE_FORMAT='json', KAFKA_TOPIC='inventory'); -- add some mock data insert into inventory_stream (id, item, qty, price) values ('1', 'Apple Magic Mouse 2', 10, 99); insert into inventory_stream (id, item, qty, price) values ('2', 'iPhoneX', 25, 999); insert into inventory_stream (id, item, qty, price) values ('3', 'MacBookPro13', 100, 1799); insert into inventory_stream (id, item, qty, price) values ('4', 'iPad4', 20, 340); insert into inventory_stream (id, item, qty, price) values ('5', 'Apple Pencil', 10, 79); insert into inventory_stream (id, item, qty, price) values ('5', 'PhoneX', 10, 899); insert into inventory_stream (id, item, qty, price) values ('4', 'iPad4', -20, 399); insert into inventory_stream (id, item, qty, price) values ('3', 'MacBookPro13', 10, 1899); insert into inventory_stream (id, item, qty, price) values ('4', 'iPad4', 20, 399); --------------------------------------------------------------------------------------------------- -- Create stateful table with up-to-date information of inventory availability --------------------------------------------------------------------------------------------------- CREATE TABLE inventory_stream_table WITH (kafka_topic='inventory_table') AS SELECT item, SUM(qty) AS item_qty FROM inventory_stream GROUP BY item EMIT CHANGES; Launch Step-by-Step Setup your Environment Sign up for Confluent Cloud , a fully-managed Apache Kafka service. After you log in to Confluent Cloud, click on Add cloud environment and name the environment learn-kafka . Using a new environment keeps your learning resources separate from your other Confluent Cloud resources. From the Billing & payment section in the Menu, apply the promo code C50INTEG to receive an additional $50 free usage on Confluent Cloud ( details ). Click on LEARN and follow the instructions to launch a Kafka cluster and to enable Schema Registry. Read the data in TODO -- Create stream of inventory CREATE STREAM inventory_stream ( id STRING key, item STRING, qty INTEGER, price DOUBLE, balance INTEGER) with (VALUE_FORMAT='json', KAFKA_TOPIC='inventory'); -- add some mock data insert into inventory_stream (id, item, qty, price) values ('1', 'Apple Magic Mouse 2', 10, 99); insert into inventory_stream (id, item, qty, price) values ('2', 'iPhoneX', 25, 999); insert into inventory_stream (id, item, qty, price) values ('3', 'MacBookPro13', 100, 1799); insert into inventory_stream (id, item, qty, price) values ('4', 'iPad4', 20, 340); insert into inventory_stream (id, item, qty, price) values ('5', 'Apple Pencil', 10, 79); insert into inventory_stream (id, item, qty, price) values ('5', 'PhoneX', 10, 899); insert into inventory_stream (id, item, qty, price) values ('4', 'iPad4', -20, 399); insert into inventory_stream (id, item, qty, price) values ('3', 'MacBookPro13', 10, 1899); insert into inventory_stream (id, item, qty, price) values ('4', 'iPad4', 20, 399); Run stream processing app TODO --------------------------------------------------------------------------------------------------- -- Create stateful table with up-to-date information of inventory availability --------------------------------------------------------------------------------------------------- CREATE TABLE inventory_stream_table WITH (kafka_topic='inventory_table') AS SELECT item, SUM(qty) AS item_qty FROM inventory_stream GROUP BY item EMIT CHANGES;","title":"Real-time Inventory"},{"location":"retail/inventory/#real-time-inventory","text":"","title":"Real-time Inventory"},{"location":"retail/inventory/#what-is-it","text":"TODO","title":"What is it?"},{"location":"retail/inventory/#cut-to-the-code","text":"-- Create stream of inventory CREATE STREAM inventory_stream ( id STRING key, item STRING, qty INTEGER, price DOUBLE, balance INTEGER) with (VALUE_FORMAT='json', KAFKA_TOPIC='inventory'); -- add some mock data insert into inventory_stream (id, item, qty, price) values ('1', 'Apple Magic Mouse 2', 10, 99); insert into inventory_stream (id, item, qty, price) values ('2', 'iPhoneX', 25, 999); insert into inventory_stream (id, item, qty, price) values ('3', 'MacBookPro13', 100, 1799); insert into inventory_stream (id, item, qty, price) values ('4', 'iPad4', 20, 340); insert into inventory_stream (id, item, qty, price) values ('5', 'Apple Pencil', 10, 79); insert into inventory_stream (id, item, qty, price) values ('5', 'PhoneX', 10, 899); insert into inventory_stream (id, item, qty, price) values ('4', 'iPad4', -20, 399); insert into inventory_stream (id, item, qty, price) values ('3', 'MacBookPro13', 10, 1899); insert into inventory_stream (id, item, qty, price) values ('4', 'iPad4', 20, 399); --------------------------------------------------------------------------------------------------- -- Create stateful table with up-to-date information of inventory availability --------------------------------------------------------------------------------------------------- CREATE TABLE inventory_stream_table WITH (kafka_topic='inventory_table') AS SELECT item, SUM(qty) AS item_qty FROM inventory_stream GROUP BY item EMIT CHANGES;","title":"Cut to the code"},{"location":"retail/inventory/#launch-step-by-step","text":"","title":"Launch Step-by-Step"},{"location":"retail/inventory/#setup-your-environment","text":"Sign up for Confluent Cloud , a fully-managed Apache Kafka service. After you log in to Confluent Cloud, click on Add cloud environment and name the environment learn-kafka . Using a new environment keeps your learning resources separate from your other Confluent Cloud resources. From the Billing & payment section in the Menu, apply the promo code C50INTEG to receive an additional $50 free usage on Confluent Cloud ( details ). Click on LEARN and follow the instructions to launch a Kafka cluster and to enable Schema Registry.","title":"Setup your Environment"},{"location":"retail/inventory/#read-the-data-in","text":"TODO -- Create stream of inventory CREATE STREAM inventory_stream ( id STRING key, item STRING, qty INTEGER, price DOUBLE, balance INTEGER) with (VALUE_FORMAT='json', KAFKA_TOPIC='inventory'); -- add some mock data insert into inventory_stream (id, item, qty, price) values ('1', 'Apple Magic Mouse 2', 10, 99); insert into inventory_stream (id, item, qty, price) values ('2', 'iPhoneX', 25, 999); insert into inventory_stream (id, item, qty, price) values ('3', 'MacBookPro13', 100, 1799); insert into inventory_stream (id, item, qty, price) values ('4', 'iPad4', 20, 340); insert into inventory_stream (id, item, qty, price) values ('5', 'Apple Pencil', 10, 79); insert into inventory_stream (id, item, qty, price) values ('5', 'PhoneX', 10, 899); insert into inventory_stream (id, item, qty, price) values ('4', 'iPad4', -20, 399); insert into inventory_stream (id, item, qty, price) values ('3', 'MacBookPro13', 10, 1899); insert into inventory_stream (id, item, qty, price) values ('4', 'iPad4', 20, 399);","title":"Read the data in"},{"location":"retail/inventory/#run-stream-processing-app","text":"TODO --------------------------------------------------------------------------------------------------- -- Create stateful table with up-to-date information of inventory availability --------------------------------------------------------------------------------------------------- CREATE TABLE inventory_stream_table WITH (kafka_topic='inventory_table') AS SELECT item, SUM(qty) AS item_qty FROM inventory_stream GROUP BY item EMIT CHANGES;","title":"Run stream processing app"},{"location":"security/","text":"Description These recipes ...","title":"Description"},{"location":"security/#description","text":"These recipes ...","title":"Description"},{"location":"security/ssh-attack/","text":"Detecting and Analyzing SSH Attacks What is it? There are lots of ways SSH can be abused but one of the most straightforward ways to detect a problem is to monitor for rejected logins. This recipe tracks Syslog data and streams out pairs of usernames and IPs of bad logins. Cut to the code Run the Syslog source connector to push syslog data into Confluent Cloud into a Kafka topic called syslog . Then proceed with ksqlDB to process the Syslog messages. -- Define Kafka parameters DEFINE topic = 'syslog'; -- Extract relevant fields from log messages CREATE OR REPLACE STREAM `syslog` WITH ( KAFKA_TOPIC = '${topic}', VALUE_FORMAT = 'avro' ); CREATE STREAM `by_facility` AS SELECT TIMESTAMPTOSTRING(TIMESTAMP, 'yyyy-MM-dd HH:mm:ss') AS SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS FROM `syslog` PARTITION BY FACILITY EMIT CHANGES; -- Flag events with invalid users, and enrich with a new field 'FACILITY_DESCRIPTION' CREATE STREAM `invalid_users` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS, CASE WHEN FACILITY = 0 THEN 'kernel messages' WHEN FACILITY = 1 THEN 'user-level messages' WHEN FACILITY = 2 THEN 'mail system' WHEN FACILITY = 3 THEN 'system daemons' WHEN FACILITY = 4 THEN 'security/authorization messages' WHEN FACILITY = 5 THEN 'messages generated internally by syslogd' WHEN FACILITY = 6 THEN 'line printer subsystem' ELSE '<unknown>' END AS FACILITY_DESCRIPTION FROM `by_facility` WHERE MESSAGE LIKE 'Invalid user%' EMIT CHANGES; -- Create actionable stream of SSH attacks, enriched with user and IP CREATE STREAM `ssh_attacks` AS SELECT SYSLOG_TIMESTAMP, FACILITY, HOST, FACILITY_DESCRIPTION, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[1] AS ATTACK_USER, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[2] AS ATTACK_IP FROM `invalid_users` EMIT CHANGES; Launch Step-by-Step Setup your Environment Sign up for Confluent Cloud , a fully-managed Apache Kafka service. After you log in to Confluent Cloud, click on Add cloud environment and name the environment learn-kafka . Using a new environment keeps your learning resources separate from your other Confluent Cloud resources. From the Billing & payment section in the Menu, apply the promo code C50INTEG to receive an additional $50 free usage on Confluent Cloud ( details ). Click on LEARN and follow the instructions to launch a Kafka cluster and to enable Schema Registry. Read the data in Create a file called Dockerfile to bundle a connect worker with kafka-connect-syslog : # # Copyright 2020 Confluent Inc. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. FROM confluentinc/cp-server-connect-base:6.2.0 ENV CONNECT_PLUGIN_PATH=\"/usr/share/java,/usr/share/confluent-hub-components\" USER root COPY --chown=appuser:appuser include/etc/confluent/docker /etc/confluent/docker USER appuser ARG CONNECTOR_OWNER=confluentinc ARG CONNECTOR_NAME ARG CONNECTOR_VERSION RUN confluent-hub install --no-prompt confluent-hub install confluentinc/kafka-connect-syslog:1.3.4 CMD [\"/etc/confluent/docker/run\"] Build the custom Docker image with the command: docker build \\ -t localbuild/connect_distributed_with_syslog:1.3.4 \\ -f Dockerfile . Create a file called docker-compose.yml with the following content, substituting your Confluent Cloud connection information: --- version: '2' services: connect: image: localbuild/connect_distributed_with_syslog:1.3.4 hostname: connect container_name: connect ports: - \"8083:8083\" environment: CONNECT_BOOTSTRAP_SERVERS: $BOOTSTRAP_SERVERS CONNECT_REST_PORT: 8083 CONNECT_GROUP_ID: \"connect\" CONNECT_CONFIG_STORAGE_TOPIC: demo-connect-configs CONNECT_OFFSET_STORAGE_TOPIC: demo-connect-offsets CONNECT_STATUS_STORAGE_TOPIC: demo-connect-status CONNECT_REPLICATION_FACTOR: 3 CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3 CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3 CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3 CONNECT_KEY_CONVERTER: \"org.apache.kafka.connect.storage.StringConverter\" CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\" CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"true\" CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: $SCHEMA_REGISTRY_URL CONNECT_VALUE_CONVERTER_BASIC_AUTH_CREDENTIALS_SOURCE: $BASIC_AUTH_CREDENTIALS_SOURCE CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_BASIC_AUTH_USER_INFO: $SCHEMA_REGISTRY_BASIC_AUTH_USER_INFO CONNECT_REST_ADVERTISED_HOST_NAME: \"connect\" CONNECT_PLUGIN_PATH: \"/usr/share/java,/usr/share/confluent-hub-components\" CONNECT_LOG4J_ROOT_LOGLEVEL: INFO CONNECT_LOG4J_LOGGERS: org.reflections=ERROR # CLASSPATH required due to CC-2422 CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-6.2.0.jar # Connect worker CONNECT_SECURITY_PROTOCOL: SASL_SSL CONNECT_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_SASL_MECHANISM: PLAIN # Connect producer CONNECT_PRODUCER_SECURITY_PROTOCOL: SASL_SSL CONNECT_PRODUCER_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_PRODUCER_SASL_MECHANISM: PLAIN CONNECT_PRODUCER_INTERCEPTOR_CLASSES: \"io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor\" CONNECT_PRODUCER_CONFLUENT_MONITORING_INTERCEPTOR_SECURITY_PROTOCOL: SASL_SSL CONNECT_PRODUCER_CONFLUENT_MONITORING_INTERCEPTOR_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_PRODUCER_CONFLUENT_MONITORING_INTERCEPTOR_SASL_MECHANISM: PLAIN # Connect consumer CONNECT_CONSUMER_SECURITY_PROTOCOL: SASL_SSL CONNECT_CONSUMER_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_CONSUMER_SASL_MECHANISM: PLAIN CONNECT_CONSUMER_INTERCEPTOR_CLASSES: \"io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor\" CONNECT_CONSUMER_CONFLUENT_MONITORING_INTERCEPTOR_SECURITY_PROTOCOL: SASL_SSL CONNECT_CONSUMER_CONFLUENT_MONITORING_INTERCEPTOR_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_CONSUMER_CONFLUENT_MONITORING_INTERCEPTOR_SASL_MECHANISM: PLAIN Run the container with: docker-compose up -d Now you should have Syslog messages being written to the topic syslog in Confluent Cloud. Run stream processing app TODO -- Define Kafka parameters DEFINE topic = 'syslog'; -- Extract relevant fields from log messages CREATE OR REPLACE STREAM `syslog` WITH ( KAFKA_TOPIC = '${topic}', VALUE_FORMAT = 'avro' ); CREATE STREAM `by_facility` AS SELECT TIMESTAMPTOSTRING(TIMESTAMP, 'yyyy-MM-dd HH:mm:ss') AS SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS FROM `syslog` PARTITION BY FACILITY EMIT CHANGES; -- Flag events with invalid users, and enrich with a new field 'FACILITY_DESCRIPTION' CREATE STREAM `invalid_users` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS, CASE WHEN FACILITY = 0 THEN 'kernel messages' WHEN FACILITY = 1 THEN 'user-level messages' WHEN FACILITY = 2 THEN 'mail system' WHEN FACILITY = 3 THEN 'system daemons' WHEN FACILITY = 4 THEN 'security/authorization messages' WHEN FACILITY = 5 THEN 'messages generated internally by syslogd' WHEN FACILITY = 6 THEN 'line printer subsystem' ELSE '<unknown>' END AS FACILITY_DESCRIPTION FROM `by_facility` WHERE MESSAGE LIKE 'Invalid user%' EMIT CHANGES; -- Create actionable stream of SSH attacks, enriched with user and IP CREATE STREAM `ssh_attacks` AS SELECT SYSLOG_TIMESTAMP, FACILITY, HOST, FACILITY_DESCRIPTION, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[1] AS ATTACK_USER, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[2] AS ATTACK_IP FROM `invalid_users` EMIT CHANGES;","title":"Detecting and Analyzing SSH Attacks"},{"location":"security/ssh-attack/#detecting-and-analyzing-ssh-attacks","text":"","title":"Detecting and Analyzing SSH Attacks"},{"location":"security/ssh-attack/#what-is-it","text":"There are lots of ways SSH can be abused but one of the most straightforward ways to detect a problem is to monitor for rejected logins. This recipe tracks Syslog data and streams out pairs of usernames and IPs of bad logins.","title":"What is it?"},{"location":"security/ssh-attack/#cut-to-the-code","text":"Run the Syslog source connector to push syslog data into Confluent Cloud into a Kafka topic called syslog . Then proceed with ksqlDB to process the Syslog messages. -- Define Kafka parameters DEFINE topic = 'syslog'; -- Extract relevant fields from log messages CREATE OR REPLACE STREAM `syslog` WITH ( KAFKA_TOPIC = '${topic}', VALUE_FORMAT = 'avro' ); CREATE STREAM `by_facility` AS SELECT TIMESTAMPTOSTRING(TIMESTAMP, 'yyyy-MM-dd HH:mm:ss') AS SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS FROM `syslog` PARTITION BY FACILITY EMIT CHANGES; -- Flag events with invalid users, and enrich with a new field 'FACILITY_DESCRIPTION' CREATE STREAM `invalid_users` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS, CASE WHEN FACILITY = 0 THEN 'kernel messages' WHEN FACILITY = 1 THEN 'user-level messages' WHEN FACILITY = 2 THEN 'mail system' WHEN FACILITY = 3 THEN 'system daemons' WHEN FACILITY = 4 THEN 'security/authorization messages' WHEN FACILITY = 5 THEN 'messages generated internally by syslogd' WHEN FACILITY = 6 THEN 'line printer subsystem' ELSE '<unknown>' END AS FACILITY_DESCRIPTION FROM `by_facility` WHERE MESSAGE LIKE 'Invalid user%' EMIT CHANGES; -- Create actionable stream of SSH attacks, enriched with user and IP CREATE STREAM `ssh_attacks` AS SELECT SYSLOG_TIMESTAMP, FACILITY, HOST, FACILITY_DESCRIPTION, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[1] AS ATTACK_USER, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[2] AS ATTACK_IP FROM `invalid_users` EMIT CHANGES;","title":"Cut to the code"},{"location":"security/ssh-attack/#launch-step-by-step","text":"","title":"Launch Step-by-Step"},{"location":"security/ssh-attack/#setup-your-environment","text":"Sign up for Confluent Cloud , a fully-managed Apache Kafka service. After you log in to Confluent Cloud, click on Add cloud environment and name the environment learn-kafka . Using a new environment keeps your learning resources separate from your other Confluent Cloud resources. From the Billing & payment section in the Menu, apply the promo code C50INTEG to receive an additional $50 free usage on Confluent Cloud ( details ). Click on LEARN and follow the instructions to launch a Kafka cluster and to enable Schema Registry.","title":"Setup your Environment"},{"location":"security/ssh-attack/#read-the-data-in","text":"Create a file called Dockerfile to bundle a connect worker with kafka-connect-syslog : # # Copyright 2020 Confluent Inc. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. FROM confluentinc/cp-server-connect-base:6.2.0 ENV CONNECT_PLUGIN_PATH=\"/usr/share/java,/usr/share/confluent-hub-components\" USER root COPY --chown=appuser:appuser include/etc/confluent/docker /etc/confluent/docker USER appuser ARG CONNECTOR_OWNER=confluentinc ARG CONNECTOR_NAME ARG CONNECTOR_VERSION RUN confluent-hub install --no-prompt confluent-hub install confluentinc/kafka-connect-syslog:1.3.4 CMD [\"/etc/confluent/docker/run\"] Build the custom Docker image with the command: docker build \\ -t localbuild/connect_distributed_with_syslog:1.3.4 \\ -f Dockerfile . Create a file called docker-compose.yml with the following content, substituting your Confluent Cloud connection information: --- version: '2' services: connect: image: localbuild/connect_distributed_with_syslog:1.3.4 hostname: connect container_name: connect ports: - \"8083:8083\" environment: CONNECT_BOOTSTRAP_SERVERS: $BOOTSTRAP_SERVERS CONNECT_REST_PORT: 8083 CONNECT_GROUP_ID: \"connect\" CONNECT_CONFIG_STORAGE_TOPIC: demo-connect-configs CONNECT_OFFSET_STORAGE_TOPIC: demo-connect-offsets CONNECT_STATUS_STORAGE_TOPIC: demo-connect-status CONNECT_REPLICATION_FACTOR: 3 CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3 CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3 CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3 CONNECT_KEY_CONVERTER: \"org.apache.kafka.connect.storage.StringConverter\" CONNECT_VALUE_CONVERTER: \"io.confluent.connect.avro.AvroConverter\" CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: \"true\" CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: $SCHEMA_REGISTRY_URL CONNECT_VALUE_CONVERTER_BASIC_AUTH_CREDENTIALS_SOURCE: $BASIC_AUTH_CREDENTIALS_SOURCE CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_BASIC_AUTH_USER_INFO: $SCHEMA_REGISTRY_BASIC_AUTH_USER_INFO CONNECT_REST_ADVERTISED_HOST_NAME: \"connect\" CONNECT_PLUGIN_PATH: \"/usr/share/java,/usr/share/confluent-hub-components\" CONNECT_LOG4J_ROOT_LOGLEVEL: INFO CONNECT_LOG4J_LOGGERS: org.reflections=ERROR # CLASSPATH required due to CC-2422 CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-6.2.0.jar # Connect worker CONNECT_SECURITY_PROTOCOL: SASL_SSL CONNECT_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_SASL_MECHANISM: PLAIN # Connect producer CONNECT_PRODUCER_SECURITY_PROTOCOL: SASL_SSL CONNECT_PRODUCER_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_PRODUCER_SASL_MECHANISM: PLAIN CONNECT_PRODUCER_INTERCEPTOR_CLASSES: \"io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor\" CONNECT_PRODUCER_CONFLUENT_MONITORING_INTERCEPTOR_SECURITY_PROTOCOL: SASL_SSL CONNECT_PRODUCER_CONFLUENT_MONITORING_INTERCEPTOR_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_PRODUCER_CONFLUENT_MONITORING_INTERCEPTOR_SASL_MECHANISM: PLAIN # Connect consumer CONNECT_CONSUMER_SECURITY_PROTOCOL: SASL_SSL CONNECT_CONSUMER_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_CONSUMER_SASL_MECHANISM: PLAIN CONNECT_CONSUMER_INTERCEPTOR_CLASSES: \"io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor\" CONNECT_CONSUMER_CONFLUENT_MONITORING_INTERCEPTOR_SECURITY_PROTOCOL: SASL_SSL CONNECT_CONSUMER_CONFLUENT_MONITORING_INTERCEPTOR_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_CONSUMER_CONFLUENT_MONITORING_INTERCEPTOR_SASL_MECHANISM: PLAIN Run the container with: docker-compose up -d Now you should have Syslog messages being written to the topic syslog in Confluent Cloud.","title":"Read the data in"},{"location":"security/ssh-attack/#run-stream-processing-app","text":"TODO -- Define Kafka parameters DEFINE topic = 'syslog'; -- Extract relevant fields from log messages CREATE OR REPLACE STREAM `syslog` WITH ( KAFKA_TOPIC = '${topic}', VALUE_FORMAT = 'avro' ); CREATE STREAM `by_facility` AS SELECT TIMESTAMPTOSTRING(TIMESTAMP, 'yyyy-MM-dd HH:mm:ss') AS SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS FROM `syslog` PARTITION BY FACILITY EMIT CHANGES; -- Flag events with invalid users, and enrich with a new field 'FACILITY_DESCRIPTION' CREATE STREAM `invalid_users` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS, CASE WHEN FACILITY = 0 THEN 'kernel messages' WHEN FACILITY = 1 THEN 'user-level messages' WHEN FACILITY = 2 THEN 'mail system' WHEN FACILITY = 3 THEN 'system daemons' WHEN FACILITY = 4 THEN 'security/authorization messages' WHEN FACILITY = 5 THEN 'messages generated internally by syslogd' WHEN FACILITY = 6 THEN 'line printer subsystem' ELSE '<unknown>' END AS FACILITY_DESCRIPTION FROM `by_facility` WHERE MESSAGE LIKE 'Invalid user%' EMIT CHANGES; -- Create actionable stream of SSH attacks, enriched with user and IP CREATE STREAM `ssh_attacks` AS SELECT SYSLOG_TIMESTAMP, FACILITY, HOST, FACILITY_DESCRIPTION, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[1] AS ATTACK_USER, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[2] AS ATTACK_IP FROM `invalid_users` EMIT CHANGES;","title":"Run stream processing app"},{"location":"shared/ccloud_setup/","text":"Sign up for Confluent Cloud , a fully-managed Apache Kafka service. After you log in to Confluent Cloud, click on Add cloud environment and name the environment learn-kafka . Using a new environment keeps your learning resources separate from your other Confluent Cloud resources. From the Billing & payment section in the Menu, apply the promo code C50INTEG to receive an additional $50 free usage on Confluent Cloud ( details ). Click on LEARN and follow the instructions to launch a Kafka cluster and to enable Schema Registry.","title":"Ccloud setup"}]}
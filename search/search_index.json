{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ksqlDB Recipes This is a Work in Progress! Click in the left nav bar to see one of the recipes. How to run: the recipe explains the use case and shows how to implement in Confluent Cloud. It can also be run on Confluent Platform, but you'll first have to make appropriate modifications to the provided configuration files. Note: these recipes will be launched on Confluent Developer https://developer.confluent.io.","title":"ksqlDB Recipes"},{"location":"#welcome-to-ksqldb-recipes","text":"This is a Work in Progress! Click in the left nav bar to see one of the recipes. How to run: the recipe explains the use case and shows how to implement in Confluent Cloud. It can also be run on Confluent Platform, but you'll first have to make appropriate modifications to the provided configuration files. Note: these recipes will be launched on Confluent Developer https://developer.confluent.io.","title":"Welcome to ksqlDB Recipes"},{"location":"customer-360/clickstream/","text":"Clickstream Data Analysis What is it? Analyzing clickstream data enables businesses to understand the behavior of its online users, for example: User activity over a given time frame: how many webpages are users viewing Requests that end in error, over a given threshold (e.g., 404 HTTP codes) Where the requests are coming from geographically in a given window of time How long users are interacting with the site (user sessions) Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data. Read the data in Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). This recipe creates simulated data with the Datagen connector. -- Stream of users CREATE SOURCE CONNECTOR datagen_clickstream_users WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_users', 'quickstart' = 'clickstream_users', 'maxInterval' = '10', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of per-user session information CREATE SOURCE CONNECTOR datagen_clickstream WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream', 'quickstart' = 'clickstream', 'maxInterval' = '30', 'format' = 'json'); Optional: to simulate a real-world scenario where user sessions aren't just always open but do close after some time, you can pause and resume the DATAGEN_CLICKSTREAM connector. Run stream processing app Now you can process the data in a variety of ways, by enriching the clickstream data with user information, analyze errors, aggregate data into windows of time, etc. -- stream of user clicks: CREATE STREAM clickstream ( _time bigint, time varchar, ip varchar, request varchar, status int, userid int, bytes bigint, agent varchar ) WITH ( kafka_topic = 'clickstream', value_format = 'json', partitions = 6 ); -- users lookup table: CREATE TABLE WEB_USERS ( user_id int primary key, registered_At BIGINT, username varchar, first_name varchar, last_name varchar, city varchar, level varchar ) WITH ( kafka_topic = 'clickstream_users', value_format = 'json', partitions = 6 ); -- Build materialized stream views: -- enrich click-stream with more user information: CREATE STREAM USER_CLICKSTREAM AS SELECT userid, u.username, ip, u.city, request, status, bytes FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id; -- Build materialized table views: -- Table of html pages per minute for each user: CREATE TABLE pages_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS pages FROM clickstream WINDOW HOPPING (size 60 second, advance by 5 second) WHERE request like '%html%' GROUP BY userid; -- User sessions table - 30 seconds of inactivity expires the session -- Table counts number of events within the session CREATE TABLE CLICK_USER_SESSIONS AS SELECT username as K, AS_VALUE(username) as username, WINDOWEND as EVENT_TS, count(*) AS events FROM USER_CLICKSTREAM window SESSION (30 second) GROUP BY username; -- number of errors per min, using 'HAVING' Filter to show ERROR codes > 400 where count > 5 CREATE TABLE ERRORS_PER_MIN_ALERT AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 20 second) WHERE status > 400 GROUP BY status HAVING count(*) > 5 AND count(*) is not NULL; -- Enriched user details table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE USER_IP_ACTIVITY WITH (key_format='json') AS SELECT username as k1, ip as k2, city as k3, AS_VALUE(username) as username, WINDOWSTART as EVENT_TS, AS_VALUE(ip) as ip, AS_VALUE(city) as city, COUNT(*) AS count FROM USER_CLICKSTREAM WINDOW TUMBLING (size 60 second) GROUP BY username, ip, city HAVING COUNT(*) > 1; Write the data out After processing the data, send it to Elasticsearch. -- Send data to Elasticsearch CREATE SINK CONNECTOR analyzed_clickstream WITH ( 'connector.class' = 'ElasticsearchSink', 'name' = 'recipe-elasticsearch-analyzed_clickstream', 'input.data.format' = 'JSON', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'USER_IP_ACTIVITY, ERRORS_PER_MIN_ALERT', 'connection.url' = '<elasticsearch-URI>', 'connection.user' = '<elasticsearch-username>', 'connection.password' = '<elasticsearch-password>', 'type.name' = 'type.name=kafkaconnect', 'key.ignore' = 'true', 'schema.ignore' = 'true' ); Full ksqlDB Statements Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of users CREATE SOURCE CONNECTOR datagen_clickstream_users WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_users', 'quickstart' = 'clickstream_users', 'maxInterval' = '10', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of per-user session information CREATE SOURCE CONNECTOR datagen_clickstream WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream', 'quickstart' = 'clickstream', 'maxInterval' = '30', 'format' = 'json'); -- stream of user clicks: CREATE STREAM clickstream ( _time bigint, time varchar, ip varchar, request varchar, status int, userid int, bytes bigint, agent varchar ) WITH ( kafka_topic = 'clickstream', value_format = 'json', partitions = 6 ); -- users lookup table: CREATE TABLE WEB_USERS ( user_id int primary key, registered_At BIGINT, username varchar, first_name varchar, last_name varchar, city varchar, level varchar ) WITH ( kafka_topic = 'clickstream_users', value_format = 'json', partitions = 6 ); -- Build materialized stream views: -- enrich click-stream with more user information: CREATE STREAM USER_CLICKSTREAM AS SELECT userid, u.username, ip, u.city, request, status, bytes FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id; -- Build materialized table views: -- Table of html pages per minute for each user: CREATE TABLE pages_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS pages FROM clickstream WINDOW HOPPING (size 60 second, advance by 5 second) WHERE request like '%html%' GROUP BY userid; -- User sessions table - 30 seconds of inactivity expires the session -- Table counts number of events within the session CREATE TABLE CLICK_USER_SESSIONS AS SELECT username as K, AS_VALUE(username) as username, WINDOWEND as EVENT_TS, count(*) AS events FROM USER_CLICKSTREAM window SESSION (30 second) GROUP BY username; -- number of errors per min, using 'HAVING' Filter to show ERROR codes > 400 where count > 5 CREATE TABLE ERRORS_PER_MIN_ALERT AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 20 second) WHERE status > 400 GROUP BY status HAVING count(*) > 5 AND count(*) is not NULL; -- Enriched user details table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE USER_IP_ACTIVITY WITH (key_format='json') AS SELECT username as k1, ip as k2, city as k3, AS_VALUE(username) as username, WINDOWSTART as EVENT_TS, AS_VALUE(ip) as ip, AS_VALUE(city) as city, COUNT(*) AS count FROM USER_CLICKSTREAM WINDOW TUMBLING (size 60 second) GROUP BY username, ip, city HAVING COUNT(*) > 1; -- Send data to Elasticsearch CREATE SINK CONNECTOR analyzed_clickstream WITH ( 'connector.class' = 'ElasticsearchSink', 'name' = 'recipe-elasticsearch-analyzed_clickstream', 'input.data.format' = 'JSON', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'USER_IP_ACTIVITY, ERRORS_PER_MIN_ALERT', 'connection.url' = '<elasticsearch-URI>', 'connection.user' = '<elasticsearch-username>', 'connection.password' = '<elasticsearch-password>', 'type.name' = 'type.name=kafkaconnect', 'key.ignore' = 'true', 'schema.ignore' = 'true' );","title":"Clickstream Data Analysis"},{"location":"customer-360/clickstream/#clickstream-data-analysis","text":"","title":"Clickstream Data Analysis"},{"location":"customer-360/clickstream/#what-is-it","text":"Analyzing clickstream data enables businesses to understand the behavior of its online users, for example: User activity over a given time frame: how many webpages are users viewing Requests that end in error, over a given threshold (e.g., 404 HTTP codes) Where the requests are coming from geographically in a given window of time How long users are interacting with the site (user sessions)","title":"What is it?"},{"location":"customer-360/clickstream/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"customer-360/clickstream/#step-by-step","text":"","title":"Step-by-step"},{"location":"customer-360/clickstream/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data.","title":"Setup your Environment"},{"location":"customer-360/clickstream/#read-the-data-in","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). This recipe creates simulated data with the Datagen connector. -- Stream of users CREATE SOURCE CONNECTOR datagen_clickstream_users WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_users', 'quickstart' = 'clickstream_users', 'maxInterval' = '10', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of per-user session information CREATE SOURCE CONNECTOR datagen_clickstream WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream', 'quickstart' = 'clickstream', 'maxInterval' = '30', 'format' = 'json'); Optional: to simulate a real-world scenario where user sessions aren't just always open but do close after some time, you can pause and resume the DATAGEN_CLICKSTREAM connector.","title":"Read the data in"},{"location":"customer-360/clickstream/#run-stream-processing-app","text":"Now you can process the data in a variety of ways, by enriching the clickstream data with user information, analyze errors, aggregate data into windows of time, etc. -- stream of user clicks: CREATE STREAM clickstream ( _time bigint, time varchar, ip varchar, request varchar, status int, userid int, bytes bigint, agent varchar ) WITH ( kafka_topic = 'clickstream', value_format = 'json', partitions = 6 ); -- users lookup table: CREATE TABLE WEB_USERS ( user_id int primary key, registered_At BIGINT, username varchar, first_name varchar, last_name varchar, city varchar, level varchar ) WITH ( kafka_topic = 'clickstream_users', value_format = 'json', partitions = 6 ); -- Build materialized stream views: -- enrich click-stream with more user information: CREATE STREAM USER_CLICKSTREAM AS SELECT userid, u.username, ip, u.city, request, status, bytes FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id; -- Build materialized table views: -- Table of html pages per minute for each user: CREATE TABLE pages_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS pages FROM clickstream WINDOW HOPPING (size 60 second, advance by 5 second) WHERE request like '%html%' GROUP BY userid; -- User sessions table - 30 seconds of inactivity expires the session -- Table counts number of events within the session CREATE TABLE CLICK_USER_SESSIONS AS SELECT username as K, AS_VALUE(username) as username, WINDOWEND as EVENT_TS, count(*) AS events FROM USER_CLICKSTREAM window SESSION (30 second) GROUP BY username; -- number of errors per min, using 'HAVING' Filter to show ERROR codes > 400 where count > 5 CREATE TABLE ERRORS_PER_MIN_ALERT AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 20 second) WHERE status > 400 GROUP BY status HAVING count(*) > 5 AND count(*) is not NULL; -- Enriched user details table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE USER_IP_ACTIVITY WITH (key_format='json') AS SELECT username as k1, ip as k2, city as k3, AS_VALUE(username) as username, WINDOWSTART as EVENT_TS, AS_VALUE(ip) as ip, AS_VALUE(city) as city, COUNT(*) AS count FROM USER_CLICKSTREAM WINDOW TUMBLING (size 60 second) GROUP BY username, ip, city HAVING COUNT(*) > 1;","title":"Run stream processing app"},{"location":"customer-360/clickstream/#write-the-data-out","text":"After processing the data, send it to Elasticsearch. -- Send data to Elasticsearch CREATE SINK CONNECTOR analyzed_clickstream WITH ( 'connector.class' = 'ElasticsearchSink', 'name' = 'recipe-elasticsearch-analyzed_clickstream', 'input.data.format' = 'JSON', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'USER_IP_ACTIVITY, ERRORS_PER_MIN_ALERT', 'connection.url' = '<elasticsearch-URI>', 'connection.user' = '<elasticsearch-username>', 'connection.password' = '<elasticsearch-password>', 'type.name' = 'type.name=kafkaconnect', 'key.ignore' = 'true', 'schema.ignore' = 'true' );","title":"Write the data out"},{"location":"customer-360/clickstream/#full-ksqldb-statements","text":"Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of users CREATE SOURCE CONNECTOR datagen_clickstream_users WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_users', 'quickstart' = 'clickstream_users', 'maxInterval' = '10', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of per-user session information CREATE SOURCE CONNECTOR datagen_clickstream WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream', 'quickstart' = 'clickstream', 'maxInterval' = '30', 'format' = 'json'); -- stream of user clicks: CREATE STREAM clickstream ( _time bigint, time varchar, ip varchar, request varchar, status int, userid int, bytes bigint, agent varchar ) WITH ( kafka_topic = 'clickstream', value_format = 'json', partitions = 6 ); -- users lookup table: CREATE TABLE WEB_USERS ( user_id int primary key, registered_At BIGINT, username varchar, first_name varchar, last_name varchar, city varchar, level varchar ) WITH ( kafka_topic = 'clickstream_users', value_format = 'json', partitions = 6 ); -- Build materialized stream views: -- enrich click-stream with more user information: CREATE STREAM USER_CLICKSTREAM AS SELECT userid, u.username, ip, u.city, request, status, bytes FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id; -- Build materialized table views: -- Table of html pages per minute for each user: CREATE TABLE pages_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS pages FROM clickstream WINDOW HOPPING (size 60 second, advance by 5 second) WHERE request like '%html%' GROUP BY userid; -- User sessions table - 30 seconds of inactivity expires the session -- Table counts number of events within the session CREATE TABLE CLICK_USER_SESSIONS AS SELECT username as K, AS_VALUE(username) as username, WINDOWEND as EVENT_TS, count(*) AS events FROM USER_CLICKSTREAM window SESSION (30 second) GROUP BY username; -- number of errors per min, using 'HAVING' Filter to show ERROR codes > 400 where count > 5 CREATE TABLE ERRORS_PER_MIN_ALERT AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 20 second) WHERE status > 400 GROUP BY status HAVING count(*) > 5 AND count(*) is not NULL; -- Enriched user details table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE USER_IP_ACTIVITY WITH (key_format='json') AS SELECT username as k1, ip as k2, city as k3, AS_VALUE(username) as username, WINDOWSTART as EVENT_TS, AS_VALUE(ip) as ip, AS_VALUE(city) as city, COUNT(*) AS count FROM USER_CLICKSTREAM WINDOW TUMBLING (size 60 second) GROUP BY username, ip, city HAVING COUNT(*) > 1; -- Send data to Elasticsearch CREATE SINK CONNECTOR analyzed_clickstream WITH ( 'connector.class' = 'ElasticsearchSink', 'name' = 'recipe-elasticsearch-analyzed_clickstream', 'input.data.format' = 'JSON', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'USER_IP_ACTIVITY, ERRORS_PER_MIN_ALERT', 'connection.url' = '<elasticsearch-URI>', 'connection.user' = '<elasticsearch-username>', 'connection.password' = '<elasticsearch-password>', 'type.name' = 'type.name=kafkaconnect', 'key.ignore' = 'true', 'schema.ignore' = 'true' );","title":"Full ksqlDB Statements"},{"location":"fin-serv/credit-card-activity/","text":"Detecting Unusual Credit Card Activity What is it? In banking, fraud can involve using stolen credit cards, forging checks, misleading accounting practices, etc. This recipe analyzes total credit card spend, and if it's more than the average credit card usage of a customer, the account will be flagged as a possible case of credit card theft. Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data. Read the data in Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). -- Stream of transactions CREATE SOURCE CONNECTOR transactions WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-transactions', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'TRANSACTIONS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams. Run stream processing app Now you can process the data in a variety of ways. -- Register the stream of transactions CREATE STREAM TRANSACTIONS_RAW ( ACCOUNT_ID BIGINT, TIMESTAMP VARCHAR, CARD_TYPE VARCHAR, AMOUNT DOUBLE, IP_ADDRESS VARCHAR, TRANSACTION_ID VARCHAR ) WITH ( KAFKA_TOPIC='transactions', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Repartition the stream on account_id in order to ensure that all the streams and tables are co-partitioned, which means that input records on both sides of the join have the same configuration settings for partitions. CREATE STREAM TRANSACTIONS_SOURCE AS SELECT * FROM TRANSACTIONS_RAW PARTITION BY ACCOUNT_ID; -- Register the existing stream of customer data CREATE STREAM CUST_RAW_STREAM ( ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, AVG_CREDIT_SPEND DOUBLE ) WITH ( KAFKA_TOPIC='customers', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Repartition the customer data stream by account_id to prepare for the join CREATE STREAM CUSTOMER_REKEYED AS SELECT * FROM CUST_RAW_STREAM PARTITION BY ID; -- Register the partitioned customer data topic as a table used for the join with the incoming stream of transactions: CREATE TABLE customer ( ID BIGINT PRIMARY KEY, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, AVG_CREDIT_SPEND DOUBLE ) WITH ( KAFKA_TOPIC='CUSTOMER_REKEYED', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Join the transactions to customer information: CREATE STREAM TRANSACTIONS_ENRICHED AS SELECT T.ACCOUNT_ID, T.CARD_TYPE, T.AMOUNT, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME, C.AVG_CREDIT_SPEND FROM TRANSACTIONS_SOURCE T INNER JOIN CUSTOMER C ON T.ACCOUNT_ID = C.ID; -- Aggregate the stream of transactions for each account ID using a two-hour tumbling window, and filter for accounts in which the total spend in a two-hour period is greater than the customer\u2019s average: CREATE TABLE POSSIBLE_STOLEN_CARD AS SELECT TIMESTAMPTOSTRING(WindowStart(), 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_START, T.ACCOUNT_ID, T.CARD_TYPE, SUM(T.AMOUNT) AS TOTAL_CREDIT_SPEND, T.FULL_NAME, MAX(T.AVG_CREDIT_SPEND) AS AVG_CREDIT_SPEND FROM TRANSACTIONS_ENRICHED T WINDOW TUMBLING (SIZE 2 HOURS) GROUP BY T.ACCOUNT_ID, T.CARD_TYPE, T.FULL_NAME HAVING SUM(T.AMOUNT) > MAX(T.AVG_CREDIT_SPEND); In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO transactions_raw (account_id, timestamp, card_type, amount, ip_address, transaction_id) VALUES (6011000990139424, '2021-09-23T10:50:00.000Z', 'visa', 542.99, '192.168.44.1', '3985757'); INSERT INTO transactions_raw (account_id, timestamp, card_type, amount, ip_address, transaction_id) VALUES (6011000990139424, '2021-09-23T10:50:01.000Z', 'visa', 611.48, '192.168.44.1', '8028435'); INSERT INTO transactions_raw (account_id, timestamp, card_type, amount, ip_address, transaction_id) VALUES (3530111333300000, '2021-09-23T10:50:00.000Z', 'mastercard', 65.0, '192.168.101.3', '1695780'); INSERT INTO cust_raw_stream (id, first_name, last_name, email, avg_credit_spend) VALUES (6011000990139424, 'Janice', 'Smith', 'jsmith@mycompany.com', 500.00); INSERT INTO cust_raw_stream (id, first_name, last_name, email, avg_credit_spend) VALUES (3530111333300000, 'George', 'Mall', 'gmall@mycompany.com', 20.00); Full ksqlDB Statements Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of transactions CREATE SOURCE CONNECTOR transactions WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-transactions', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'TRANSACTIONS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Register the stream of transactions CREATE STREAM TRANSACTIONS_RAW ( ACCOUNT_ID BIGINT, TIMESTAMP VARCHAR, CARD_TYPE VARCHAR, AMOUNT DOUBLE, IP_ADDRESS VARCHAR, TRANSACTION_ID VARCHAR ) WITH ( KAFKA_TOPIC='transactions', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Repartition the stream on account_id in order to ensure that all the streams and tables are co-partitioned, which means that input records on both sides of the join have the same configuration settings for partitions. CREATE STREAM TRANSACTIONS_SOURCE AS SELECT * FROM TRANSACTIONS_RAW PARTITION BY ACCOUNT_ID; -- Register the existing stream of customer data CREATE STREAM CUST_RAW_STREAM ( ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, AVG_CREDIT_SPEND DOUBLE ) WITH ( KAFKA_TOPIC='customers', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Repartition the customer data stream by account_id to prepare for the join CREATE STREAM CUSTOMER_REKEYED AS SELECT * FROM CUST_RAW_STREAM PARTITION BY ID; -- Register the partitioned customer data topic as a table used for the join with the incoming stream of transactions: CREATE TABLE customer ( ID BIGINT PRIMARY KEY, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, AVG_CREDIT_SPEND DOUBLE ) WITH ( KAFKA_TOPIC='CUSTOMER_REKEYED', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Join the transactions to customer information: CREATE STREAM TRANSACTIONS_ENRICHED AS SELECT T.ACCOUNT_ID, T.CARD_TYPE, T.AMOUNT, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME, C.AVG_CREDIT_SPEND FROM TRANSACTIONS_SOURCE T INNER JOIN CUSTOMER C ON T.ACCOUNT_ID = C.ID; -- Aggregate the stream of transactions for each account ID using a two-hour tumbling window, and filter for accounts in which the total spend in a two-hour period is greater than the customer\u2019s average: CREATE TABLE POSSIBLE_STOLEN_CARD AS SELECT TIMESTAMPTOSTRING(WindowStart(), 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_START, T.ACCOUNT_ID, T.CARD_TYPE, SUM(T.AMOUNT) AS TOTAL_CREDIT_SPEND, T.FULL_NAME, MAX(T.AVG_CREDIT_SPEND) AS AVG_CREDIT_SPEND FROM TRANSACTIONS_ENRICHED T WINDOW TUMBLING (SIZE 2 HOURS) GROUP BY T.ACCOUNT_ID, T.CARD_TYPE, T.FULL_NAME HAVING SUM(T.AMOUNT) > MAX(T.AVG_CREDIT_SPEND);","title":"Detecting Unusual Credit Card Activity"},{"location":"fin-serv/credit-card-activity/#detecting-unusual-credit-card-activity","text":"","title":"Detecting Unusual Credit Card Activity"},{"location":"fin-serv/credit-card-activity/#what-is-it","text":"In banking, fraud can involve using stolen credit cards, forging checks, misleading accounting practices, etc. This recipe analyzes total credit card spend, and if it's more than the average credit card usage of a customer, the account will be flagged as a possible case of credit card theft.","title":"What is it?"},{"location":"fin-serv/credit-card-activity/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"fin-serv/credit-card-activity/#step-by-step","text":"","title":"Step-by-step"},{"location":"fin-serv/credit-card-activity/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data.","title":"Setup your Environment"},{"location":"fin-serv/credit-card-activity/#read-the-data-in","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). -- Stream of transactions CREATE SOURCE CONNECTOR transactions WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-transactions', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'TRANSACTIONS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams.","title":"Read the data in"},{"location":"fin-serv/credit-card-activity/#run-stream-processing-app","text":"Now you can process the data in a variety of ways. -- Register the stream of transactions CREATE STREAM TRANSACTIONS_RAW ( ACCOUNT_ID BIGINT, TIMESTAMP VARCHAR, CARD_TYPE VARCHAR, AMOUNT DOUBLE, IP_ADDRESS VARCHAR, TRANSACTION_ID VARCHAR ) WITH ( KAFKA_TOPIC='transactions', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Repartition the stream on account_id in order to ensure that all the streams and tables are co-partitioned, which means that input records on both sides of the join have the same configuration settings for partitions. CREATE STREAM TRANSACTIONS_SOURCE AS SELECT * FROM TRANSACTIONS_RAW PARTITION BY ACCOUNT_ID; -- Register the existing stream of customer data CREATE STREAM CUST_RAW_STREAM ( ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, AVG_CREDIT_SPEND DOUBLE ) WITH ( KAFKA_TOPIC='customers', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Repartition the customer data stream by account_id to prepare for the join CREATE STREAM CUSTOMER_REKEYED AS SELECT * FROM CUST_RAW_STREAM PARTITION BY ID; -- Register the partitioned customer data topic as a table used for the join with the incoming stream of transactions: CREATE TABLE customer ( ID BIGINT PRIMARY KEY, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, AVG_CREDIT_SPEND DOUBLE ) WITH ( KAFKA_TOPIC='CUSTOMER_REKEYED', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Join the transactions to customer information: CREATE STREAM TRANSACTIONS_ENRICHED AS SELECT T.ACCOUNT_ID, T.CARD_TYPE, T.AMOUNT, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME, C.AVG_CREDIT_SPEND FROM TRANSACTIONS_SOURCE T INNER JOIN CUSTOMER C ON T.ACCOUNT_ID = C.ID; -- Aggregate the stream of transactions for each account ID using a two-hour tumbling window, and filter for accounts in which the total spend in a two-hour period is greater than the customer\u2019s average: CREATE TABLE POSSIBLE_STOLEN_CARD AS SELECT TIMESTAMPTOSTRING(WindowStart(), 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_START, T.ACCOUNT_ID, T.CARD_TYPE, SUM(T.AMOUNT) AS TOTAL_CREDIT_SPEND, T.FULL_NAME, MAX(T.AVG_CREDIT_SPEND) AS AVG_CREDIT_SPEND FROM TRANSACTIONS_ENRICHED T WINDOW TUMBLING (SIZE 2 HOURS) GROUP BY T.ACCOUNT_ID, T.CARD_TYPE, T.FULL_NAME HAVING SUM(T.AMOUNT) > MAX(T.AVG_CREDIT_SPEND); In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO transactions_raw (account_id, timestamp, card_type, amount, ip_address, transaction_id) VALUES (6011000990139424, '2021-09-23T10:50:00.000Z', 'visa', 542.99, '192.168.44.1', '3985757'); INSERT INTO transactions_raw (account_id, timestamp, card_type, amount, ip_address, transaction_id) VALUES (6011000990139424, '2021-09-23T10:50:01.000Z', 'visa', 611.48, '192.168.44.1', '8028435'); INSERT INTO transactions_raw (account_id, timestamp, card_type, amount, ip_address, transaction_id) VALUES (3530111333300000, '2021-09-23T10:50:00.000Z', 'mastercard', 65.0, '192.168.101.3', '1695780'); INSERT INTO cust_raw_stream (id, first_name, last_name, email, avg_credit_spend) VALUES (6011000990139424, 'Janice', 'Smith', 'jsmith@mycompany.com', 500.00); INSERT INTO cust_raw_stream (id, first_name, last_name, email, avg_credit_spend) VALUES (3530111333300000, 'George', 'Mall', 'gmall@mycompany.com', 20.00);","title":"Run stream processing app"},{"location":"fin-serv/credit-card-activity/#full-ksqldb-statements","text":"Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of transactions CREATE SOURCE CONNECTOR transactions WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-transactions', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'TRANSACTIONS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Register the stream of transactions CREATE STREAM TRANSACTIONS_RAW ( ACCOUNT_ID BIGINT, TIMESTAMP VARCHAR, CARD_TYPE VARCHAR, AMOUNT DOUBLE, IP_ADDRESS VARCHAR, TRANSACTION_ID VARCHAR ) WITH ( KAFKA_TOPIC='transactions', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Repartition the stream on account_id in order to ensure that all the streams and tables are co-partitioned, which means that input records on both sides of the join have the same configuration settings for partitions. CREATE STREAM TRANSACTIONS_SOURCE AS SELECT * FROM TRANSACTIONS_RAW PARTITION BY ACCOUNT_ID; -- Register the existing stream of customer data CREATE STREAM CUST_RAW_STREAM ( ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, AVG_CREDIT_SPEND DOUBLE ) WITH ( KAFKA_TOPIC='customers', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Repartition the customer data stream by account_id to prepare for the join CREATE STREAM CUSTOMER_REKEYED AS SELECT * FROM CUST_RAW_STREAM PARTITION BY ID; -- Register the partitioned customer data topic as a table used for the join with the incoming stream of transactions: CREATE TABLE customer ( ID BIGINT PRIMARY KEY, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, AVG_CREDIT_SPEND DOUBLE ) WITH ( KAFKA_TOPIC='CUSTOMER_REKEYED', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Join the transactions to customer information: CREATE STREAM TRANSACTIONS_ENRICHED AS SELECT T.ACCOUNT_ID, T.CARD_TYPE, T.AMOUNT, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME, C.AVG_CREDIT_SPEND FROM TRANSACTIONS_SOURCE T INNER JOIN CUSTOMER C ON T.ACCOUNT_ID = C.ID; -- Aggregate the stream of transactions for each account ID using a two-hour tumbling window, and filter for accounts in which the total spend in a two-hour period is greater than the customer\u2019s average: CREATE TABLE POSSIBLE_STOLEN_CARD AS SELECT TIMESTAMPTOSTRING(WindowStart(), 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_START, T.ACCOUNT_ID, T.CARD_TYPE, SUM(T.AMOUNT) AS TOTAL_CREDIT_SPEND, T.FULL_NAME, MAX(T.AVG_CREDIT_SPEND) AS AVG_CREDIT_SPEND FROM TRANSACTIONS_ENRICHED T WINDOW TUMBLING (SIZE 2 HOURS) GROUP BY T.ACCOUNT_ID, T.CARD_TYPE, T.FULL_NAME HAVING SUM(T.AMOUNT) > MAX(T.AVG_CREDIT_SPEND);","title":"Full ksqlDB Statements"},{"location":"fin-serv/denormalization/","text":"Change Data Capture (CDC) for Orders What is it? If you have transactional events for orders in a marketplace, you can stream the Change Data Capture (CDC) and denormalize it. Denormalization is a well-established pattern for performance, because most times querying a single table of data will perform better than querying across multiple at runtime. Then open it up to be consumed by downstream applications in your business, or stream it out to another destination. This recipe demonstrates this principle by streaming from a SQL Server, denormalizing the data, and writing to Snowflake. Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data. Read the data in Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). Change Data Capture (CDC) for orders is being written to a SQL Server database, and there is an Oracle database with customer data. -- Stream of orders CREATE SOURCE CONNECTOR orders WITH ( 'connector.class' = 'SqlServerCdcSource', 'name' = 'recipe-sqlservercdc-orders', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'database.hostname' = '<db-name>', 'database.port' = '1433', 'database.user' = '<database-username>', 'database.password' = '<database-password>', 'database.dbname' = 'database-name', 'database.server.name' = 'sql', 'table.include.list' ='<table_name>', 'snapshot.mode' = 'initial', 'output.data.format' = 'JSON', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topic.prefix' = 'oracle_', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams. Run stream processing app This streams the user orders and denormalizes the data by joining facts (orders) with the dimension (customer). -- Stream of user orders: CREATE STREAM orders ( ORDER_ID BIGINT, CUSTOMER_ID BIGINT, ITEM VARCHAR, ORDER_TOTAL_USD DOUBLE ) WITH ( kafka_topic = 'orders', value_format = 'json', partitions = 6 ); -- Register the existing stream of customer data CREATE STREAM CUST_RAW_STREAM ( ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR ) WITH ( KAFKA_TOPIC='customers', VALUE_FORMAT='JSON', PARTITIONS = 6 ); -- Register the customer data topic as a table CREATE TABLE customer ( ID BIGINT PRIMARY KEY ) WITH ( KAFKA_TOPIC='CUST_RAW_STREAM', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Denormalize data: joining facts (orders) with the dimension (customer) CREATE STREAM ORDERS_ENRICHED AS SELECT O.order_id AS order_id, O.item AS item, O.order_total_usd AS order_total_usd, C.first_name || ' ' || C.last_name AS full_name, C.email AS email FROM ORDERS O LEFT JOIN CUSTOMERS C ON O.customer_id = C.id; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697328, 375, 'book', 29.99); INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697329, 375, 'guitar', 215.99); INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697330, 983, 'thermometer', 12.99); INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697331, 983, 'scarf', 32.99); INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697332, 375, 'doormat', 15.99); INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697333, 983, 'clippers', 65.99); INSERT INTO customers (id, first_name, last_name, email) VALUES (375, 'Janice', 'Smith', 'jsmith@mycompany.com'); INSERT INTO customers (id, first_name, last_name, email) VALUES (983, 'George', 'Mall', 'gmall@mycompany.com'); Write the data out Any downstream application or database can receive the denormalized data. -- Send data to Snowflake CREATE SINK CONNECTOR orders_enriched WITH ( 'connector.class' = 'SnowflakeSink', 'name' = 'recipe-snowflake-analyzed_clickstream', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'ORDERS_ENRICHED', 'input.data.format' = 'JSON', 'snowflake.url.name' = 'https=//wm83168.us-central1.gcp.snowflakecomputing.com=443', 'snowflake.user.name' = '<login-username>', 'snowflake.private.key' = '<private-key>', 'snowflake.database.name' = '<database-name>', 'snowflake.schema.name' = '<schema-name>', 'tasks.max' = '1'); Full ksqlDB Statements Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of orders CREATE SOURCE CONNECTOR orders WITH ( 'connector.class' = 'SqlServerCdcSource', 'name' = 'recipe-sqlservercdc-orders', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'database.hostname' = '<db-name>', 'database.port' = '1433', 'database.user' = '<database-username>', 'database.password' = '<database-password>', 'database.dbname' = 'database-name', 'database.server.name' = 'sql', 'table.include.list' ='<table_name>', 'snapshot.mode' = 'initial', 'output.data.format' = 'JSON', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topic.prefix' = 'oracle_', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Stream of user orders: CREATE STREAM orders ( ORDER_ID BIGINT, CUSTOMER_ID BIGINT, ITEM VARCHAR, ORDER_TOTAL_USD DOUBLE ) WITH ( kafka_topic = 'orders', value_format = 'json', partitions = 6 ); -- Register the existing stream of customer data CREATE STREAM CUST_RAW_STREAM ( ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR ) WITH ( KAFKA_TOPIC='customers', VALUE_FORMAT='JSON', PARTITIONS = 6 ); -- Register the customer data topic as a table CREATE TABLE customer ( ID BIGINT PRIMARY KEY ) WITH ( KAFKA_TOPIC='CUST_RAW_STREAM', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Denormalize data: joining facts (orders) with the dimension (customer) CREATE STREAM ORDERS_ENRICHED AS SELECT O.order_id AS order_id, O.item AS item, O.order_total_usd AS order_total_usd, C.first_name || ' ' || C.last_name AS full_name, C.email AS email FROM ORDERS O LEFT JOIN CUSTOMERS C ON O.customer_id = C.id; -- Send data to Snowflake CREATE SINK CONNECTOR orders_enriched WITH ( 'connector.class' = 'SnowflakeSink', 'name' = 'recipe-snowflake-analyzed_clickstream', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'ORDERS_ENRICHED', 'input.data.format' = 'JSON', 'snowflake.url.name' = 'https=//wm83168.us-central1.gcp.snowflakecomputing.com=443', 'snowflake.user.name' = '<login-username>', 'snowflake.private.key' = '<private-key>', 'snowflake.database.name' = '<database-name>', 'snowflake.schema.name' = '<schema-name>', 'tasks.max' = '1');","title":"Change Data Capture (CDC) for Orders"},{"location":"fin-serv/denormalization/#change-data-capture-cdc-for-orders","text":"","title":"Change Data Capture (CDC) for Orders"},{"location":"fin-serv/denormalization/#what-is-it","text":"If you have transactional events for orders in a marketplace, you can stream the Change Data Capture (CDC) and denormalize it. Denormalization is a well-established pattern for performance, because most times querying a single table of data will perform better than querying across multiple at runtime. Then open it up to be consumed by downstream applications in your business, or stream it out to another destination. This recipe demonstrates this principle by streaming from a SQL Server, denormalizing the data, and writing to Snowflake.","title":"What is it?"},{"location":"fin-serv/denormalization/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"fin-serv/denormalization/#step-by-step","text":"","title":"Step-by-step"},{"location":"fin-serv/denormalization/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data.","title":"Setup your Environment"},{"location":"fin-serv/denormalization/#read-the-data-in","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). Change Data Capture (CDC) for orders is being written to a SQL Server database, and there is an Oracle database with customer data. -- Stream of orders CREATE SOURCE CONNECTOR orders WITH ( 'connector.class' = 'SqlServerCdcSource', 'name' = 'recipe-sqlservercdc-orders', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'database.hostname' = '<db-name>', 'database.port' = '1433', 'database.user' = '<database-username>', 'database.password' = '<database-password>', 'database.dbname' = 'database-name', 'database.server.name' = 'sql', 'table.include.list' ='<table_name>', 'snapshot.mode' = 'initial', 'output.data.format' = 'JSON', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topic.prefix' = 'oracle_', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams.","title":"Read the data in"},{"location":"fin-serv/denormalization/#run-stream-processing-app","text":"This streams the user orders and denormalizes the data by joining facts (orders) with the dimension (customer). -- Stream of user orders: CREATE STREAM orders ( ORDER_ID BIGINT, CUSTOMER_ID BIGINT, ITEM VARCHAR, ORDER_TOTAL_USD DOUBLE ) WITH ( kafka_topic = 'orders', value_format = 'json', partitions = 6 ); -- Register the existing stream of customer data CREATE STREAM CUST_RAW_STREAM ( ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR ) WITH ( KAFKA_TOPIC='customers', VALUE_FORMAT='JSON', PARTITIONS = 6 ); -- Register the customer data topic as a table CREATE TABLE customer ( ID BIGINT PRIMARY KEY ) WITH ( KAFKA_TOPIC='CUST_RAW_STREAM', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Denormalize data: joining facts (orders) with the dimension (customer) CREATE STREAM ORDERS_ENRICHED AS SELECT O.order_id AS order_id, O.item AS item, O.order_total_usd AS order_total_usd, C.first_name || ' ' || C.last_name AS full_name, C.email AS email FROM ORDERS O LEFT JOIN CUSTOMERS C ON O.customer_id = C.id; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697328, 375, 'book', 29.99); INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697329, 375, 'guitar', 215.99); INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697330, 983, 'thermometer', 12.99); INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697331, 983, 'scarf', 32.99); INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697332, 375, 'doormat', 15.99); INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697333, 983, 'clippers', 65.99); INSERT INTO customers (id, first_name, last_name, email) VALUES (375, 'Janice', 'Smith', 'jsmith@mycompany.com'); INSERT INTO customers (id, first_name, last_name, email) VALUES (983, 'George', 'Mall', 'gmall@mycompany.com');","title":"Run stream processing app"},{"location":"fin-serv/denormalization/#write-the-data-out","text":"Any downstream application or database can receive the denormalized data. -- Send data to Snowflake CREATE SINK CONNECTOR orders_enriched WITH ( 'connector.class' = 'SnowflakeSink', 'name' = 'recipe-snowflake-analyzed_clickstream', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'ORDERS_ENRICHED', 'input.data.format' = 'JSON', 'snowflake.url.name' = 'https=//wm83168.us-central1.gcp.snowflakecomputing.com=443', 'snowflake.user.name' = '<login-username>', 'snowflake.private.key' = '<private-key>', 'snowflake.database.name' = '<database-name>', 'snowflake.schema.name' = '<schema-name>', 'tasks.max' = '1');","title":"Write the data out"},{"location":"fin-serv/denormalization/#full-ksqldb-statements","text":"Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of orders CREATE SOURCE CONNECTOR orders WITH ( 'connector.class' = 'SqlServerCdcSource', 'name' = 'recipe-sqlservercdc-orders', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'database.hostname' = '<db-name>', 'database.port' = '1433', 'database.user' = '<database-username>', 'database.password' = '<database-password>', 'database.dbname' = 'database-name', 'database.server.name' = 'sql', 'table.include.list' ='<table_name>', 'snapshot.mode' = 'initial', 'output.data.format' = 'JSON', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topic.prefix' = 'oracle_', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Stream of user orders: CREATE STREAM orders ( ORDER_ID BIGINT, CUSTOMER_ID BIGINT, ITEM VARCHAR, ORDER_TOTAL_USD DOUBLE ) WITH ( kafka_topic = 'orders', value_format = 'json', partitions = 6 ); -- Register the existing stream of customer data CREATE STREAM CUST_RAW_STREAM ( ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR ) WITH ( KAFKA_TOPIC='customers', VALUE_FORMAT='JSON', PARTITIONS = 6 ); -- Register the customer data topic as a table CREATE TABLE customer ( ID BIGINT PRIMARY KEY ) WITH ( KAFKA_TOPIC='CUST_RAW_STREAM', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Denormalize data: joining facts (orders) with the dimension (customer) CREATE STREAM ORDERS_ENRICHED AS SELECT O.order_id AS order_id, O.item AS item, O.order_total_usd AS order_total_usd, C.first_name || ' ' || C.last_name AS full_name, C.email AS email FROM ORDERS O LEFT JOIN CUSTOMERS C ON O.customer_id = C.id; -- Send data to Snowflake CREATE SINK CONNECTOR orders_enriched WITH ( 'connector.class' = 'SnowflakeSink', 'name' = 'recipe-snowflake-analyzed_clickstream', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'ORDERS_ENRICHED', 'input.data.format' = 'JSON', 'snowflake.url.name' = 'https=//wm83168.us-central1.gcp.snowflakecomputing.com=443', 'snowflake.user.name' = '<login-username>', 'snowflake.private.key' = '<private-key>', 'snowflake.database.name' = '<database-name>', 'snowflake.schema.name' = '<schema-name>', 'tasks.max' = '1');","title":"Full ksqlDB Statements"},{"location":"fin-serv/payment-status-check/","text":"Payment Status Check What is it? TODO Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data. Read the data in Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-customers', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'customers', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of payments CREATE SOURCE CONNECTOR payments WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-payments', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'payments', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of aml_status CREATE SOURCE CONNECTOR aml_status WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-aml_status', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'aml_status', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of funds_status CREATE SOURCE CONNECTOR funds_status WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-funds_status', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'funds_status', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams. Run stream processing app Now you can process the data in a variety of ways. -- Register the initial streams and tables from the Kafka topics CREATE STREAM PAYMENTS ( PAYMENT_ID INTEGER KEY, CUSTID INTEGER, ACCOUNTID INTEGER, AMOUNT INTEGER, BANK VARCHAR ) WITH ( kafka_topic='payments', value_format='json' ); create stream aml_status ( PAYMENT_ID INTEGER, BANK VARCHAR, STATUS VARCHAR ) with ( kafka_topic='aml_status', value_format='json' ); create stream funds_status ( PAYMENT_ID INTEGER, REASON_CODE VARCHAR, STATUS VARCHAR ) with ( kafka_topic='funds_status', value_format='json' ); create table customers ( ID INTEGER PRIMARY KEY, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, GENDER VARCHAR, STATUS360 VARCHAR ) WITH ( kafka_topic='customers', value_format='JSON' ); -- Enrich Payments stream with Customers table create stream enriched_payments as select p.payment_id as payment_id, p.custid as customer_id, p.accountid, p.amount, p.bank, c.first_name, c.last_name, c.email, c.status360 from payments p left join customers c on p.custid = c.id; -- Combine the status streams CREATE STREAM payment_statuses AS SELECT payment_id, status, 'AML' as source_system FROM aml_status; INSERT INTO payment_statuses SELECT payment_id, status, 'FUNDS' as source_system FROM funds_status; -- Combine payment and status events in 1 hour window. Why we need a timing window for stream-stream join? CREATE STREAM payments_with_status AS SELECT ep.payment_id as payment_id, ep.accountid, ep.amount, ep.bank, ep.first_name, ep.last_name, ep.email, ep.status360, ps.status, ps.source_system FROM enriched_payments ep LEFT JOIN payment_statuses ps WITHIN 1 HOUR ON ep.payment_id = ps.payment_id ; -- Aggregate data to the final table CREATE TABLE payments_final AS SELECT payment_id, histogram(status) as status_counts, collect_list('{ \"system\" : \"' + source_system + '\", \"status\" : \"' + STATUS + '\"}') as service_status_list from payments_with_status where status is not null group by payment_id; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: -- Customer Data INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (10,'Brena','Tollerton','btollerton9@furl.net','Female','silver'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (9,'Even','Tinham','etinham8@facebook.com','Male','silver'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (8,'Patti','Rosten','prosten7@ihg.com','Female','silver'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (7,'Fay','Huc','fhuc6@quantcast.com','Female','bronze'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (6,'Robinet','Leheude','rleheude5@reddit.com','Female','platinum'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (5,'Hansiain','Coda','hcoda4@senate.gov','Male','platinum'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (4,'Hashim','Rumke','hrumke3@sohu.com','Male','platinum'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (3,'Mariejeanne','Cocci','mcocci2@techcrunch.com','Female','bronze'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (2,'Ruthie','Brockherst','rbrockherst1@ow.ly','Female','platinum'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (1,'Rica','Blaisdell','rblaisdell0@rambler.ru','Female','bronze'); -- Payment Instruction Data INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (1,1,1234000,100,'DBS'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (3,2,1234100,200,'Barclays Bank'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (5,3,1234200,300,'BNP Paribas'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (7,4,1234300,400,'Wells Fargo'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (9,5,1234400,500,'DBS'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (11,6,1234500,600,'Royal Bank of Canada'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (13,7,1234600,700,'DBS'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (15,8,1234700,800,'DBS'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (17,9,1234800,900,'DBS'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (19,10,1234900,1000,'United Overseas Bank'); -- ALM Status Data INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (1,'Wells Fargo','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (3,'Commonwealth Bank of Australia','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (5,'Deutsche Bank','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (7,'DBS','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (9,'United Overseas Bank','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (11,'Citi','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (13,'Commonwealth Bank of Australia','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (15,'Barclays Bank','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (17,'United Overseas Bank','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (19,'Royal Bank of Canada','OK'); -- Funds Status Data INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (1,'00','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (3,'99','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (5,'30','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (7,'00','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (9,'00','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (11,'00','NOT OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (13,'30','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (15,'00','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (17,'10','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (19,'10','OK'); Full ksqlDB Statements Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-customers', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'customers', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of payments CREATE SOURCE CONNECTOR payments WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-payments', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'payments', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of aml_status CREATE SOURCE CONNECTOR aml_status WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-aml_status', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'aml_status', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of funds_status CREATE SOURCE CONNECTOR funds_status WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-funds_status', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'funds_status', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Register the initial streams and tables from the Kafka topics CREATE STREAM PAYMENTS ( PAYMENT_ID INTEGER KEY, CUSTID INTEGER, ACCOUNTID INTEGER, AMOUNT INTEGER, BANK VARCHAR ) WITH ( kafka_topic='payments', value_format='json' ); create stream aml_status ( PAYMENT_ID INTEGER, BANK VARCHAR, STATUS VARCHAR ) with ( kafka_topic='aml_status', value_format='json' ); create stream funds_status ( PAYMENT_ID INTEGER, REASON_CODE VARCHAR, STATUS VARCHAR ) with ( kafka_topic='funds_status', value_format='json' ); create table customers ( ID INTEGER PRIMARY KEY, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, GENDER VARCHAR, STATUS360 VARCHAR ) WITH ( kafka_topic='customers', value_format='JSON' ); -- Enrich Payments stream with Customers table create stream enriched_payments as select p.payment_id as payment_id, p.custid as customer_id, p.accountid, p.amount, p.bank, c.first_name, c.last_name, c.email, c.status360 from payments p left join customers c on p.custid = c.id; -- Combine the status streams CREATE STREAM payment_statuses AS SELECT payment_id, status, 'AML' as source_system FROM aml_status; INSERT INTO payment_statuses SELECT payment_id, status, 'FUNDS' as source_system FROM funds_status; -- Combine payment and status events in 1 hour window. Why we need a timing window for stream-stream join? CREATE STREAM payments_with_status AS SELECT ep.payment_id as payment_id, ep.accountid, ep.amount, ep.bank, ep.first_name, ep.last_name, ep.email, ep.status360, ps.status, ps.source_system FROM enriched_payments ep LEFT JOIN payment_statuses ps WITHIN 1 HOUR ON ep.payment_id = ps.payment_id ; -- Aggregate data to the final table CREATE TABLE payments_final AS SELECT payment_id, histogram(status) as status_counts, collect_list('{ \"system\" : \"' + source_system + '\", \"status\" : \"' + STATUS + '\"}') as service_status_list from payments_with_status where status is not null group by payment_id;","title":"Payment Status Check"},{"location":"fin-serv/payment-status-check/#payment-status-check","text":"","title":"Payment Status Check"},{"location":"fin-serv/payment-status-check/#what-is-it","text":"TODO","title":"What is it?"},{"location":"fin-serv/payment-status-check/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"fin-serv/payment-status-check/#step-by-step","text":"","title":"Step-by-step"},{"location":"fin-serv/payment-status-check/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data.","title":"Setup your Environment"},{"location":"fin-serv/payment-status-check/#read-the-data-in","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-customers', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'customers', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of payments CREATE SOURCE CONNECTOR payments WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-payments', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'payments', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of aml_status CREATE SOURCE CONNECTOR aml_status WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-aml_status', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'aml_status', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of funds_status CREATE SOURCE CONNECTOR funds_status WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-funds_status', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'funds_status', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams.","title":"Read the data in"},{"location":"fin-serv/payment-status-check/#run-stream-processing-app","text":"Now you can process the data in a variety of ways. -- Register the initial streams and tables from the Kafka topics CREATE STREAM PAYMENTS ( PAYMENT_ID INTEGER KEY, CUSTID INTEGER, ACCOUNTID INTEGER, AMOUNT INTEGER, BANK VARCHAR ) WITH ( kafka_topic='payments', value_format='json' ); create stream aml_status ( PAYMENT_ID INTEGER, BANK VARCHAR, STATUS VARCHAR ) with ( kafka_topic='aml_status', value_format='json' ); create stream funds_status ( PAYMENT_ID INTEGER, REASON_CODE VARCHAR, STATUS VARCHAR ) with ( kafka_topic='funds_status', value_format='json' ); create table customers ( ID INTEGER PRIMARY KEY, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, GENDER VARCHAR, STATUS360 VARCHAR ) WITH ( kafka_topic='customers', value_format='JSON' ); -- Enrich Payments stream with Customers table create stream enriched_payments as select p.payment_id as payment_id, p.custid as customer_id, p.accountid, p.amount, p.bank, c.first_name, c.last_name, c.email, c.status360 from payments p left join customers c on p.custid = c.id; -- Combine the status streams CREATE STREAM payment_statuses AS SELECT payment_id, status, 'AML' as source_system FROM aml_status; INSERT INTO payment_statuses SELECT payment_id, status, 'FUNDS' as source_system FROM funds_status; -- Combine payment and status events in 1 hour window. Why we need a timing window for stream-stream join? CREATE STREAM payments_with_status AS SELECT ep.payment_id as payment_id, ep.accountid, ep.amount, ep.bank, ep.first_name, ep.last_name, ep.email, ep.status360, ps.status, ps.source_system FROM enriched_payments ep LEFT JOIN payment_statuses ps WITHIN 1 HOUR ON ep.payment_id = ps.payment_id ; -- Aggregate data to the final table CREATE TABLE payments_final AS SELECT payment_id, histogram(status) as status_counts, collect_list('{ \"system\" : \"' + source_system + '\", \"status\" : \"' + STATUS + '\"}') as service_status_list from payments_with_status where status is not null group by payment_id; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: -- Customer Data INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (10,'Brena','Tollerton','btollerton9@furl.net','Female','silver'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (9,'Even','Tinham','etinham8@facebook.com','Male','silver'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (8,'Patti','Rosten','prosten7@ihg.com','Female','silver'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (7,'Fay','Huc','fhuc6@quantcast.com','Female','bronze'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (6,'Robinet','Leheude','rleheude5@reddit.com','Female','platinum'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (5,'Hansiain','Coda','hcoda4@senate.gov','Male','platinum'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (4,'Hashim','Rumke','hrumke3@sohu.com','Male','platinum'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (3,'Mariejeanne','Cocci','mcocci2@techcrunch.com','Female','bronze'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (2,'Ruthie','Brockherst','rbrockherst1@ow.ly','Female','platinum'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (1,'Rica','Blaisdell','rblaisdell0@rambler.ru','Female','bronze'); -- Payment Instruction Data INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (1,1,1234000,100,'DBS'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (3,2,1234100,200,'Barclays Bank'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (5,3,1234200,300,'BNP Paribas'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (7,4,1234300,400,'Wells Fargo'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (9,5,1234400,500,'DBS'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (11,6,1234500,600,'Royal Bank of Canada'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (13,7,1234600,700,'DBS'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (15,8,1234700,800,'DBS'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (17,9,1234800,900,'DBS'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (19,10,1234900,1000,'United Overseas Bank'); -- ALM Status Data INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (1,'Wells Fargo','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (3,'Commonwealth Bank of Australia','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (5,'Deutsche Bank','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (7,'DBS','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (9,'United Overseas Bank','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (11,'Citi','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (13,'Commonwealth Bank of Australia','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (15,'Barclays Bank','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (17,'United Overseas Bank','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (19,'Royal Bank of Canada','OK'); -- Funds Status Data INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (1,'00','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (3,'99','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (5,'30','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (7,'00','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (9,'00','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (11,'00','NOT OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (13,'30','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (15,'00','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (17,'10','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (19,'10','OK');","title":"Run stream processing app"},{"location":"fin-serv/payment-status-check/#full-ksqldb-statements","text":"Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-customers', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'customers', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of payments CREATE SOURCE CONNECTOR payments WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-payments', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'payments', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of aml_status CREATE SOURCE CONNECTOR aml_status WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-aml_status', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'aml_status', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of funds_status CREATE SOURCE CONNECTOR funds_status WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-funds_status', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'funds_status', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Register the initial streams and tables from the Kafka topics CREATE STREAM PAYMENTS ( PAYMENT_ID INTEGER KEY, CUSTID INTEGER, ACCOUNTID INTEGER, AMOUNT INTEGER, BANK VARCHAR ) WITH ( kafka_topic='payments', value_format='json' ); create stream aml_status ( PAYMENT_ID INTEGER, BANK VARCHAR, STATUS VARCHAR ) with ( kafka_topic='aml_status', value_format='json' ); create stream funds_status ( PAYMENT_ID INTEGER, REASON_CODE VARCHAR, STATUS VARCHAR ) with ( kafka_topic='funds_status', value_format='json' ); create table customers ( ID INTEGER PRIMARY KEY, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, GENDER VARCHAR, STATUS360 VARCHAR ) WITH ( kafka_topic='customers', value_format='JSON' ); -- Enrich Payments stream with Customers table create stream enriched_payments as select p.payment_id as payment_id, p.custid as customer_id, p.accountid, p.amount, p.bank, c.first_name, c.last_name, c.email, c.status360 from payments p left join customers c on p.custid = c.id; -- Combine the status streams CREATE STREAM payment_statuses AS SELECT payment_id, status, 'AML' as source_system FROM aml_status; INSERT INTO payment_statuses SELECT payment_id, status, 'FUNDS' as source_system FROM funds_status; -- Combine payment and status events in 1 hour window. Why we need a timing window for stream-stream join? CREATE STREAM payments_with_status AS SELECT ep.payment_id as payment_id, ep.accountid, ep.amount, ep.bank, ep.first_name, ep.last_name, ep.email, ep.status360, ps.status, ps.source_system FROM enriched_payments ep LEFT JOIN payment_statuses ps WITHIN 1 HOUR ON ep.payment_id = ps.payment_id ; -- Aggregate data to the final table CREATE TABLE payments_final AS SELECT payment_id, histogram(status) as status_counts, collect_list('{ \"system\" : \"' + source_system + '\", \"status\" : \"' + STATUS + '\"}') as service_status_list from payments_with_status where status is not null group by payment_id;","title":"Full ksqlDB Statements"},{"location":"retail/fleet_management/","text":"Fleet Management What is it? More and more fleet management relies on knowing real-time information on vehicles, their locations, and vehicle telemetry. This enables businesses to improve route efficiency, fuel efficiency, automate service schedules, etc. This recipe enriches fleet location with information about each vehicle to be able to have a real-time view with consolidation information on the entire fleet. TODO--add diagram Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data. Read the data in Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). -- Stream of fleet descriptions CREATE SOURCE CONNECTOR fleet_description WITH ( 'connector.class' = 'MongoDbAtlasSource', 'name' = 'recipe-mongodb-fleet_description', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<database-host-address>', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'database' = '<database-name>', 'collection' = '<database-collection-name>', 'poll.await.time.ms' = '5000', 'poll.max.batch.size' = '1000', 'copy.existing' = 'true', 'output.data.format' = 'JSON' 'tasks.max' = '1'); -- Stream of current location of each vehicle in the fleet CREATE SOURCE CONNECTOR fleet_location WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-fleet_location', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'fleet_location', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams. Run stream processing app Now you can process the data in a variety of ways. In this case, the original fleet telemetry is enriched with details about the vehicle. -- create stream of locations CREATE STREAM locations ( vehicle_id int, latitude double, longitude double, timestamp varchar ) WITH ( kafka_topic = 'locations', value_format = 'json', partitions = 6 ); -- fleet lookup table CREATE TABLE fleet ( vehicle_id int primary key, driver_id int, license bigint ) WITH ( kafka_topic = 'descriptions', value_format = 'json', partitions = 6 ); -- enrich fleet location stream with more fleet information CREATE STREAM fleet_location_enhanced AS SELECT l.vehicle_id, latitude, longitude, timestamp, f.driver_id, f.license FROM locations l LEFT JOIN fleet f ON l.vehicle_id = f.vehicle_id; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO locations (vehicle_id, latitude, longitude, timestamp) VALUES (5401, 16.7587537, 96.2482149, '2021-09-23T10:50:00.000Z'); INSERT INTO locations (vehicle_id, latitude, longitude, timestamp) VALUES (5401, 16.004175, 120.7806412, '2021-09-27T06:39:00.000Z'); INSERT INTO locations (vehicle_id, latitude, longitude, timestamp) VALUES (5402, 32.755613, 22.6377432, '2021-09-25T20:22:00.000Z'); INSERT INTO fleet (vehicle_id, driver_id, license) VALUES (5401, 847383, 8852693196); INSERT INTO fleet (vehicle_id, driver_id, license) VALUES (5402, 922947, 1255144201); INSERT INTO fleet (vehicle_id, driver_id, license) VALUES (5403, 435309, 2132311746); Full ksqlDB Statements Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of fleet descriptions CREATE SOURCE CONNECTOR fleet_description WITH ( 'connector.class' = 'MongoDbAtlasSource', 'name' = 'recipe-mongodb-fleet_description', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<database-host-address>', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'database' = '<database-name>', 'collection' = '<database-collection-name>', 'poll.await.time.ms' = '5000', 'poll.max.batch.size' = '1000', 'copy.existing' = 'true', 'output.data.format' = 'JSON' 'tasks.max' = '1'); -- Stream of current location of each vehicle in the fleet CREATE SOURCE CONNECTOR fleet_location WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-fleet_location', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'fleet_location', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- create stream of locations CREATE STREAM locations ( vehicle_id int, latitude double, longitude double, timestamp varchar ) WITH ( kafka_topic = 'locations', value_format = 'json', partitions = 6 ); -- fleet lookup table CREATE TABLE fleet ( vehicle_id int primary key, driver_id int, license bigint ) WITH ( kafka_topic = 'descriptions', value_format = 'json', partitions = 6 ); -- enrich fleet location stream with more fleet information CREATE STREAM fleet_location_enhanced AS SELECT l.vehicle_id, latitude, longitude, timestamp, f.driver_id, f.license FROM locations l LEFT JOIN fleet f ON l.vehicle_id = f.vehicle_id;","title":"Fleet Management"},{"location":"retail/fleet_management/#fleet-management","text":"","title":"Fleet Management"},{"location":"retail/fleet_management/#what-is-it","text":"More and more fleet management relies on knowing real-time information on vehicles, their locations, and vehicle telemetry. This enables businesses to improve route efficiency, fuel efficiency, automate service schedules, etc. This recipe enriches fleet location with information about each vehicle to be able to have a real-time view with consolidation information on the entire fleet. TODO--add diagram","title":"What is it?"},{"location":"retail/fleet_management/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"retail/fleet_management/#step-by-step","text":"","title":"Step-by-step"},{"location":"retail/fleet_management/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data.","title":"Setup your Environment"},{"location":"retail/fleet_management/#read-the-data-in","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). -- Stream of fleet descriptions CREATE SOURCE CONNECTOR fleet_description WITH ( 'connector.class' = 'MongoDbAtlasSource', 'name' = 'recipe-mongodb-fleet_description', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<database-host-address>', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'database' = '<database-name>', 'collection' = '<database-collection-name>', 'poll.await.time.ms' = '5000', 'poll.max.batch.size' = '1000', 'copy.existing' = 'true', 'output.data.format' = 'JSON' 'tasks.max' = '1'); -- Stream of current location of each vehicle in the fleet CREATE SOURCE CONNECTOR fleet_location WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-fleet_location', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'fleet_location', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams.","title":"Read the data in"},{"location":"retail/fleet_management/#run-stream-processing-app","text":"Now you can process the data in a variety of ways. In this case, the original fleet telemetry is enriched with details about the vehicle. -- create stream of locations CREATE STREAM locations ( vehicle_id int, latitude double, longitude double, timestamp varchar ) WITH ( kafka_topic = 'locations', value_format = 'json', partitions = 6 ); -- fleet lookup table CREATE TABLE fleet ( vehicle_id int primary key, driver_id int, license bigint ) WITH ( kafka_topic = 'descriptions', value_format = 'json', partitions = 6 ); -- enrich fleet location stream with more fleet information CREATE STREAM fleet_location_enhanced AS SELECT l.vehicle_id, latitude, longitude, timestamp, f.driver_id, f.license FROM locations l LEFT JOIN fleet f ON l.vehicle_id = f.vehicle_id; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO locations (vehicle_id, latitude, longitude, timestamp) VALUES (5401, 16.7587537, 96.2482149, '2021-09-23T10:50:00.000Z'); INSERT INTO locations (vehicle_id, latitude, longitude, timestamp) VALUES (5401, 16.004175, 120.7806412, '2021-09-27T06:39:00.000Z'); INSERT INTO locations (vehicle_id, latitude, longitude, timestamp) VALUES (5402, 32.755613, 22.6377432, '2021-09-25T20:22:00.000Z'); INSERT INTO fleet (vehicle_id, driver_id, license) VALUES (5401, 847383, 8852693196); INSERT INTO fleet (vehicle_id, driver_id, license) VALUES (5402, 922947, 1255144201); INSERT INTO fleet (vehicle_id, driver_id, license) VALUES (5403, 435309, 2132311746);","title":"Run stream processing app"},{"location":"retail/fleet_management/#full-ksqldb-statements","text":"Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of fleet descriptions CREATE SOURCE CONNECTOR fleet_description WITH ( 'connector.class' = 'MongoDbAtlasSource', 'name' = 'recipe-mongodb-fleet_description', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<database-host-address>', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'database' = '<database-name>', 'collection' = '<database-collection-name>', 'poll.await.time.ms' = '5000', 'poll.max.batch.size' = '1000', 'copy.existing' = 'true', 'output.data.format' = 'JSON' 'tasks.max' = '1'); -- Stream of current location of each vehicle in the fleet CREATE SOURCE CONNECTOR fleet_location WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-fleet_location', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'fleet_location', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- create stream of locations CREATE STREAM locations ( vehicle_id int, latitude double, longitude double, timestamp varchar ) WITH ( kafka_topic = 'locations', value_format = 'json', partitions = 6 ); -- fleet lookup table CREATE TABLE fleet ( vehicle_id int primary key, driver_id int, license bigint ) WITH ( kafka_topic = 'descriptions', value_format = 'json', partitions = 6 ); -- enrich fleet location stream with more fleet information CREATE STREAM fleet_location_enhanced AS SELECT l.vehicle_id, latitude, longitude, timestamp, f.driver_id, f.license FROM locations l LEFT JOIN fleet f ON l.vehicle_id = f.vehicle_id;","title":"Full ksqlDB Statements"},{"location":"retail/inventory/","text":"Real-time Inventory What is it? Having an up-to-date view of inventory on every item is essential in today's online marketplaces. This helps businesses maintain the right amount of inventory\u2014not too much and not too little\u2014so that they can meet demand while minimizing costs. This recipe demonstrates how to update inventory in real time and always have an up-to-date snapshot of your stock. Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-Step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data. Read the data in Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). For this recipe, we are interested in knowing each event for an item that affects its quantity. This creates a stream of events, where each event results in the addition or removal of inventory. CREATE SOURCE CONNECTOR inventory WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-inventory', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = '<table>', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams. Run stream processing app Create a ksqlDB TABLE , which is a mutable, partitioned collection that models change over time that represents what is true as of \"now\". -- Create stream of inventory CREATE STREAM inventory_stream ( id STRING key, item STRING, quantity INTEGER, price DOUBLE ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='inventory', PARTITIONS = 6); -- Create stateful table with up-to-date information of inventory availability CREATE TABLE inventory_stream_table WITH (kafka_topic='inventory_table') AS SELECT item, SUM(quantity) AS item_quantity FROM inventory_stream GROUP BY item EMIT CHANGES; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('1', 'Apple Magic Mouse 2', 10, 99.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('2', 'iPhoneX', 25, 999.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('3', 'MacBookPro13', 100, 1799.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('4', 'iPad4', 20, 340.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('5', 'Apple Pencil', 10, 79.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('5', 'PhoneX', 10, 899.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('4', 'iPad4', -20, 399.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('3', 'MacBookPro13', 10, 1899.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('4', 'iPad4', 20, 399.00); Full ksqlDB Statements Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. CREATE SOURCE CONNECTOR inventory WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-inventory', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = '<table>', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Create stream of inventory CREATE STREAM inventory_stream ( id STRING key, item STRING, quantity INTEGER, price DOUBLE ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='inventory', PARTITIONS = 6); -- Create stateful table with up-to-date information of inventory availability CREATE TABLE inventory_stream_table WITH (kafka_topic='inventory_table') AS SELECT item, SUM(quantity) AS item_quantity FROM inventory_stream GROUP BY item EMIT CHANGES;","title":"Real-time Inventory"},{"location":"retail/inventory/#real-time-inventory","text":"","title":"Real-time Inventory"},{"location":"retail/inventory/#what-is-it","text":"Having an up-to-date view of inventory on every item is essential in today's online marketplaces. This helps businesses maintain the right amount of inventory\u2014not too much and not too little\u2014so that they can meet demand while minimizing costs. This recipe demonstrates how to update inventory in real time and always have an up-to-date snapshot of your stock.","title":"What is it?"},{"location":"retail/inventory/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"retail/inventory/#step-by-step","text":"","title":"Step-by-Step"},{"location":"retail/inventory/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data.","title":"Setup your Environment"},{"location":"retail/inventory/#read-the-data-in","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). For this recipe, we are interested in knowing each event for an item that affects its quantity. This creates a stream of events, where each event results in the addition or removal of inventory. CREATE SOURCE CONNECTOR inventory WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-inventory', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = '<table>', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams.","title":"Read the data in"},{"location":"retail/inventory/#run-stream-processing-app","text":"Create a ksqlDB TABLE , which is a mutable, partitioned collection that models change over time that represents what is true as of \"now\". -- Create stream of inventory CREATE STREAM inventory_stream ( id STRING key, item STRING, quantity INTEGER, price DOUBLE ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='inventory', PARTITIONS = 6); -- Create stateful table with up-to-date information of inventory availability CREATE TABLE inventory_stream_table WITH (kafka_topic='inventory_table') AS SELECT item, SUM(quantity) AS item_quantity FROM inventory_stream GROUP BY item EMIT CHANGES; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('1', 'Apple Magic Mouse 2', 10, 99.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('2', 'iPhoneX', 25, 999.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('3', 'MacBookPro13', 100, 1799.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('4', 'iPad4', 20, 340.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('5', 'Apple Pencil', 10, 79.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('5', 'PhoneX', 10, 899.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('4', 'iPad4', -20, 399.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('3', 'MacBookPro13', 10, 1899.00); INSERT INTO inventory_stream (id, item, quantity, price) VALUES ('4', 'iPad4', 20, 399.00);","title":"Run stream processing app"},{"location":"retail/inventory/#full-ksqldb-statements","text":"Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. CREATE SOURCE CONNECTOR inventory WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-inventory', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = '<table>', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Create stream of inventory CREATE STREAM inventory_stream ( id STRING key, item STRING, quantity INTEGER, price DOUBLE ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='inventory', PARTITIONS = 6); -- Create stateful table with up-to-date information of inventory availability CREATE TABLE inventory_stream_table WITH (kafka_topic='inventory_table') AS SELECT item, SUM(quantity) AS item_quantity FROM inventory_stream GROUP BY item EMIT CHANGES;","title":"Full ksqlDB Statements"},{"location":"security/ssh-attack/","text":"Detecting and Analyzing SSH Attacks What is it? There are lots of ways SSH can be abused but one of the most straightforward ways to detect a problem is to monitor for rejected logins. This recipe processes Syslog data to detect bad logins and streams out those pairs of usernames and IP addresses. With ksqlDB, you can filter and react to events in real time rather than performing historical analysis of Syslog data from cold storage. Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-Step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data. Read the data in This recipe is a great demonstration on how to run a self-managed connector, to push syslog data into Confluent Cloud into a Kafka topic called syslog . Create a file called Dockerfile to bundle a connect worker with kafka-connect-syslog : FROM confluentinc/cp-server-connect-base:6.2.0 ENV CONNECT_PLUGIN_PATH=\"/usr/share/java,/usr/share/confluent-hub-components\" USER root COPY --chown=appuser:appuser include/etc/confluent/docker /etc/confluent/docker USER appuser ARG CONNECTOR_OWNER=confluentinc ARG CONNECTOR_NAME ARG CONNECTOR_VERSION RUN confluent-hub install --no-prompt confluent-hub install confluentinc/kafka-connect-syslog:1.3.4 CMD [\"/etc/confluent/docker/run\"] Build the custom Docker image with the command: docker build \\ -t localbuild/connect_distributed_with_syslog:1.3.4 \\ -f Dockerfile . Create a file called docker-compose.yml with the following content, substituting your Confluent Cloud connection information: --- version: '2' services: connect: image: localbuild/connect_distributed_with_syslog:1.3.4 hostname: connect container_name: connect ports: - \"8083:8083\" environment: CONNECT_BOOTSTRAP_SERVERS: $BOOTSTRAP_SERVERS CONNECT_REST_PORT: 8083 CONNECT_GROUP_ID: \"connect\" CONNECT_CONFIG_STORAGE_TOPIC: recipe-connect-configs CONNECT_OFFSET_STORAGE_TOPIC: recipe-connect-offsets CONNECT_STATUS_STORAGE_TOPIC: recipe-connect-status CONNECT_REPLICATION_FACTOR: 3 CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3 CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3 CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3 CONNECT_KEY_CONVERTER: \"org.apache.kafka.connect.storage.StringConverter\" CONNECT_VALUE_CONVERTER: \"org.apache.kafka.connect.json.JsonConverter\" CONNECT_REST_ADVERTISED_HOST_NAME: \"connect\" CONNECT_PLUGIN_PATH: \"/usr/share/java,/usr/share/confluent-hub-components\" CONNECT_LOG4J_ROOT_LOGLEVEL: INFO CONNECT_LOG4J_LOGGERS: org.reflections=ERROR # Connect worker CONNECT_SECURITY_PROTOCOL: SASL_SSL CONNECT_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_SASL_MECHANISM: PLAIN # Connect producer CONNECT_PRODUCER_SECURITY_PROTOCOL: SASL_SSL CONNECT_PRODUCER_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_PRODUCER_SASL_MECHANISM: PLAIN # Connect consumer CONNECT_CONSUMER_SECURITY_PROTOCOL: SASL_SSL CONNECT_CONSUMER_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_CONSUMER_SASL_MECHANISM: PLAIN Run the container with: docker-compose up -d Now you should have Syslog messages being written to the topic syslog in Confluent Cloud. Run stream processing app Process the syslog events by flagging events with invalid users, stripping out all the other unnecessary fields, and creating just a stream of relevant information. There are many ways to customize the resulting stream to fit the business needs: this example also demonstrates how to enrich the stream with a new field FACILITY_DESCRIPTION with human-readable content. -- Define Kafka parameters DEFINE topic = 'syslog'; -- Extract relevant fields from log messages CREATE OR REPLACE STREAM `syslog` WITH ( KAFKA_TOPIC = '${topic}', VALUE_FORMAT = 'json', PARTITIONS = 6 ); CREATE STREAM `by_facility` AS SELECT TIMESTAMPTOSTRING(TIMESTAMP, 'yyyy-MM-dd HH:mm:ss') AS SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS FROM `syslog` PARTITION BY FACILITY EMIT CHANGES; -- Flag events with invalid users, and enrich with a new field 'FACILITY_DESCRIPTION' CREATE STREAM `invalid_users` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS, CASE WHEN FACILITY = 0 THEN 'kernel messages' WHEN FACILITY = 1 THEN 'user-level messages' WHEN FACILITY = 2 THEN 'mail system' WHEN FACILITY = 3 THEN 'system daemons' WHEN FACILITY = 4 THEN 'security/authorization messages' WHEN FACILITY = 5 THEN 'messages generated internally by syslogd' WHEN FACILITY = 6 THEN 'line printer subsystem' ELSE '<unknown>' END AS FACILITY_DESCRIPTION FROM `by_facility` WHERE MESSAGE LIKE 'Invalid user%' EMIT CHANGES; -- Create actionable stream of SSH attacks, enriched with user and IP CREATE STREAM `ssh_attacks` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY_DESCRIPTION, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[1] AS ATTACK_USER, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[2] AS ATTACK_IP FROM `invalid_users` EMIT CHANGES; Full ksqlDB Statements Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. Run the Syslog source connector locally, then proceed with ksqlDB to process the Syslog messages. -- Define Kafka parameters DEFINE topic = 'syslog'; -- Extract relevant fields from log messages CREATE OR REPLACE STREAM `syslog` WITH ( KAFKA_TOPIC = '${topic}', VALUE_FORMAT = 'json', PARTITIONS = 6 ); CREATE STREAM `by_facility` AS SELECT TIMESTAMPTOSTRING(TIMESTAMP, 'yyyy-MM-dd HH:mm:ss') AS SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS FROM `syslog` PARTITION BY FACILITY EMIT CHANGES; -- Flag events with invalid users, and enrich with a new field 'FACILITY_DESCRIPTION' CREATE STREAM `invalid_users` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS, CASE WHEN FACILITY = 0 THEN 'kernel messages' WHEN FACILITY = 1 THEN 'user-level messages' WHEN FACILITY = 2 THEN 'mail system' WHEN FACILITY = 3 THEN 'system daemons' WHEN FACILITY = 4 THEN 'security/authorization messages' WHEN FACILITY = 5 THEN 'messages generated internally by syslogd' WHEN FACILITY = 6 THEN 'line printer subsystem' ELSE '<unknown>' END AS FACILITY_DESCRIPTION FROM `by_facility` WHERE MESSAGE LIKE 'Invalid user%' EMIT CHANGES; -- Create actionable stream of SSH attacks, enriched with user and IP CREATE STREAM `ssh_attacks` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY_DESCRIPTION, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[1] AS ATTACK_USER, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[2] AS ATTACK_IP FROM `invalid_users` EMIT CHANGES;","title":"Detecting and Analyzing SSH Attacks"},{"location":"security/ssh-attack/#detecting-and-analyzing-ssh-attacks","text":"","title":"Detecting and Analyzing SSH Attacks"},{"location":"security/ssh-attack/#what-is-it","text":"There are lots of ways SSH can be abused but one of the most straightforward ways to detect a problem is to monitor for rejected logins. This recipe processes Syslog data to detect bad logins and streams out those pairs of usernames and IP addresses. With ksqlDB, you can filter and react to events in real time rather than performing historical analysis of Syslog data from cold storage.","title":"What is it?"},{"location":"security/ssh-attack/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"security/ssh-attack/#step-by-step","text":"","title":"Step-by-Step"},{"location":"security/ssh-attack/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data.","title":"Setup your Environment"},{"location":"security/ssh-attack/#read-the-data-in","text":"This recipe is a great demonstration on how to run a self-managed connector, to push syslog data into Confluent Cloud into a Kafka topic called syslog . Create a file called Dockerfile to bundle a connect worker with kafka-connect-syslog : FROM confluentinc/cp-server-connect-base:6.2.0 ENV CONNECT_PLUGIN_PATH=\"/usr/share/java,/usr/share/confluent-hub-components\" USER root COPY --chown=appuser:appuser include/etc/confluent/docker /etc/confluent/docker USER appuser ARG CONNECTOR_OWNER=confluentinc ARG CONNECTOR_NAME ARG CONNECTOR_VERSION RUN confluent-hub install --no-prompt confluent-hub install confluentinc/kafka-connect-syslog:1.3.4 CMD [\"/etc/confluent/docker/run\"] Build the custom Docker image with the command: docker build \\ -t localbuild/connect_distributed_with_syslog:1.3.4 \\ -f Dockerfile . Create a file called docker-compose.yml with the following content, substituting your Confluent Cloud connection information: --- version: '2' services: connect: image: localbuild/connect_distributed_with_syslog:1.3.4 hostname: connect container_name: connect ports: - \"8083:8083\" environment: CONNECT_BOOTSTRAP_SERVERS: $BOOTSTRAP_SERVERS CONNECT_REST_PORT: 8083 CONNECT_GROUP_ID: \"connect\" CONNECT_CONFIG_STORAGE_TOPIC: recipe-connect-configs CONNECT_OFFSET_STORAGE_TOPIC: recipe-connect-offsets CONNECT_STATUS_STORAGE_TOPIC: recipe-connect-status CONNECT_REPLICATION_FACTOR: 3 CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3 CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3 CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3 CONNECT_KEY_CONVERTER: \"org.apache.kafka.connect.storage.StringConverter\" CONNECT_VALUE_CONVERTER: \"org.apache.kafka.connect.json.JsonConverter\" CONNECT_REST_ADVERTISED_HOST_NAME: \"connect\" CONNECT_PLUGIN_PATH: \"/usr/share/java,/usr/share/confluent-hub-components\" CONNECT_LOG4J_ROOT_LOGLEVEL: INFO CONNECT_LOG4J_LOGGERS: org.reflections=ERROR # Connect worker CONNECT_SECURITY_PROTOCOL: SASL_SSL CONNECT_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_SASL_MECHANISM: PLAIN # Connect producer CONNECT_PRODUCER_SECURITY_PROTOCOL: SASL_SSL CONNECT_PRODUCER_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_PRODUCER_SASL_MECHANISM: PLAIN # Connect consumer CONNECT_CONSUMER_SECURITY_PROTOCOL: SASL_SSL CONNECT_CONSUMER_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_CONSUMER_SASL_MECHANISM: PLAIN Run the container with: docker-compose up -d Now you should have Syslog messages being written to the topic syslog in Confluent Cloud.","title":"Read the data in"},{"location":"security/ssh-attack/#run-stream-processing-app","text":"Process the syslog events by flagging events with invalid users, stripping out all the other unnecessary fields, and creating just a stream of relevant information. There are many ways to customize the resulting stream to fit the business needs: this example also demonstrates how to enrich the stream with a new field FACILITY_DESCRIPTION with human-readable content. -- Define Kafka parameters DEFINE topic = 'syslog'; -- Extract relevant fields from log messages CREATE OR REPLACE STREAM `syslog` WITH ( KAFKA_TOPIC = '${topic}', VALUE_FORMAT = 'json', PARTITIONS = 6 ); CREATE STREAM `by_facility` AS SELECT TIMESTAMPTOSTRING(TIMESTAMP, 'yyyy-MM-dd HH:mm:ss') AS SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS FROM `syslog` PARTITION BY FACILITY EMIT CHANGES; -- Flag events with invalid users, and enrich with a new field 'FACILITY_DESCRIPTION' CREATE STREAM `invalid_users` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS, CASE WHEN FACILITY = 0 THEN 'kernel messages' WHEN FACILITY = 1 THEN 'user-level messages' WHEN FACILITY = 2 THEN 'mail system' WHEN FACILITY = 3 THEN 'system daemons' WHEN FACILITY = 4 THEN 'security/authorization messages' WHEN FACILITY = 5 THEN 'messages generated internally by syslogd' WHEN FACILITY = 6 THEN 'line printer subsystem' ELSE '<unknown>' END AS FACILITY_DESCRIPTION FROM `by_facility` WHERE MESSAGE LIKE 'Invalid user%' EMIT CHANGES; -- Create actionable stream of SSH attacks, enriched with user and IP CREATE STREAM `ssh_attacks` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY_DESCRIPTION, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[1] AS ATTACK_USER, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[2] AS ATTACK_IP FROM `invalid_users` EMIT CHANGES;","title":"Run stream processing app"},{"location":"security/ssh-attack/#full-ksqldb-statements","text":"Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. Run the Syslog source connector locally, then proceed with ksqlDB to process the Syslog messages. -- Define Kafka parameters DEFINE topic = 'syslog'; -- Extract relevant fields from log messages CREATE OR REPLACE STREAM `syslog` WITH ( KAFKA_TOPIC = '${topic}', VALUE_FORMAT = 'json', PARTITIONS = 6 ); CREATE STREAM `by_facility` AS SELECT TIMESTAMPTOSTRING(TIMESTAMP, 'yyyy-MM-dd HH:mm:ss') AS SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS FROM `syslog` PARTITION BY FACILITY EMIT CHANGES; -- Flag events with invalid users, and enrich with a new field 'FACILITY_DESCRIPTION' CREATE STREAM `invalid_users` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS, CASE WHEN FACILITY = 0 THEN 'kernel messages' WHEN FACILITY = 1 THEN 'user-level messages' WHEN FACILITY = 2 THEN 'mail system' WHEN FACILITY = 3 THEN 'system daemons' WHEN FACILITY = 4 THEN 'security/authorization messages' WHEN FACILITY = 5 THEN 'messages generated internally by syslogd' WHEN FACILITY = 6 THEN 'line printer subsystem' ELSE '<unknown>' END AS FACILITY_DESCRIPTION FROM `by_facility` WHERE MESSAGE LIKE 'Invalid user%' EMIT CHANGES; -- Create actionable stream of SSH attacks, enriched with user and IP CREATE STREAM `ssh_attacks` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY_DESCRIPTION, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[1] AS ATTACK_USER, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[2] AS ATTACK_IP FROM `invalid_users` EMIT CHANGES;","title":"Full ksqlDB Statements"},{"location":"shared/ccloud_launch/","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Ccloud launch"},{"location":"shared/ccloud_setup/","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which now has connector integration for reading and writing data to other data sources and sinks, as well as language for processing the data.","title":"Ccloud setup"},{"location":"shared/code_summary/","text":"Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above.","title":"Code summary"},{"location":"shared/connect/","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ).","title":"Connect"},{"location":"shared/manual_cue/","text":"In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements:","title":"Manual cue"},{"location":"shared/manual_insert/","text":"","title":"Manual insert"}]}
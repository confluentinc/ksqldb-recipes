{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Event Streaming Patterns Event Streaming Platforms provide an architecture that enables software to react and operate as events occur. These platforms allow for software components to work together in a real-time, decoupled, and scalable fashion. When software is modeled as streams of events, new capabilities surface along with new unique technical challenges. This catalog contains simple and reusable architectural patterns that can be applied over event streaming systems. When composed together, these patterns can help meet the design demands of modern real-time distributed systems. The following patterns are categorized by their function in the event streaming system, including sourcing data, processing events as streams, to integrations with external systems.","title":"Event Streaming Patterns"},{"location":"#welcome-to-event-streaming-patterns","text":"Event Streaming Platforms provide an architecture that enables software to react and operate as events occur. These platforms allow for software components to work together in a real-time, decoupled, and scalable fashion. When software is modeled as streams of events, new capabilities surface along with new unique technical challenges. This catalog contains simple and reusable architectural patterns that can be applied over event streaming systems. When composed together, these patterns can help meet the design demands of modern real-time distributed systems. The following patterns are categorized by their function in the event streaming system, including sourcing data, processing events as streams, to integrations with external systems.","title":"Welcome to Event Streaming Patterns"},{"location":"compositional-patterns/command-query-responsibility-segregation/","text":"Command Query Responsibility Segregation (CQRS) Databases conflate the writing of data and the reading of data in the same place: the database. In some situations, it is preferable to separate reads from writes. There are several reasons to do this but the most prevalent is that the application can now save data in the exact form in which it arrives, accurately reflecting what happened in the real world, while reading it in a different form, one that is optimized for reading. For example, a user adding and removing items from their cart would all be recorded as a stream of immutable events: t-shirt added, t-shirt removed, etc. These are then summarized into a separate view that used to serve reads, for example summarizing the various user events to represent the accurate contents of the cart. Problem How can we store and hold data in the exact form in which it arrived but read from a summarized and curated view? Solution Represent changes that happen in the real world as Events - an order is shipped, a ride is accepted, etc. - and retain these events as the system of record. Subsequently, aggregate those Events into a view that summarizes the events to represent the current state, allowing applications to query the current values. So for example, the current balance of an account would be the total of all the payment events that added money to or removed it from the account. The system of record is the stream of payment events. The view we read from would be the account balance. Implementation The streaming database ksqlDB can implement a CQRS using an Event Stream and Table . Event Streams are built into to the streaming database design. Creating a new stream is straightforward: CREATE STREAM purchases (customer VARCHAR, item VARCHAR, qty INT WITH (kafka_topic='purchases-topic', value_format='json', partitions=1); Events can be directly using familiar SQL syntax. INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'hats', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'hats', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'pants', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'sweaters', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'pants', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'pants', -1); We can create a Materialized View of the data as a Table : CREATE TABLE customer_purchases WITH (KEY_FORMAT='JSON') AS SELECT customer, item, SUM(qty) as total_qty from purchases GROUP BY customer, item emit changes; And continuously query for changes to the state of the customer_purchases table: SELECT * FROM customer_purchases EMIT CHANGES; Considerations CQRS adds complexity over a traditional simple CRUD database implementation. High performance applications may benefit from a CQRS design. Isolating the load of writing and reading of data may allow us to scale those aspects independently and properly. Microservices applications often use CQRS to scale-out with many views provided for different services. The same pattern is applicable to geographically dispersed applications such as a flight booking system which are read heavy across many locations. A write to a CQRS system is eventually consistent. Writes cannot be read immediately as there is a delay between the write of the command Event and the query-model being updated. This can cause complexity for some client applications, particularly online services. References See Martin Fowler's detailed explanation of CQRS for more information.","title":"Command Query Responsibility Segregation (CQRS)"},{"location":"compositional-patterns/command-query-responsibility-segregation/#command-query-responsibility-segregation-cqrs","text":"Databases conflate the writing of data and the reading of data in the same place: the database. In some situations, it is preferable to separate reads from writes. There are several reasons to do this but the most prevalent is that the application can now save data in the exact form in which it arrives, accurately reflecting what happened in the real world, while reading it in a different form, one that is optimized for reading. For example, a user adding and removing items from their cart would all be recorded as a stream of immutable events: t-shirt added, t-shirt removed, etc. These are then summarized into a separate view that used to serve reads, for example summarizing the various user events to represent the accurate contents of the cart.","title":"Command Query Responsibility Segregation (CQRS)"},{"location":"compositional-patterns/command-query-responsibility-segregation/#problem","text":"How can we store and hold data in the exact form in which it arrived but read from a summarized and curated view?","title":"Problem"},{"location":"compositional-patterns/command-query-responsibility-segregation/#solution","text":"Represent changes that happen in the real world as Events - an order is shipped, a ride is accepted, etc. - and retain these events as the system of record. Subsequently, aggregate those Events into a view that summarizes the events to represent the current state, allowing applications to query the current values. So for example, the current balance of an account would be the total of all the payment events that added money to or removed it from the account. The system of record is the stream of payment events. The view we read from would be the account balance.","title":"Solution"},{"location":"compositional-patterns/command-query-responsibility-segregation/#implementation","text":"The streaming database ksqlDB can implement a CQRS using an Event Stream and Table . Event Streams are built into to the streaming database design. Creating a new stream is straightforward: CREATE STREAM purchases (customer VARCHAR, item VARCHAR, qty INT WITH (kafka_topic='purchases-topic', value_format='json', partitions=1); Events can be directly using familiar SQL syntax. INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'hats', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'hats', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'pants', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'sweaters', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'pants', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'pants', -1); We can create a Materialized View of the data as a Table : CREATE TABLE customer_purchases WITH (KEY_FORMAT='JSON') AS SELECT customer, item, SUM(qty) as total_qty from purchases GROUP BY customer, item emit changes; And continuously query for changes to the state of the customer_purchases table: SELECT * FROM customer_purchases EMIT CHANGES;","title":"Implementation"},{"location":"compositional-patterns/command-query-responsibility-segregation/#considerations","text":"CQRS adds complexity over a traditional simple CRUD database implementation. High performance applications may benefit from a CQRS design. Isolating the load of writing and reading of data may allow us to scale those aspects independently and properly. Microservices applications often use CQRS to scale-out with many views provided for different services. The same pattern is applicable to geographically dispersed applications such as a flight booking system which are read heavy across many locations. A write to a CQRS system is eventually consistent. Writes cannot be read immediately as there is a delay between the write of the command Event and the query-model being updated. This can cause complexity for some client applications, particularly online services.","title":"Considerations"},{"location":"compositional-patterns/command-query-responsibility-segregation/#references","text":"See Martin Fowler's detailed explanation of CQRS for more information.","title":"References"},{"location":"compositional-patterns/event-collaboration/","text":"Event Collaboration Building distributed business workflows requires coordinating multiple services and Event Processing Applications . Business actions and reactions must be coordinated asynchronously as complex workflows transition through various states. Problem How can we build a distributed workflow in a way that allows components to evolve independently? Solution Event Collaboration allows services and applications to collaborate around a single business workflow on top of an Event Streaming Platform . Service components publish Events to Event Streams as notification of the completion of a step in the workflow. The Events serve the additional purpose of carrying state information about the workflow which is used by downstream components in the next steps of the workflow. The process repeats the until the workflow is complete. Considerations In Event Collaboration, the logic for the choreography of the progression of the business workflow is decentralized and spread across many components. This contrasts with traditional orchestration design where the logic is isolated in a dedicated \"controller\" service that coordinates the actions and reactions of the workflow components. With Event Collaboration, some workflow components will need to be able to ascertain the state of the workflow some time after they have generated their own Event. A classic example would be an order request service which generates a new order request event and wants to be notified when the order is complete. These Events need to be correlated through the distributed workflow to support such functionality. The Correlation Identifier pattern describes a method of coupling Events when processed asynchronously by way of a global identifier which traverses the workflow within the events. References Event Collaboration by Martin Fowler","title":"Event Collaboration"},{"location":"compositional-patterns/event-collaboration/#event-collaboration","text":"Building distributed business workflows requires coordinating multiple services and Event Processing Applications . Business actions and reactions must be coordinated asynchronously as complex workflows transition through various states.","title":"Event Collaboration"},{"location":"compositional-patterns/event-collaboration/#problem","text":"How can we build a distributed workflow in a way that allows components to evolve independently?","title":"Problem"},{"location":"compositional-patterns/event-collaboration/#solution","text":"Event Collaboration allows services and applications to collaborate around a single business workflow on top of an Event Streaming Platform . Service components publish Events to Event Streams as notification of the completion of a step in the workflow. The Events serve the additional purpose of carrying state information about the workflow which is used by downstream components in the next steps of the workflow. The process repeats the until the workflow is complete.","title":"Solution"},{"location":"compositional-patterns/event-collaboration/#considerations","text":"In Event Collaboration, the logic for the choreography of the progression of the business workflow is decentralized and spread across many components. This contrasts with traditional orchestration design where the logic is isolated in a dedicated \"controller\" service that coordinates the actions and reactions of the workflow components. With Event Collaboration, some workflow components will need to be able to ascertain the state of the workflow some time after they have generated their own Event. A classic example would be an order request service which generates a new order request event and wants to be notified when the order is complete. These Events need to be correlated through the distributed workflow to support such functionality. The Correlation Identifier pattern describes a method of coupling Events when processed asynchronously by way of a global identifier which traverses the workflow within the events.","title":"Considerations"},{"location":"compositional-patterns/event-collaboration/#references","text":"Event Collaboration by Martin Fowler","title":"References"},{"location":"compositional-patterns/geo-replication/","text":"Geo Replication Many architectures have streams of events deployed across multiple datacenters spanning boundaries of Event Streaming Platforms , datacenters, or geographical regions. In these situations, it may be useful for client applications in one event streaming platform to have access to Events produced in another one. All clients shouldn't be forced to read from the source event streaming platform, which can incur high latency and data egress costs. Instead, with a move-once-read-many approach, the data can be replicated to a local datacenter where clients can do all their processing quickly and cheaply. Problem How can multiple Event Streaming Platforms be connected so that events available in one site are also available on the others? Solution Create a connection between the two Event Streaming Platforms , enabling the destination platform to read from the source one. Ideally this is done in realtime such that as new events are published in the source event streaming platform, they can be immediately copied, byte for byte, to the destination event streaming platform. This allows the client applications in the destination to leverage the same set of data. Implementation Typically, replication is not enabled on all event streams in practice. There are always exceptions, organizational limitations, technical constraints, or other reasons why we wouldn't want to copy absolutely everything. Instead, we can do this on a per-stream basis, where we can map a source stream to a destination stream. With Apache Kafka\u00ae, we can do this in one of several ways. Option 1: Cluster Linking Cluster Linking enables easy data sharing between event streaming platforms, mirroring Kafka topics (i.e., streams) across them. Because Cluster Linking uses native replication protocols, client applications can easily failover in the case of a disaster recovery scenario. ccloud kafka link create east-west ... ccloud kafka topic create <destination topic> --link east-west --mirror-topic <source topic> ... Other messaging systems like RabbitMQ and ActiveMQ provide similar functionality, but without the same levels of parallelism. Option 2: Connect-based Replication Operators can set up inter-cluster data flows with Confluent's Replicator or Kafka's MirrorMaker (version 2), tools that replicate data between different Kafka environments. Unlike Cluster Linking, these are separate services built upon Kafka Connect, with built-in producers and consumers. Considerations Note that this type of replication between event streaming platforms is asynchronous, which means an event that is recorded in the source may not be immediately available at the destination. There is also synchronous replication across event streaming platforms (e.g. Multi Region Clusters ) but this is often limited to when the event streaming platforms are in the same operational domain. References This pattern is derived from Messaging Bridge in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Geo Replication"},{"location":"compositional-patterns/geo-replication/#geo-replication","text":"Many architectures have streams of events deployed across multiple datacenters spanning boundaries of Event Streaming Platforms , datacenters, or geographical regions. In these situations, it may be useful for client applications in one event streaming platform to have access to Events produced in another one. All clients shouldn't be forced to read from the source event streaming platform, which can incur high latency and data egress costs. Instead, with a move-once-read-many approach, the data can be replicated to a local datacenter where clients can do all their processing quickly and cheaply.","title":"Geo Replication"},{"location":"compositional-patterns/geo-replication/#problem","text":"How can multiple Event Streaming Platforms be connected so that events available in one site are also available on the others?","title":"Problem"},{"location":"compositional-patterns/geo-replication/#solution","text":"Create a connection between the two Event Streaming Platforms , enabling the destination platform to read from the source one. Ideally this is done in realtime such that as new events are published in the source event streaming platform, they can be immediately copied, byte for byte, to the destination event streaming platform. This allows the client applications in the destination to leverage the same set of data.","title":"Solution"},{"location":"compositional-patterns/geo-replication/#implementation","text":"Typically, replication is not enabled on all event streams in practice. There are always exceptions, organizational limitations, technical constraints, or other reasons why we wouldn't want to copy absolutely everything. Instead, we can do this on a per-stream basis, where we can map a source stream to a destination stream. With Apache Kafka\u00ae, we can do this in one of several ways.","title":"Implementation"},{"location":"compositional-patterns/geo-replication/#option-1-cluster-linking","text":"Cluster Linking enables easy data sharing between event streaming platforms, mirroring Kafka topics (i.e., streams) across them. Because Cluster Linking uses native replication protocols, client applications can easily failover in the case of a disaster recovery scenario. ccloud kafka link create east-west ... ccloud kafka topic create <destination topic> --link east-west --mirror-topic <source topic> ... Other messaging systems like RabbitMQ and ActiveMQ provide similar functionality, but without the same levels of parallelism.","title":"Option 1: Cluster Linking"},{"location":"compositional-patterns/geo-replication/#option-2-connect-based-replication","text":"Operators can set up inter-cluster data flows with Confluent's Replicator or Kafka's MirrorMaker (version 2), tools that replicate data between different Kafka environments. Unlike Cluster Linking, these are separate services built upon Kafka Connect, with built-in producers and consumers.","title":"Option 2: Connect-based Replication"},{"location":"compositional-patterns/geo-replication/#considerations","text":"Note that this type of replication between event streaming platforms is asynchronous, which means an event that is recorded in the source may not be immediately available at the destination. There is also synchronous replication across event streaming platforms (e.g. Multi Region Clusters ) but this is often limited to when the event streaming platforms are in the same operational domain.","title":"Considerations"},{"location":"compositional-patterns/geo-replication/#references","text":"This pattern is derived from Messaging Bridge in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"compositional-patterns/pipeline/","text":"Pipeline A single Event Stream or Table can be used by multiple Event Processing Applications , and its Events may go through multiple processing stages along the way (e.g., filters, transformations, joins, aggregations) to implement more complex use cases. Problem How can a single processing objective for a set of Event Streams and/or Tables be achieved through a series of independent processing stages? Solution We can compose Event Streams and Tables in an Event Streaming Platform via an Event Processing Application to a create a pipeline\u2014also called a topology\u2014of Event Processors , which continuously process the events flowing through them. Here, the output of one processor is the input for one or more downstream processors. Pipelines, notably when created for use cases such as Streaming ETL , may include Event Source Connectors and Event Sink Connectors , which continuously import and export data as streams from/to external services and systems, respectively. Connectors are particularly useful for turning data at rest in such systems into data in motion. Taking a step back, we can see that pipelines in an Event Streaming Platform help companies build a \"central nervous system\" for data in motion. Implementation As an example we can use the streaming database ksqlDB to run a stream of events through a series of processing stages, thus creating a Pipeline that continuously processes data in motion. CREATE STREAM orders ( customer_id INTEGER, items ARRAY<STRUCT<name VARCHAR, price DOUBLE>> ) WITH ( KAFKA_TOPIC = 'orders', PARTITIONS = 1, VALUE_FORMAT = 'AVRO' ); We'll also create a (continuously updated) customers table that will contain the latest profile information about each customer, such as their current home address. CREATE TABLE customers ( customer_id INTEGER PRIMARY KEY, name VARCHAR, ADDRESS VARCHAR ) WITH ( KAFKA_TOPIC = 'customers', PARTITIONS = 1, VALUE_FORMAT = 'AVRO' ); Next, we create a new stream by joining the orders stream with our customer table: CREATE STREAM orders_enriched WITH (KAFKA_TOPIC='orders_enriched', PARTITIONS=1, VALUE_FORMAT='AVRO') AS SELECT o.customer_id AS cust_id, o.items, c.name, c.address FROM orders o LEFT JOIN customers c ON o.customer_id = c.customer_id EMIT CHANGES; Next, we create a stream, where we add the order total to each order by aggregating the price of the individual items in the order: CREATE STREAM orders_with_totals WITH (KAFKA_TOPIC='orders_totaled', PARTITIONS=1, VALUE_FORMAT='AVRO') AS SELECT cust_id, items, name, address, REDUCE(TRANSFORM(items, i=> i->price ), 0e0, (i,x) => (i + x)) AS total FROM orders_enriched EMIT CHANGES; Considerations The same event stream or table can participate in multiple pipelines. Because streams and tables are stored durably, applications have a lot of flexibility how and when they process the respective data, and they can do so independently from each other. The various processing stages in a pipeline create their own derived streams/tables (such as the orders_enriched stream in the ksqlDB example above), which in turn can be used as input for other pipelines and applications. This allows for further and more complex composition and re-use of events throughout an organization. References This pattern was influenced by Pipes and Filters in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf. However, it is much more powerful and flexible because it is using Event Streams as the pipes.","title":"Pipeline"},{"location":"compositional-patterns/pipeline/#pipeline","text":"A single Event Stream or Table can be used by multiple Event Processing Applications , and its Events may go through multiple processing stages along the way (e.g., filters, transformations, joins, aggregations) to implement more complex use cases.","title":"Pipeline"},{"location":"compositional-patterns/pipeline/#problem","text":"How can a single processing objective for a set of Event Streams and/or Tables be achieved through a series of independent processing stages?","title":"Problem"},{"location":"compositional-patterns/pipeline/#solution","text":"We can compose Event Streams and Tables in an Event Streaming Platform via an Event Processing Application to a create a pipeline\u2014also called a topology\u2014of Event Processors , which continuously process the events flowing through them. Here, the output of one processor is the input for one or more downstream processors. Pipelines, notably when created for use cases such as Streaming ETL , may include Event Source Connectors and Event Sink Connectors , which continuously import and export data as streams from/to external services and systems, respectively. Connectors are particularly useful for turning data at rest in such systems into data in motion. Taking a step back, we can see that pipelines in an Event Streaming Platform help companies build a \"central nervous system\" for data in motion.","title":"Solution"},{"location":"compositional-patterns/pipeline/#implementation","text":"As an example we can use the streaming database ksqlDB to run a stream of events through a series of processing stages, thus creating a Pipeline that continuously processes data in motion. CREATE STREAM orders ( customer_id INTEGER, items ARRAY<STRUCT<name VARCHAR, price DOUBLE>> ) WITH ( KAFKA_TOPIC = 'orders', PARTITIONS = 1, VALUE_FORMAT = 'AVRO' ); We'll also create a (continuously updated) customers table that will contain the latest profile information about each customer, such as their current home address. CREATE TABLE customers ( customer_id INTEGER PRIMARY KEY, name VARCHAR, ADDRESS VARCHAR ) WITH ( KAFKA_TOPIC = 'customers', PARTITIONS = 1, VALUE_FORMAT = 'AVRO' ); Next, we create a new stream by joining the orders stream with our customer table: CREATE STREAM orders_enriched WITH (KAFKA_TOPIC='orders_enriched', PARTITIONS=1, VALUE_FORMAT='AVRO') AS SELECT o.customer_id AS cust_id, o.items, c.name, c.address FROM orders o LEFT JOIN customers c ON o.customer_id = c.customer_id EMIT CHANGES; Next, we create a stream, where we add the order total to each order by aggregating the price of the individual items in the order: CREATE STREAM orders_with_totals WITH (KAFKA_TOPIC='orders_totaled', PARTITIONS=1, VALUE_FORMAT='AVRO') AS SELECT cust_id, items, name, address, REDUCE(TRANSFORM(items, i=> i->price ), 0e0, (i,x) => (i + x)) AS total FROM orders_enriched EMIT CHANGES;","title":"Implementation"},{"location":"compositional-patterns/pipeline/#considerations","text":"The same event stream or table can participate in multiple pipelines. Because streams and tables are stored durably, applications have a lot of flexibility how and when they process the respective data, and they can do so independently from each other. The various processing stages in a pipeline create their own derived streams/tables (such as the orders_enriched stream in the ksqlDB example above), which in turn can be used as input for other pipelines and applications. This allows for further and more complex composition and re-use of events throughout an organization.","title":"Considerations"},{"location":"compositional-patterns/pipeline/#references","text":"This pattern was influenced by Pipes and Filters in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf. However, it is much more powerful and flexible because it is using Event Streams as the pipes.","title":"References"},{"location":"compositional-patterns/strangler-fig/","text":"Strangler Fig Move functionality from a montolithic application to microservices gradually until the monolith is no longer being used and can be shut down. Problem How can we migrate a monolithic application to a microservices architecture without the risks associated with a large cut-over rewrite? Solution We can use the Strangler Fig pattern to migrate from a monolith to microservices safely and gradually. Implementation First we identify a module of the monolith, or a set of existing APIs that will be replaced by our first microservice. We'll call this module A. Then we can use change data capture (CDC) to convert any changes to module A's data to an Event Stream that will feed our microservice. Then we will place a proxy between the monolith and any clients, with all read calls to module A being routed to our microservice. When our microservice is ready to handle writes, we can change the proxy to route all module A's writes as well. At this point, we can, again, use CDC to stream changes back to our monolith so it doesn't know what it's missing. As we continue this process with more modules, we gradually replace the functionality of the monolith until it can be shut down safely. Considerations The Event Streams that we create using CDC are not only there for feeding the microservices, but are also available to be used by other applications. So that the Strangler Fig pattern is not only helpful in replacing legacy application, but can be used to gradually transform our enterprise architecture to an Event Streaming Platform References This pattern was originally articulated by Martin Fowler in his post: Strangler Fig Application . There is also a good representation of it among Microsoft's Cloud Design Patterns . Gunnar Morling and Hans-Peter Grahsl demonstrate this pattern in a presentation from Kafka Summit Europe 2021 .","title":"Strangler Fig"},{"location":"compositional-patterns/strangler-fig/#strangler-fig","text":"Move functionality from a montolithic application to microservices gradually until the monolith is no longer being used and can be shut down.","title":"Strangler Fig"},{"location":"compositional-patterns/strangler-fig/#problem","text":"How can we migrate a monolithic application to a microservices architecture without the risks associated with a large cut-over rewrite?","title":"Problem"},{"location":"compositional-patterns/strangler-fig/#solution","text":"We can use the Strangler Fig pattern to migrate from a monolith to microservices safely and gradually.","title":"Solution"},{"location":"compositional-patterns/strangler-fig/#implementation","text":"First we identify a module of the monolith, or a set of existing APIs that will be replaced by our first microservice. We'll call this module A. Then we can use change data capture (CDC) to convert any changes to module A's data to an Event Stream that will feed our microservice. Then we will place a proxy between the monolith and any clients, with all read calls to module A being routed to our microservice. When our microservice is ready to handle writes, we can change the proxy to route all module A's writes as well. At this point, we can, again, use CDC to stream changes back to our monolith so it doesn't know what it's missing. As we continue this process with more modules, we gradually replace the functionality of the monolith until it can be shut down safely.","title":"Implementation"},{"location":"compositional-patterns/strangler-fig/#considerations","text":"The Event Streams that we create using CDC are not only there for feeding the microservices, but are also available to be used by other applications. So that the Strangler Fig pattern is not only helpful in replacing legacy application, but can be used to gradually transform our enterprise architecture to an Event Streaming Platform","title":"Considerations"},{"location":"compositional-patterns/strangler-fig/#references","text":"This pattern was originally articulated by Martin Fowler in his post: Strangler Fig Application . There is also a good representation of it among Microsoft's Cloud Design Patterns . Gunnar Morling and Hans-Peter Grahsl demonstrate this pattern in a presentation from Kafka Summit Europe 2021 .","title":"References"},{"location":"event/command-event/","text":"Command Event Events often fall into one of two categories: messages and commands. Message-like events resemble simple facts--a user sends their new address, a product leaves the warehouse--and we record these facts first, without immediately considering what happens next. Other events resemble commands to invoke a specific action--a user clicks a [BUY] button--and the system takes the action (for example, by triggering order processing). Problem How can we use an Event Streaming Platform to invoke a procedure in another application? Solution Separate out the function call into a service that writes an event to an Event Stream , detailing the necessary action and its arguments. Then write a separate service that watches for that event before invoking the procedure. In terms of application logic, a Command Event is typically dispatched in a fire-and-forget manner (events themselves are delivered and stored with strong guarantees, such as exactly-once semantics). The writer assumes that the event will be handled correctly, and the responsibility for monitoring and error-handling falls elsewhere in the system. This is very similar to the Actor model: actors have an inbox, and we write messages to that inbox and trust that they will be handled in due course. If a return value is explicitly required, the downstream service can write a result event back to a second stream. Correlation of Command Events with their return value is typically performed using a Correlation Identifier . Implementation Suppose that we have a [BUY] button that should trigger a dispatchProduct(12005) function call in our warehousing system. Rather than calling the function directly, we can split the call up. We create a command stream: CREATE STREAM dispatch_products ( order_id BIGINT KEY, address VARCHAR ) WITH ( KAFKA_TOPIC = ' dispatch_products', VALUE_FORMAT = 'AVRO', PARTITIONS = 2 ); We start a process that inserts into the stream: INSERT INTO dispatch_products ( order_id, address ) VALUES ( 12004, '1 Streetford Road' ); INSERT INTO dispatch_products ( order_id, address ) VALUES ( 12005, '2 Roadford Avenue' ); INSERT INTO dispatch_products ( order_id, address ) VALUES ( 12006, '3 Avenue Fordstreet' ); And finally, we start a second process that watches the stream of events and invokes the dispatchProduct procedure foreach : ... Serde<GenericRecord> valueGenericAvroSerde = ... StreamsBuilder builder = new StreamsBuilder(); KStream<Long, GenericRecord> dispatchStream = builder.stream( \"dispatch_products\", Consumed.with(Serdes.Long(), valueGenericAvroSerde) ); dispatchStream.foreach((key, value) -> warehouse.dispatchProduct(key, value)); Considerations This approach works, but it may indicate a missed opportunity to improve the overall architecture of the system. Consider what happens when we need more actions. Suppose that [BUY] should also trigger an email and a text notification to the customer. Should the warehouse software finish its work and then write SendEmail and SendText commands to two new topics? Or should these two new events be written by the same process that wrote DispatchProduct ? A month later, when we need our sales figures, should we count the number of products dispatched, or the number of emails sent? Should we count both, to verify that they agree? The system grows a little more, and we have to ask, \"How much code is behind that [BUY] button? What's the release cycle? Is changing it becoming a blocker?\" The [BUY] button is important to the whole company, and rightly so, but its maintenance shouldn't hold the company to ransom. The root problem here is that, in moving from a function call within a monolith to a system that posts a specific command to a specific recipient, we've decoupled the function call without decoupling the underlying concepts. When we do that, the architecture responds with growing pains 1 . A better solution is to realize that our \"Command Event\" is actually two concepts woven together: \"What happened?\" and \"Who needs to know?\" By teasing those concepts apart, we can clean up our architecture. We allow one process to focus on recording the facts of what happened, while other processes decide for themselves if they care about those facts. When the [BUY] click happens, we should just write an Order event. Then warehousing, notifications, and sales can choose to react, without any need for coordination. In short, commands are tightly coupled to an audience of one, whereas an event should be simply a decoupled fact, available for anyone interested. Commands aren't bad per se , but they are a red flag signaling an opportunity for further decoupling. Seeing systems in this way requires a slight shift of perspective--a new way of modeling our processes--and opens up the opportunity for systems that collaborate more easily while actually taking on less individual responsibility. References This approach can become complex if there is a chain of functions, where the result of one is fed into the arguments of the next. In that situation, consider using Event Collaboration . See Designing Event Driven Systems --in particular, chapter 5, \"Events: A Basis for Collaboration\"--for further discussion. This pattern is derived from Command Message in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. Footnotes 1 At this point, someone in the team will say, \"We were better off just calling the function directly.\" And if we stopped there, they would have a fair point.","title":"Command Event"},{"location":"event/command-event/#command-event","text":"Events often fall into one of two categories: messages and commands. Message-like events resemble simple facts--a user sends their new address, a product leaves the warehouse--and we record these facts first, without immediately considering what happens next. Other events resemble commands to invoke a specific action--a user clicks a [BUY] button--and the system takes the action (for example, by triggering order processing).","title":"Command Event"},{"location":"event/command-event/#problem","text":"How can we use an Event Streaming Platform to invoke a procedure in another application?","title":"Problem"},{"location":"event/command-event/#solution","text":"Separate out the function call into a service that writes an event to an Event Stream , detailing the necessary action and its arguments. Then write a separate service that watches for that event before invoking the procedure. In terms of application logic, a Command Event is typically dispatched in a fire-and-forget manner (events themselves are delivered and stored with strong guarantees, such as exactly-once semantics). The writer assumes that the event will be handled correctly, and the responsibility for monitoring and error-handling falls elsewhere in the system. This is very similar to the Actor model: actors have an inbox, and we write messages to that inbox and trust that they will be handled in due course. If a return value is explicitly required, the downstream service can write a result event back to a second stream. Correlation of Command Events with their return value is typically performed using a Correlation Identifier .","title":"Solution"},{"location":"event/command-event/#implementation","text":"Suppose that we have a [BUY] button that should trigger a dispatchProduct(12005) function call in our warehousing system. Rather than calling the function directly, we can split the call up. We create a command stream: CREATE STREAM dispatch_products ( order_id BIGINT KEY, address VARCHAR ) WITH ( KAFKA_TOPIC = ' dispatch_products', VALUE_FORMAT = 'AVRO', PARTITIONS = 2 ); We start a process that inserts into the stream: INSERT INTO dispatch_products ( order_id, address ) VALUES ( 12004, '1 Streetford Road' ); INSERT INTO dispatch_products ( order_id, address ) VALUES ( 12005, '2 Roadford Avenue' ); INSERT INTO dispatch_products ( order_id, address ) VALUES ( 12006, '3 Avenue Fordstreet' ); And finally, we start a second process that watches the stream of events and invokes the dispatchProduct procedure foreach : ... Serde<GenericRecord> valueGenericAvroSerde = ... StreamsBuilder builder = new StreamsBuilder(); KStream<Long, GenericRecord> dispatchStream = builder.stream( \"dispatch_products\", Consumed.with(Serdes.Long(), valueGenericAvroSerde) ); dispatchStream.foreach((key, value) -> warehouse.dispatchProduct(key, value));","title":"Implementation"},{"location":"event/command-event/#considerations","text":"This approach works, but it may indicate a missed opportunity to improve the overall architecture of the system. Consider what happens when we need more actions. Suppose that [BUY] should also trigger an email and a text notification to the customer. Should the warehouse software finish its work and then write SendEmail and SendText commands to two new topics? Or should these two new events be written by the same process that wrote DispatchProduct ? A month later, when we need our sales figures, should we count the number of products dispatched, or the number of emails sent? Should we count both, to verify that they agree? The system grows a little more, and we have to ask, \"How much code is behind that [BUY] button? What's the release cycle? Is changing it becoming a blocker?\" The [BUY] button is important to the whole company, and rightly so, but its maintenance shouldn't hold the company to ransom. The root problem here is that, in moving from a function call within a monolith to a system that posts a specific command to a specific recipient, we've decoupled the function call without decoupling the underlying concepts. When we do that, the architecture responds with growing pains 1 . A better solution is to realize that our \"Command Event\" is actually two concepts woven together: \"What happened?\" and \"Who needs to know?\" By teasing those concepts apart, we can clean up our architecture. We allow one process to focus on recording the facts of what happened, while other processes decide for themselves if they care about those facts. When the [BUY] click happens, we should just write an Order event. Then warehousing, notifications, and sales can choose to react, without any need for coordination. In short, commands are tightly coupled to an audience of one, whereas an event should be simply a decoupled fact, available for anyone interested. Commands aren't bad per se , but they are a red flag signaling an opportunity for further decoupling. Seeing systems in this way requires a slight shift of perspective--a new way of modeling our processes--and opens up the opportunity for systems that collaborate more easily while actually taking on less individual responsibility.","title":"Considerations"},{"location":"event/command-event/#references","text":"This approach can become complex if there is a chain of functions, where the result of one is fed into the arguments of the next. In that situation, consider using Event Collaboration . See Designing Event Driven Systems --in particular, chapter 5, \"Events: A Basis for Collaboration\"--for further discussion. This pattern is derived from Command Message in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf.","title":"References"},{"location":"event/command-event/#footnotes","text":"1 At this point, someone in the team will say, \"We were better off just calling the function directly.\" And if we stopped there, they would have a fair point.","title":"Footnotes"},{"location":"event/correlation-identifier/","text":"Correlation Identifier Event Processing Applications may want to implement Event Collaboration , where Events are used to transport requests and responses. When applications collaborate via events, they need a way to correlate event response data for specific requests. Problem When an application requests information and receives a response, how can the application know which request corresponds to that particular response? Solution An Event Processor generates an event, which acts as the request. A globally unique identifier is added to the request event before it is sent. This allows the responding Event Processor to include the identifier in the response event, so that the requesting processor can correlate the request and response. Implementation In Kafka, we can add a globally unique identifier to Kafka record headers when producing a request event. The following example uses the Kafka Java producer client. ProducerRecord<String, String> requestEvent = new ProducerRecord<>(\"request-event-key\", \"request-event-value\"); requestEvent.headers().add(\"requestID\", UUID.randomUUID().toString()); requestEvent.send(producerRecord); In the responding event processor, we first extract the correlation identifier from the request event (here, the identifier is called requestID ) and then add that identifier to the response event. ProducerRecord<String, String> responseEvent = new ProducerRecord<>(\"response-event-key\", \"response-event-value\"); requestEvent.headers().add(\"requestID\", requestEvent.headers().lastHeader(\"requestID\").value()); requestEvent.send(producerRecord); References This pattern is derived from Correlation Identifier in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. For a case study on coordinating microservices towards higher-level business goals, see Building a Microservices Ecosystem with Kafka Streams and ksqlDB . Correlation identifiers can be used as part of Event Collaboration , a pattern in which decentralized Event Processing Applications collaborate to implement a distributed workflow solution. The idea of tagging requests and their associated responses exists in many other protocols. For example, an email client connecting over IMAP will send commands prefixed with a unique ID (typically a001 , a002 , etc.), and the server will respond asynchronously, tagging its responses with the matching ID.","title":"Correlation Identifier"},{"location":"event/correlation-identifier/#correlation-identifier","text":"Event Processing Applications may want to implement Event Collaboration , where Events are used to transport requests and responses. When applications collaborate via events, they need a way to correlate event response data for specific requests.","title":"Correlation Identifier"},{"location":"event/correlation-identifier/#problem","text":"When an application requests information and receives a response, how can the application know which request corresponds to that particular response?","title":"Problem"},{"location":"event/correlation-identifier/#solution","text":"An Event Processor generates an event, which acts as the request. A globally unique identifier is added to the request event before it is sent. This allows the responding Event Processor to include the identifier in the response event, so that the requesting processor can correlate the request and response.","title":"Solution"},{"location":"event/correlation-identifier/#implementation","text":"In Kafka, we can add a globally unique identifier to Kafka record headers when producing a request event. The following example uses the Kafka Java producer client. ProducerRecord<String, String> requestEvent = new ProducerRecord<>(\"request-event-key\", \"request-event-value\"); requestEvent.headers().add(\"requestID\", UUID.randomUUID().toString()); requestEvent.send(producerRecord); In the responding event processor, we first extract the correlation identifier from the request event (here, the identifier is called requestID ) and then add that identifier to the response event. ProducerRecord<String, String> responseEvent = new ProducerRecord<>(\"response-event-key\", \"response-event-value\"); requestEvent.headers().add(\"requestID\", requestEvent.headers().lastHeader(\"requestID\").value()); requestEvent.send(producerRecord);","title":"Implementation"},{"location":"event/correlation-identifier/#references","text":"This pattern is derived from Correlation Identifier in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. For a case study on coordinating microservices towards higher-level business goals, see Building a Microservices Ecosystem with Kafka Streams and ksqlDB . Correlation identifiers can be used as part of Event Collaboration , a pattern in which decentralized Event Processing Applications collaborate to implement a distributed workflow solution. The idea of tagging requests and their associated responses exists in many other protocols. For example, an email client connecting over IMAP will send commands prefixed with a unique ID (typically a001 , a002 , etc.), and the server will respond asynchronously, tagging its responses with the matching ID.","title":"References"},{"location":"event/data-contract/","text":"Data Contract An Event Processing Application can send an Event to another Event Processing Application. It's essential that the communicating applications understand how to process these shared events. Problem How can an application know how to process an Event sent by another application? Solution Using a Data Contract or Schema, different Event Processing Applications can share Events and understand how to process them, without the sending application and receiving application knowing any details about each other. The Data Contract pattern allows these different applications to cooperate while remaining loosely coupled, and thus insulated from any internal changes that they may implement. By implementing a data contract or schema, you can provide the same record consistency guarantees as a relational database management system (RDBMS), which integrates a schema by default. Implementation By using a schema to model event objects, Apache Kafka\u00ae clients (such as a Kafka producer, a Kafka Streams application, or the streaming database ksqlDB ) can understand how to handle events from different applications that use the same schema. For example, we can use Apache Avro to describe a schema: { \"type\":\"record\", \"namespace\": \"io.confluent.developer.avro\", \"name\":\"Purchase\", \"fields\": [ {\"name\": \"item\", \"type\":\"string\"}, {\"name\": \"amount\", \"type\": \"double\"}, {\"name\": \"customer_id\", \"type\": \"string\"} ] } Additionally, using a central repository, such as the Confluent Schema Registry , makes it easy for Kafka clients to leverage schemas. Considerations Rather than implementing custom support for a data contract or schema, consider using an industry-accepted framework for schema support, such as the following: Apache Avro Protocol Buffers (Protobuf) JSON Schema . References Yes, Virginia, You Really Do Need a Schema Registry","title":"Data Contract"},{"location":"event/data-contract/#data-contract","text":"An Event Processing Application can send an Event to another Event Processing Application. It's essential that the communicating applications understand how to process these shared events.","title":"Data Contract"},{"location":"event/data-contract/#problem","text":"How can an application know how to process an Event sent by another application?","title":"Problem"},{"location":"event/data-contract/#solution","text":"Using a Data Contract or Schema, different Event Processing Applications can share Events and understand how to process them, without the sending application and receiving application knowing any details about each other. The Data Contract pattern allows these different applications to cooperate while remaining loosely coupled, and thus insulated from any internal changes that they may implement. By implementing a data contract or schema, you can provide the same record consistency guarantees as a relational database management system (RDBMS), which integrates a schema by default.","title":"Solution"},{"location":"event/data-contract/#implementation","text":"By using a schema to model event objects, Apache Kafka\u00ae clients (such as a Kafka producer, a Kafka Streams application, or the streaming database ksqlDB ) can understand how to handle events from different applications that use the same schema. For example, we can use Apache Avro to describe a schema: { \"type\":\"record\", \"namespace\": \"io.confluent.developer.avro\", \"name\":\"Purchase\", \"fields\": [ {\"name\": \"item\", \"type\":\"string\"}, {\"name\": \"amount\", \"type\": \"double\"}, {\"name\": \"customer_id\", \"type\": \"string\"} ] } Additionally, using a central repository, such as the Confluent Schema Registry , makes it easy for Kafka clients to leverage schemas.","title":"Implementation"},{"location":"event/data-contract/#considerations","text":"Rather than implementing custom support for a data contract or schema, consider using an industry-accepted framework for schema support, such as the following: Apache Avro Protocol Buffers (Protobuf) JSON Schema .","title":"Considerations"},{"location":"event/data-contract/#references","text":"Yes, Virginia, You Really Do Need a Schema Registry","title":"References"},{"location":"event/event-deserializer/","text":"Event Deserializer Data has a long lifecycle, often outliving the programs that originally gathered and stored it. And data originates from a wide variety of systems and programming languages. The more easily we can access that ocean of data, the richer the analysis we can perform. In an online shopping business, data recorded by the order-processing system and from data user behavior may prove invaluable to the website design department, provided they can actually access it . It's vital to be able to read data from an Event Store regardless of which process and which department put it there originally. To a large degree, the accessibility of data is determined at write time, by our choice of Event Serializer . Still, the story is certainly not complete until we've read the data back out. Problem How can I reconstruct the original event from its representation in the event streaming platform? Solution Use an Event Streaming Platform that integrates well with a schema registry. This makes it easy to encourage (or require) writers to record the event's data description for later use. Having both the event data and its schema readily available makes deserialization easy. While some data formats are reasonably discoverable , in practice it becomes invaluable to have a precise, permanent record of how the data was encoded at the time it was written. This is particularly true if the data format has evolved over time and the Event Stream may contain more than one encoding of semantically-equivalent data. Implementation Confluent\u2019s Schema Registry stores a versioned history of the data's schema in Apache Kafka\u00ae itself. The client libraries can then use this metadata to seamlessly reconstruct the original event data, while we can use the registry API to manually inspect the schemas, or to build libraries for other languages. For example, in the Event Serializer pattern we wrote a stream of fx_trade events. If we want to recall the structure of those events we can ask ksqlDB: DESCRIBE fx_trade; Name : FX_TRADE Field | Type ---------------------------------------- TRADE_ID | BIGINT (key) FROM_CURRENCY | VARCHAR(STRING) TO_CURRENCY | VARCHAR(STRING) PRICE | DECIMAL(10, 5) ---------------------------------------- Or we can query the Schema Registry directly to see the structure in a machine-readable format: curl http://localhost:8081/subjects/fx_trade-value/versions/latest | jq . { \"subject\": \"fx_trade-value\", \"version\": 1, \"id\": 44, \"schema\": \"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"KsqlDataSourceSchema\\\",\\\"namespace\\\":\\\"io.confluent.ksql.avro_schemas\\\",\\\"fields\\\":[{\\\"name\\\":\\\"FROM_CURRENCY\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"],\\\"default\\\":null},{\\\"name\\\":\\\"TO_CURRENCY\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"],\\\"default\\\":null},{\\\"name\\\":\\\"PRICE\\\",\\\"type\\\":[\\\"null\\\",{\\\"type\\\":\\\"bytes\\\",\\\"scale\\\":5,\\\"precision\\\":10,\\\"connect.version\\\":1,\\\"connect.parameters\\\":{\\\"scale\\\":\\\"5\\\",\\\"connect.decimal.precision\\\":\\\"10\\\"},\\\"connect.name\\\":\\\"org.apache.kafka.connect.data.Decimal\\\",\\\"logicalType\\\":\\\"decimal\\\"}],\\\"default\\\":null}],\\\"connect.name\\\":\\\"io.confluent.ksql.avro_schemas.KsqlDataSourceSchema\\\"}\" } Unpacking that schema field reveals the Avro specification: curl http://localhost:8081/subjects/fx_trade-value/versions/latest | jq -rc .schema | jq . { \"type\": \"record\", \"name\": \"KsqlDataSourceSchema\", \"namespace\": \"io.confluent.ksql.avro_schemas\", \"fields\": [ { \"name\": \"FROM_CURRENCY\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"TO_CURRENCY\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"PRICE\", \"type\": [ \"null\", { \"type\": \"bytes\", \"scale\": 5, \"precision\": 10, \"connect.version\": 1, \"connect.parameters\": { \"scale\": \"5\", \"connect.decimal.precision\": \"10\" }, \"connect.name\": \"org.apache.kafka.connect.data.Decimal\", \"logicalType\": \"decimal\" } ], \"default\": null } ], \"connect.name\": \"io.confluent.ksql.avro_schemas.KsqlDataSourceSchema\" } An Avro library can use this schema to deserialize the events seamlessly. And any client libraries that are Schema Registry-aware can automate this lookup, allowing us to forget about encodings entirely and focus on the data. Considerations In addition to Avro, Schema Registry supports Protobuf and JSON Schema. See Event Serializer for a discussion of these formats. While the choice of serialization format is important, it doesn't have to be set in stone. For example, it's straightforward to translate between supported formats with ksqlDB . For more complex scenarios, we have several strategies for managing schema migration: Schema Compatibility discusses the kinds of \"safe\" schema changes that Avro is designed to handle transparently. Event Translators can convert between different encodings to aid consumption by different systems. Schema Evolution discusses splitting and joining streams to simplify serving consumers that can only handle certain versions of the event's schema. An Event Standardizer can reformat disparate data encodings into a single unified format. And we always have the option of handling encoding problems directly in code with a Schema-on-Read strategy. References The counterpart of an event deserializer (for reading) is an Event Serializer (for writing). Serializers and deserializers are closely related to Data Contracts , in which we want to adhere to a specific serialization format, and constrain the individual events to a certain schema within that format. See also: Event Mapper .","title":"Event Deserializer"},{"location":"event/event-deserializer/#event-deserializer","text":"Data has a long lifecycle, often outliving the programs that originally gathered and stored it. And data originates from a wide variety of systems and programming languages. The more easily we can access that ocean of data, the richer the analysis we can perform. In an online shopping business, data recorded by the order-processing system and from data user behavior may prove invaluable to the website design department, provided they can actually access it . It's vital to be able to read data from an Event Store regardless of which process and which department put it there originally. To a large degree, the accessibility of data is determined at write time, by our choice of Event Serializer . Still, the story is certainly not complete until we've read the data back out.","title":"Event Deserializer"},{"location":"event/event-deserializer/#problem","text":"How can I reconstruct the original event from its representation in the event streaming platform?","title":"Problem"},{"location":"event/event-deserializer/#solution","text":"Use an Event Streaming Platform that integrates well with a schema registry. This makes it easy to encourage (or require) writers to record the event's data description for later use. Having both the event data and its schema readily available makes deserialization easy. While some data formats are reasonably discoverable , in practice it becomes invaluable to have a precise, permanent record of how the data was encoded at the time it was written. This is particularly true if the data format has evolved over time and the Event Stream may contain more than one encoding of semantically-equivalent data.","title":"Solution"},{"location":"event/event-deserializer/#implementation","text":"Confluent\u2019s Schema Registry stores a versioned history of the data's schema in Apache Kafka\u00ae itself. The client libraries can then use this metadata to seamlessly reconstruct the original event data, while we can use the registry API to manually inspect the schemas, or to build libraries for other languages. For example, in the Event Serializer pattern we wrote a stream of fx_trade events. If we want to recall the structure of those events we can ask ksqlDB: DESCRIBE fx_trade; Name : FX_TRADE Field | Type ---------------------------------------- TRADE_ID | BIGINT (key) FROM_CURRENCY | VARCHAR(STRING) TO_CURRENCY | VARCHAR(STRING) PRICE | DECIMAL(10, 5) ---------------------------------------- Or we can query the Schema Registry directly to see the structure in a machine-readable format: curl http://localhost:8081/subjects/fx_trade-value/versions/latest | jq . { \"subject\": \"fx_trade-value\", \"version\": 1, \"id\": 44, \"schema\": \"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"KsqlDataSourceSchema\\\",\\\"namespace\\\":\\\"io.confluent.ksql.avro_schemas\\\",\\\"fields\\\":[{\\\"name\\\":\\\"FROM_CURRENCY\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"],\\\"default\\\":null},{\\\"name\\\":\\\"TO_CURRENCY\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"],\\\"default\\\":null},{\\\"name\\\":\\\"PRICE\\\",\\\"type\\\":[\\\"null\\\",{\\\"type\\\":\\\"bytes\\\",\\\"scale\\\":5,\\\"precision\\\":10,\\\"connect.version\\\":1,\\\"connect.parameters\\\":{\\\"scale\\\":\\\"5\\\",\\\"connect.decimal.precision\\\":\\\"10\\\"},\\\"connect.name\\\":\\\"org.apache.kafka.connect.data.Decimal\\\",\\\"logicalType\\\":\\\"decimal\\\"}],\\\"default\\\":null}],\\\"connect.name\\\":\\\"io.confluent.ksql.avro_schemas.KsqlDataSourceSchema\\\"}\" } Unpacking that schema field reveals the Avro specification: curl http://localhost:8081/subjects/fx_trade-value/versions/latest | jq -rc .schema | jq . { \"type\": \"record\", \"name\": \"KsqlDataSourceSchema\", \"namespace\": \"io.confluent.ksql.avro_schemas\", \"fields\": [ { \"name\": \"FROM_CURRENCY\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"TO_CURRENCY\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"PRICE\", \"type\": [ \"null\", { \"type\": \"bytes\", \"scale\": 5, \"precision\": 10, \"connect.version\": 1, \"connect.parameters\": { \"scale\": \"5\", \"connect.decimal.precision\": \"10\" }, \"connect.name\": \"org.apache.kafka.connect.data.Decimal\", \"logicalType\": \"decimal\" } ], \"default\": null } ], \"connect.name\": \"io.confluent.ksql.avro_schemas.KsqlDataSourceSchema\" } An Avro library can use this schema to deserialize the events seamlessly. And any client libraries that are Schema Registry-aware can automate this lookup, allowing us to forget about encodings entirely and focus on the data.","title":"Implementation"},{"location":"event/event-deserializer/#considerations","text":"In addition to Avro, Schema Registry supports Protobuf and JSON Schema. See Event Serializer for a discussion of these formats. While the choice of serialization format is important, it doesn't have to be set in stone. For example, it's straightforward to translate between supported formats with ksqlDB . For more complex scenarios, we have several strategies for managing schema migration: Schema Compatibility discusses the kinds of \"safe\" schema changes that Avro is designed to handle transparently. Event Translators can convert between different encodings to aid consumption by different systems. Schema Evolution discusses splitting and joining streams to simplify serving consumers that can only handle certain versions of the event's schema. An Event Standardizer can reformat disparate data encodings into a single unified format. And we always have the option of handling encoding problems directly in code with a Schema-on-Read strategy.","title":"Considerations"},{"location":"event/event-deserializer/#references","text":"The counterpart of an event deserializer (for reading) is an Event Serializer (for writing). Serializers and deserializers are closely related to Data Contracts , in which we want to adhere to a specific serialization format, and constrain the individual events to a certain schema within that format. See also: Event Mapper .","title":"References"},{"location":"event/event-envelope/","text":"Event Envelope Event Streaming Platforms allow many different types of applications to work together. An Event Envelope provides a standard set of well-known fields across all Events sent through the Event Streaming Applications . The envelope is independent of the underlying event format, and often references attributes such as the encryption type, schema, key, and serialization format. Envelopes are analogous to protocol headers in networking (for example, TCP-IP). Problem How can I convey information to all participants in an Event Streaming Platform independently of the event payload? For example, how can I convey how to decrypt an Event , what schema is used, or what ID defines the uniqueness of the event? Solution Use an Event Envelope to wrap the event data using a standard format agreed on by all participants of the Event Streaming Platform , or more broadly. Cloud Events \u2014which standardize access to ID, Schema, Key, and other common event attributes\u2014are an industry-standard example of the Event Envelope pattern. Example Implementation Using basic Java consumers and producers, you can use a helper function to wrap an application's immutable payload into an envelope which conforms to the expected format of the Event Streaming Platform : static <T> Envelope<T> wrap(T payload, Iterable<Header> headers) { return new Envelope(serializer(payload), headers); } static <T> T unwrap(Envelope<T> envelope) { return envelope.payload; } References This pattern is derived from Envelope Wrapper in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. How to Choose Between Strict and Dynamic Schemas See CloudEvents for a specification that describes event header information in a common way.","title":"Event Envelope"},{"location":"event/event-envelope/#event-envelope","text":"Event Streaming Platforms allow many different types of applications to work together. An Event Envelope provides a standard set of well-known fields across all Events sent through the Event Streaming Applications . The envelope is independent of the underlying event format, and often references attributes such as the encryption type, schema, key, and serialization format. Envelopes are analogous to protocol headers in networking (for example, TCP-IP).","title":"Event Envelope"},{"location":"event/event-envelope/#problem","text":"How can I convey information to all participants in an Event Streaming Platform independently of the event payload? For example, how can I convey how to decrypt an Event , what schema is used, or what ID defines the uniqueness of the event?","title":"Problem"},{"location":"event/event-envelope/#solution","text":"Use an Event Envelope to wrap the event data using a standard format agreed on by all participants of the Event Streaming Platform , or more broadly. Cloud Events \u2014which standardize access to ID, Schema, Key, and other common event attributes\u2014are an industry-standard example of the Event Envelope pattern.","title":"Solution"},{"location":"event/event-envelope/#example-implementation","text":"Using basic Java consumers and producers, you can use a helper function to wrap an application's immutable payload into an envelope which conforms to the expected format of the Event Streaming Platform : static <T> Envelope<T> wrap(T payload, Iterable<Header> headers) { return new Envelope(serializer(payload), headers); } static <T> T unwrap(Envelope<T> envelope) { return envelope.payload; }","title":"Example Implementation"},{"location":"event/event-envelope/#references","text":"This pattern is derived from Envelope Wrapper in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. How to Choose Between Strict and Dynamic Schemas See CloudEvents for a specification that describes event header information in a common way.","title":"References"},{"location":"event/event-serializer/","text":"Event Serializer Data has a long lifecycle, often outliving the programs that originally gathered and stored it. And data demands a broad audience - the more accessible our data is, the more departments in our organization can find use for it. In a successful system, data gathered by the sales department in one year may prove invaluable to the marketing department a few years later, provided they can actually access it . For maximum utility and longevity, data should be written in a way that doesn't obscure it from future readers and writers. The data is more important than today's technology choices. How does this affect an event-based system? Are there any special concerns for this kind of architecture, or will the programming language's serialization tools suffice? Problem How can I convert an event into a format understood by the event streaming platform and applications that use it? Solution Use a language-agnostic serialization format. The ideal format would be self-documenting, space-efficient, and designed to support some degree of backwards and forwards -compatibility. We recommend Avro . (See \"Considerations\".) An optional, recommended step is to register the serialization details with a schema registry. A registry provides a reliable, machine-readable reference point for Event Deserializers and Schema Validators , making event consumption vastly simpler. Implementation For example, we can use Avro to define a structure for Foreign Exchange trade deals as: {\"namespace\": \"io.confluent.developer\", \"type\": \"record\", \"name\": \"FxTrade\", \"fields\": [ {\"name\": \"trade_id\", \"type\": \"long\"}, {\"name\": \"from_currency\", \"type\": \"string\"}, {\"name\": \"to_currency\", \"type\": \"string\"}, {\"name\": \"price\", \"type\": \"bytes\", \"logicalType\": \"decimal\", \"precision\": 10, \"scale\": 5} ] } ...and then use our language's Avro libraries to take care of serialization for us: FxTrade fxTrade = new FxTrade( ... ); ProducerRecord<long, FxTrade> producerRecord = new ProducerRecord<>(\"fx_trade\", fxTrade.getTradeId(), fxTrade); producer.send(producerRecord); Alternatively, with the streaming database ksqlDB , we can define an Event Stream in a way that enforces that format and records the Avro definition using Confluent\u2019s Schema Registry : CREATE OR REPLACE STREAM fx_trade ( trade_id BIGINT KEY, from_currency VARCHAR(3), to_currency VARCHAR(3), price DECIMAL(10,5) ) WITH ( KAFKA_TOPIC = 'fx_trade', KEY_FORMAT = 'avro', VALUE_FORMAT = 'avro', PARTITIONS = 3 ); With this setup, both serialization and deserialization of data is performed automatically by ksqlDB behind the scenes. Considerations Event Streaming Platforms are typically serialization-agnostic, accepting any serialized data from human-readable text to raw bytes. However, by constraining ourselves to more widely-accepted, structured data formats, we can open the door to easier collaboration with other projects and programming languages. Finding a \"universal\" serialization format isn't a new problem, or one unique to event streaming. As such we have a number of technology-agnostic solutions readily available. To briefly cover a few: JSON . Arguably the most successful serialization format in the history of computing. JSON is a text-based format that's easy to read, write and discover 1 , as evidenced by the number of languages and projects that produce and consume JSON data across the world with minimal collaboration. Protocol buffers . Backed by Google and supported by a wide variety of languages, Protobuf is a binary format that sacrifices the discoverability of JSON for a much more compact representation that uses less disk space and network bandwidth. Protobuf is also a strongly-typed format, allowing enforcement of a particular data schema from writers, and describing the structure of that data to readers. Avro . A binary format similar to Protocol Buffers, Avro's design has focuses on supporting the evolution of schemas, allowing the data format to change over time while minimizing the impact to future readers and writer. While the choice of serialization format is important, it doesn't have to be set in stone. It's straightforward to translate between supported formats with ksqlDB . For more complex scenarios, we have several strategies for managing schema migration: Schema Compatibility discusses the kinds of \"safe\" schema changes that Avro is designed to handle transparently. Event Translators can convert between different encodings to aid consumption by different systems. Schema Evolution discusses splitting and joining streams to simplify serving consumers that can only handle certain versions of the event's schema. An Event Standardizer can reformat disparate data encodings into a single unified format. And as a fallback we can push the problem to the consumer's code with a Schema-on-Read strategy. References The counterpart of an event serializer (for writing) is an Event Deserializer (for reading). Serializers and deserializers are closely related to Data Contracts , in which we want to adhere to a specific serialization format and constrain the individual events to a certain schema within that format. See also: Event Mapper . Footnotes 1 Older programmers will tell tales of the less-discoverable serialization formats used by banks in the 80s, in which deciphering the meaning of a message meant wading through a thick, ring-bound printout of the data specification which explained the meaning of \"Field 78\" by cross-referencing \"Encoding Subformat 22\".","title":"Event Serializer"},{"location":"event/event-serializer/#event-serializer","text":"Data has a long lifecycle, often outliving the programs that originally gathered and stored it. And data demands a broad audience - the more accessible our data is, the more departments in our organization can find use for it. In a successful system, data gathered by the sales department in one year may prove invaluable to the marketing department a few years later, provided they can actually access it . For maximum utility and longevity, data should be written in a way that doesn't obscure it from future readers and writers. The data is more important than today's technology choices. How does this affect an event-based system? Are there any special concerns for this kind of architecture, or will the programming language's serialization tools suffice?","title":"Event Serializer"},{"location":"event/event-serializer/#problem","text":"How can I convert an event into a format understood by the event streaming platform and applications that use it?","title":"Problem"},{"location":"event/event-serializer/#solution","text":"Use a language-agnostic serialization format. The ideal format would be self-documenting, space-efficient, and designed to support some degree of backwards and forwards -compatibility. We recommend Avro . (See \"Considerations\".) An optional, recommended step is to register the serialization details with a schema registry. A registry provides a reliable, machine-readable reference point for Event Deserializers and Schema Validators , making event consumption vastly simpler.","title":"Solution"},{"location":"event/event-serializer/#implementation","text":"For example, we can use Avro to define a structure for Foreign Exchange trade deals as: {\"namespace\": \"io.confluent.developer\", \"type\": \"record\", \"name\": \"FxTrade\", \"fields\": [ {\"name\": \"trade_id\", \"type\": \"long\"}, {\"name\": \"from_currency\", \"type\": \"string\"}, {\"name\": \"to_currency\", \"type\": \"string\"}, {\"name\": \"price\", \"type\": \"bytes\", \"logicalType\": \"decimal\", \"precision\": 10, \"scale\": 5} ] } ...and then use our language's Avro libraries to take care of serialization for us: FxTrade fxTrade = new FxTrade( ... ); ProducerRecord<long, FxTrade> producerRecord = new ProducerRecord<>(\"fx_trade\", fxTrade.getTradeId(), fxTrade); producer.send(producerRecord); Alternatively, with the streaming database ksqlDB , we can define an Event Stream in a way that enforces that format and records the Avro definition using Confluent\u2019s Schema Registry : CREATE OR REPLACE STREAM fx_trade ( trade_id BIGINT KEY, from_currency VARCHAR(3), to_currency VARCHAR(3), price DECIMAL(10,5) ) WITH ( KAFKA_TOPIC = 'fx_trade', KEY_FORMAT = 'avro', VALUE_FORMAT = 'avro', PARTITIONS = 3 ); With this setup, both serialization and deserialization of data is performed automatically by ksqlDB behind the scenes.","title":"Implementation"},{"location":"event/event-serializer/#considerations","text":"Event Streaming Platforms are typically serialization-agnostic, accepting any serialized data from human-readable text to raw bytes. However, by constraining ourselves to more widely-accepted, structured data formats, we can open the door to easier collaboration with other projects and programming languages. Finding a \"universal\" serialization format isn't a new problem, or one unique to event streaming. As such we have a number of technology-agnostic solutions readily available. To briefly cover a few: JSON . Arguably the most successful serialization format in the history of computing. JSON is a text-based format that's easy to read, write and discover 1 , as evidenced by the number of languages and projects that produce and consume JSON data across the world with minimal collaboration. Protocol buffers . Backed by Google and supported by a wide variety of languages, Protobuf is a binary format that sacrifices the discoverability of JSON for a much more compact representation that uses less disk space and network bandwidth. Protobuf is also a strongly-typed format, allowing enforcement of a particular data schema from writers, and describing the structure of that data to readers. Avro . A binary format similar to Protocol Buffers, Avro's design has focuses on supporting the evolution of schemas, allowing the data format to change over time while minimizing the impact to future readers and writer. While the choice of serialization format is important, it doesn't have to be set in stone. It's straightforward to translate between supported formats with ksqlDB . For more complex scenarios, we have several strategies for managing schema migration: Schema Compatibility discusses the kinds of \"safe\" schema changes that Avro is designed to handle transparently. Event Translators can convert between different encodings to aid consumption by different systems. Schema Evolution discusses splitting and joining streams to simplify serving consumers that can only handle certain versions of the event's schema. An Event Standardizer can reformat disparate data encodings into a single unified format. And as a fallback we can push the problem to the consumer's code with a Schema-on-Read strategy.","title":"Considerations"},{"location":"event/event-serializer/#references","text":"The counterpart of an event serializer (for writing) is an Event Deserializer (for reading). Serializers and deserializers are closely related to Data Contracts , in which we want to adhere to a specific serialization format and constrain the individual events to a certain schema within that format. See also: Event Mapper .","title":"References"},{"location":"event/event-serializer/#footnotes","text":"1 Older programmers will tell tales of the less-discoverable serialization formats used by banks in the 80s, in which deciphering the meaning of a message meant wading through a thick, ring-bound printout of the data specification which explained the meaning of \"Field 78\" by cross-referencing \"Encoding Subformat 22\".","title":"Footnotes"},{"location":"event/event-standardizer/","text":"Event Standardizer In most businesses, a variety of traditional and Event Processing Applications need to exchange Events across Event Streams . Downstream Event Processing Applications will require standardized data formats in order to properly process these events. However, the reality of having many sources for these events often leads to a lack of such standards, or to different interpretations of the same standard. Problem How can I process events that are semantically equivalent but arrive in different formats? Solution Source all of the input Event Streams into an Event Standardizer that passes events to a specialized Event Translator , which in turn converts the events into a common format understood by the downstream Event Processors . Implementation As an example, we can use the Kafka Streams client library of Apache Kafka\u00ae to build an Event Processing Application that reads from multiple input Event Streams and then maps the values to a new type. Specifically, we can use the mapValues function to translate each event type into the standard type expected on the output Event Stream . SpecificAvroSerde<SpecificRecord> inputValueSerde = constructSerde(); builder .stream(List.of(\"inputStreamA\", \"inputStreamB\", \"inputStreamC\"), Consumed.with(Serdes.String(), inputValueSerde)) .mapValues((eventKey, eventValue) -> { if (eventValue.getClass() == TypeA.class) return typeATranslator.normalize(eventValue); else if (eventValue.getClass() == TypeB.class) return typeBTranslator.normalize(eventValue); else if (eventValue.getClass() == TypeC.class) return typeCTranslator.normalize(eventValue); else { // exception or dead letter stream } }) .to(\"my-standardized-output-stream\", Produced.with(Serdes.String(), outputSerdeType)); Considerations When possible, diverging data formats should be normalized \"at the source\". This data governance is often called \"Schema on Write\", and may be implemented using the Schema Validator pattern. Enforcing schema validation prior to writing an event to the Event Stream allows consuming applications to delegate their data format validation logic to the schema validation layer. Error handling should be considered in the design of the standardizer. Categories of errors may include serialization failures, unexpected or missing values, and unknown types (as in the example above). Dead Letter Stream is one pattern commonly used to handle exceptional events in the Event Processing Application. References See also the Stream Merger pattern, for unifying related streams without changing their format. This pattern is derived from Normalizer in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. Kafka Streams map stateless transformation documentation Error Handling Patterns for Apache Kafka Applications is a blog post with details on strategies and patterns for error handling in Event Processing Applications","title":"Event Standardizer"},{"location":"event/event-standardizer/#event-standardizer","text":"In most businesses, a variety of traditional and Event Processing Applications need to exchange Events across Event Streams . Downstream Event Processing Applications will require standardized data formats in order to properly process these events. However, the reality of having many sources for these events often leads to a lack of such standards, or to different interpretations of the same standard.","title":"Event Standardizer"},{"location":"event/event-standardizer/#problem","text":"How can I process events that are semantically equivalent but arrive in different formats?","title":"Problem"},{"location":"event/event-standardizer/#solution","text":"Source all of the input Event Streams into an Event Standardizer that passes events to a specialized Event Translator , which in turn converts the events into a common format understood by the downstream Event Processors .","title":"Solution"},{"location":"event/event-standardizer/#implementation","text":"As an example, we can use the Kafka Streams client library of Apache Kafka\u00ae to build an Event Processing Application that reads from multiple input Event Streams and then maps the values to a new type. Specifically, we can use the mapValues function to translate each event type into the standard type expected on the output Event Stream . SpecificAvroSerde<SpecificRecord> inputValueSerde = constructSerde(); builder .stream(List.of(\"inputStreamA\", \"inputStreamB\", \"inputStreamC\"), Consumed.with(Serdes.String(), inputValueSerde)) .mapValues((eventKey, eventValue) -> { if (eventValue.getClass() == TypeA.class) return typeATranslator.normalize(eventValue); else if (eventValue.getClass() == TypeB.class) return typeBTranslator.normalize(eventValue); else if (eventValue.getClass() == TypeC.class) return typeCTranslator.normalize(eventValue); else { // exception or dead letter stream } }) .to(\"my-standardized-output-stream\", Produced.with(Serdes.String(), outputSerdeType));","title":"Implementation"},{"location":"event/event-standardizer/#considerations","text":"When possible, diverging data formats should be normalized \"at the source\". This data governance is often called \"Schema on Write\", and may be implemented using the Schema Validator pattern. Enforcing schema validation prior to writing an event to the Event Stream allows consuming applications to delegate their data format validation logic to the schema validation layer. Error handling should be considered in the design of the standardizer. Categories of errors may include serialization failures, unexpected or missing values, and unknown types (as in the example above). Dead Letter Stream is one pattern commonly used to handle exceptional events in the Event Processing Application.","title":"Considerations"},{"location":"event/event-standardizer/#references","text":"See also the Stream Merger pattern, for unifying related streams without changing their format. This pattern is derived from Normalizer in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. Kafka Streams map stateless transformation documentation Error Handling Patterns for Apache Kafka Applications is a blog post with details on strategies and patterns for error handling in Event Processing Applications","title":"References"},{"location":"event/event/","text":"Event Events represent facts and are used by decoupled applications, services, and systems to exchange data across an Event Streaming Platform . Problem How do I represent a fact about something that has happened? Solution An event represents an immutable fact about something that happened. Examples of Events might be orders, payments, activities, or measurements. Events are produced to, stored in, and consumed from an Event Stream . An Event typically contains one or more data fields that describe the fact, as well as a timestamp that denotes when the Event was created by its Event Source . The Event may also contain various metadata, such as its source of origin (for example, the application or cloud service that created the event) and storage-level information (for example, its position in the event stream). Implementation In Apache Kafka\u00ae, Events are referred to as records . Records are modeled as a key / value pair with a timestamp and optional metadata (called headers). The value of the record usually contains the representation of an application domain object or some form of raw message value, such as the output of a sensor or other metric reading. The record key is useful for a few reasons, but critically, it is used by Kafka to determine how the data is partitioned within a stream, also called a topic (for more details on partitioning, see Partitioned Parallelism ). The key is often best thought of as a categorization of the Event, like the identity of a particular user or connected device. Headers are a place for record metadata that can help to describe the Event data itself, and are themselves modeled as a map of keys and values. Record keys, values, and headers are opaque data types, meaning that Kafka, by deliberate design to achieve its high scalability and performance, does not define a type interface for them: they are read, stored, and written by Kafka's server-side brokers as raw arrays of bytes. It is the responsibility of Kafka client applications, such as the streaming database ksqlDB , or microservices implemented with the client libraries, such as Kafka Streams or the Kafka Go client , to perform serialization and deserialization of the data within the record keys, values, and headers. When using the Java client library, events are created using the ProducerRecord type and are sent to Kafka using the KafkaProducer . In this example, we have set the key and value types as strings, and we have added a header: ProducerRecord<String, String> newEvent = new ProducerRecord<>( paymentEvent.getCustomerId().toString() /* key */, paymentEvent.toString() /* value */); producerRecord.headers() .add(\"origin-cloud\", \"aws\".getBytes(StandardCharsets.UTF_8)); producer.send(producerRecord); Considerations To ensure that Events from an Event Source can be read correctly by an Event Processor , they are often created in reference to an Event schema. Event schemas are commonly defined in Apache Avro , Protocol Buffers (Protobuf), or JSON Schema . For cloud-based architectures, evaluate the use of CloudEvents . CloudEvents provide a standardized Event Envelope that wraps an event, making common event properties such as source, type, time, and ID universally accessible, regardless of how the event itself was serialized. In certain scenarios, Events may represent commands (instructions, actions, and so on) that should be carried out by an Event Processor reading the events. See the Command Event pattern for details. References This pattern is derived in part from Message , Event Message , and Document Message in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. Apache Kafka 101: Introduction provides a primer on \"What is Kafka, and how does it work?\" including information on core concepts like Events","title":"Event"},{"location":"event/event/#event","text":"Events represent facts and are used by decoupled applications, services, and systems to exchange data across an Event Streaming Platform .","title":"Event"},{"location":"event/event/#problem","text":"How do I represent a fact about something that has happened?","title":"Problem"},{"location":"event/event/#solution","text":"An event represents an immutable fact about something that happened. Examples of Events might be orders, payments, activities, or measurements. Events are produced to, stored in, and consumed from an Event Stream . An Event typically contains one or more data fields that describe the fact, as well as a timestamp that denotes when the Event was created by its Event Source . The Event may also contain various metadata, such as its source of origin (for example, the application or cloud service that created the event) and storage-level information (for example, its position in the event stream).","title":"Solution"},{"location":"event/event/#implementation","text":"In Apache Kafka\u00ae, Events are referred to as records . Records are modeled as a key / value pair with a timestamp and optional metadata (called headers). The value of the record usually contains the representation of an application domain object or some form of raw message value, such as the output of a sensor or other metric reading. The record key is useful for a few reasons, but critically, it is used by Kafka to determine how the data is partitioned within a stream, also called a topic (for more details on partitioning, see Partitioned Parallelism ). The key is often best thought of as a categorization of the Event, like the identity of a particular user or connected device. Headers are a place for record metadata that can help to describe the Event data itself, and are themselves modeled as a map of keys and values. Record keys, values, and headers are opaque data types, meaning that Kafka, by deliberate design to achieve its high scalability and performance, does not define a type interface for them: they are read, stored, and written by Kafka's server-side brokers as raw arrays of bytes. It is the responsibility of Kafka client applications, such as the streaming database ksqlDB , or microservices implemented with the client libraries, such as Kafka Streams or the Kafka Go client , to perform serialization and deserialization of the data within the record keys, values, and headers. When using the Java client library, events are created using the ProducerRecord type and are sent to Kafka using the KafkaProducer . In this example, we have set the key and value types as strings, and we have added a header: ProducerRecord<String, String> newEvent = new ProducerRecord<>( paymentEvent.getCustomerId().toString() /* key */, paymentEvent.toString() /* value */); producerRecord.headers() .add(\"origin-cloud\", \"aws\".getBytes(StandardCharsets.UTF_8)); producer.send(producerRecord);","title":"Implementation"},{"location":"event/event/#considerations","text":"To ensure that Events from an Event Source can be read correctly by an Event Processor , they are often created in reference to an Event schema. Event schemas are commonly defined in Apache Avro , Protocol Buffers (Protobuf), or JSON Schema . For cloud-based architectures, evaluate the use of CloudEvents . CloudEvents provide a standardized Event Envelope that wraps an event, making common event properties such as source, type, time, and ID universally accessible, regardless of how the event itself was serialized. In certain scenarios, Events may represent commands (instructions, actions, and so on) that should be carried out by an Event Processor reading the events. See the Command Event pattern for details.","title":"Considerations"},{"location":"event/event/#references","text":"This pattern is derived in part from Message , Event Message , and Document Message in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. Apache Kafka 101: Introduction provides a primer on \"What is Kafka, and how does it work?\" including information on core concepts like Events","title":"References"},{"location":"event/schema-on-read/","text":"Schema-on-Read Schema-on-Read leaves the validation of an Event schema to the reader. There are several use cases for this pattern, and all of them provide a lot of flexibility to Event Processors and Event Sinks : There may be different versions of the same schema type, and the reader wants to choose which version to apply to a given event. The sequencing may matter between events of different types, while all of the different event types are put into a single stream. For example, consider a banking use case where a customer first opens an account, then gets approval, then makes a deposit, and so on. Put these heterogeneous Event types into the same stream, allowing the Event Streaming Platform to maintain event ordering and allowing any consuming Event Processors and Event Sinks to deserialize the events as needed. There may be unstructured data written into an Event Stream , and the reader can then apply whichever schema it wants. Problem How can an Event Processor apply a schema to data while reading the data from an Event Streaming Platform ? Solution The Schema-on-Read approach enables each reader to decide how to read data, and which version of which schema to apply to each Event that it reads. To make schema management easier, the design can use a centralized repository that stores multiple versions of different schemas, and then client applications can choose which schema to apply to events at runtime. Implementation With Confluent Schema Registry , all of the schemas are managed in a centralized repository. In addition to storing the schema information, Schema Registry can be configured to check and enforce that schema changes are compatible with previous versions. For example, if a business started with a schema definition for an event that has two fields, but the business needs later evolve to warrant an optional third field, then the schema evolves with the needs of the business. Schema Registry ensures that the new schema is compatible with the old schema. In this particular case, for backward compatibility, the third field, status , can be defined with a default value, which will be used for the missing field when deserializing data encoded with the old schema. This ensures that all events in a given stream follow Schema Compatibility rules and that applications can continue to process those events. { \"namespace\": \"example.avro\", \"type\": \"record\", \"name\": \"user\", \"fields\": [ {\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"address\", \"type\": \"string\"}, {\"name\": \"status\", \"type\": \"string\", \"default\": \"waiting\"} ] } As another example, if a use case warrants different event types being written into a single stream, with Apache Kafka\u00ae you can set the \"subject naming strategy\" to register schemas against the record type instead of the Kafka topic. Schema Registry will then allow for Schema Evolution and Schema Compatibility validation to be performed within the scope of each event type rather than the topic. The consumer application can read schema versions assigned to the data type, and if there are different data types in any given stream, the application can cast each event to the appropriate type at processing time and then follow the appropriate code path: if (Account.equals(record.getClass()) { ... } else if (Approval.equals(record.getClass())) { ... } else if (Transaction.equals(record.getClass())) { ... } else { ... } Considerations The schema's subject naming strategy can be set to record type (rather than Kafka topic) in one of two ways. The less restrictive is RecordNameStrategy , which sets the namespace to the record, regardless of the topic to which the event is written. The more restrictive is TopicRecordNameStrategy , which sets the namespace to both the record and the topic to which the event is written. References Should You Put Several Event Types in the Same Kafka Topic? Confluent Schema Registry documentation","title":"Schema-on-Read"},{"location":"event/schema-on-read/#schema-on-read","text":"Schema-on-Read leaves the validation of an Event schema to the reader. There are several use cases for this pattern, and all of them provide a lot of flexibility to Event Processors and Event Sinks : There may be different versions of the same schema type, and the reader wants to choose which version to apply to a given event. The sequencing may matter between events of different types, while all of the different event types are put into a single stream. For example, consider a banking use case where a customer first opens an account, then gets approval, then makes a deposit, and so on. Put these heterogeneous Event types into the same stream, allowing the Event Streaming Platform to maintain event ordering and allowing any consuming Event Processors and Event Sinks to deserialize the events as needed. There may be unstructured data written into an Event Stream , and the reader can then apply whichever schema it wants.","title":"Schema-on-Read"},{"location":"event/schema-on-read/#problem","text":"How can an Event Processor apply a schema to data while reading the data from an Event Streaming Platform ?","title":"Problem"},{"location":"event/schema-on-read/#solution","text":"The Schema-on-Read approach enables each reader to decide how to read data, and which version of which schema to apply to each Event that it reads. To make schema management easier, the design can use a centralized repository that stores multiple versions of different schemas, and then client applications can choose which schema to apply to events at runtime.","title":"Solution"},{"location":"event/schema-on-read/#implementation","text":"With Confluent Schema Registry , all of the schemas are managed in a centralized repository. In addition to storing the schema information, Schema Registry can be configured to check and enforce that schema changes are compatible with previous versions. For example, if a business started with a schema definition for an event that has two fields, but the business needs later evolve to warrant an optional third field, then the schema evolves with the needs of the business. Schema Registry ensures that the new schema is compatible with the old schema. In this particular case, for backward compatibility, the third field, status , can be defined with a default value, which will be used for the missing field when deserializing data encoded with the old schema. This ensures that all events in a given stream follow Schema Compatibility rules and that applications can continue to process those events. { \"namespace\": \"example.avro\", \"type\": \"record\", \"name\": \"user\", \"fields\": [ {\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"address\", \"type\": \"string\"}, {\"name\": \"status\", \"type\": \"string\", \"default\": \"waiting\"} ] } As another example, if a use case warrants different event types being written into a single stream, with Apache Kafka\u00ae you can set the \"subject naming strategy\" to register schemas against the record type instead of the Kafka topic. Schema Registry will then allow for Schema Evolution and Schema Compatibility validation to be performed within the scope of each event type rather than the topic. The consumer application can read schema versions assigned to the data type, and if there are different data types in any given stream, the application can cast each event to the appropriate type at processing time and then follow the appropriate code path: if (Account.equals(record.getClass()) { ... } else if (Approval.equals(record.getClass())) { ... } else if (Transaction.equals(record.getClass())) { ... } else { ... }","title":"Implementation"},{"location":"event/schema-on-read/#considerations","text":"The schema's subject naming strategy can be set to record type (rather than Kafka topic) in one of two ways. The less restrictive is RecordNameStrategy , which sets the namespace to the record, regardless of the topic to which the event is written. The more restrictive is TopicRecordNameStrategy , which sets the namespace to both the record and the topic to which the event is written.","title":"Considerations"},{"location":"event/schema-on-read/#references","text":"Should You Put Several Event Types in the Same Kafka Topic? Confluent Schema Registry documentation","title":"References"},{"location":"event-processing/claim-check/","text":"Claim Check Sometimes compression can reduce message size but there are various use cases that entail large message payloads where compression may not be enough. Often these use cases are related to image, video, or audio processing: image recognition, video analytics, audio analytics, etc. Problem How can we handle use cases where the Event payload is too large or too expensive to move through the Event Streaming Platform ? Solution Instead of storing the entire event in the event streaming platform, store the event payload in a persistent external store that can be shared between producers and consumers. The producer can write the reference address into the event streaming platform, and downstream clients use the address to retrieve the event from the external store and then process it as needed. Implementation The event stored in Kafka contains only a reference to the object in the external store. This can be a full URI string, an abstract data type (e.g., Java object) with separate fields for bucket name and filename, or whatever fields are required to identify the object. Optionally, the event may contain additional data fields to better describe the object (e.g., metadata such as who created the object). The following example uses Kafka's Java producer client. Here, we keep things simple as the event's value stores no information other than the reference (URI) to its respective object in external storage. // Write object to external storage storageClient.putObject(bucketName, objectName, object); // Write URI to Kafka URI eventValue = new URI(bucketName, objectName); producer.send(new ProducerRecord<String, URI>(topic, eventKey, eventValue)); Considerations The Event Source is responsible for ensuring that the data is properly stored in the external store, such that the reference passed within the Event is valid. Since the producer should be doing this atomically, take into consideration the same issues as mentioned in Database Write Aside . Also, if a Compacted Event Stream is used for storing the \"reference\" events (e.g., topic compaction in the case of Kafka), then the compaction will remove just the event with the reference. However, it will not remove the referenced (large) object itself from the external store, so that object needs a different expiry mechanism. References This pattern is similar in idea to Claim Check in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf An alternative approach to handling large messages is Event Chunking","title":"Claim Check"},{"location":"event-processing/claim-check/#claim-check","text":"Sometimes compression can reduce message size but there are various use cases that entail large message payloads where compression may not be enough. Often these use cases are related to image, video, or audio processing: image recognition, video analytics, audio analytics, etc.","title":"Claim Check"},{"location":"event-processing/claim-check/#problem","text":"How can we handle use cases where the Event payload is too large or too expensive to move through the Event Streaming Platform ?","title":"Problem"},{"location":"event-processing/claim-check/#solution","text":"Instead of storing the entire event in the event streaming platform, store the event payload in a persistent external store that can be shared between producers and consumers. The producer can write the reference address into the event streaming platform, and downstream clients use the address to retrieve the event from the external store and then process it as needed.","title":"Solution"},{"location":"event-processing/claim-check/#implementation","text":"The event stored in Kafka contains only a reference to the object in the external store. This can be a full URI string, an abstract data type (e.g., Java object) with separate fields for bucket name and filename, or whatever fields are required to identify the object. Optionally, the event may contain additional data fields to better describe the object (e.g., metadata such as who created the object). The following example uses Kafka's Java producer client. Here, we keep things simple as the event's value stores no information other than the reference (URI) to its respective object in external storage. // Write object to external storage storageClient.putObject(bucketName, objectName, object); // Write URI to Kafka URI eventValue = new URI(bucketName, objectName); producer.send(new ProducerRecord<String, URI>(topic, eventKey, eventValue));","title":"Implementation"},{"location":"event-processing/claim-check/#considerations","text":"The Event Source is responsible for ensuring that the data is properly stored in the external store, such that the reference passed within the Event is valid. Since the producer should be doing this atomically, take into consideration the same issues as mentioned in Database Write Aside . Also, if a Compacted Event Stream is used for storing the \"reference\" events (e.g., topic compaction in the case of Kafka), then the compaction will remove just the event with the reference. However, it will not remove the referenced (large) object itself from the external store, so that object needs a different expiry mechanism.","title":"Considerations"},{"location":"event-processing/claim-check/#references","text":"This pattern is similar in idea to Claim Check in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf An alternative approach to handling large messages is Event Chunking","title":"References"},{"location":"event-processing/content-filter/","text":"Content Filter Events in an Event Processing Application can often be very large. We tend to capture data exactly as it arrives, and then process it, rather than processing it first and only storing the results. So the event that we want to consume often contains much more information than we actually need for the task in hand. For example, we might pull in a product feed from a third-party API and store that data exactly as it was received. Later, we might ask the question, \"How many products are in each product category?\" and find that every event contains 100 fields, when we're really only interested in counting one. At the very least, this is inefficient; the network, memory, and serialization costs are 100x higher than they need to be. But manually inspecting the data actually becomes painful -- hunting through 100 fields to find and check the one that we care about. Equally important, we may have security and data privacy concerns to address. Imagine that we have a stream of data representing users' personal details and site preferences. If the marketing department wants to get more information about our global customer base, we might be able to share the users' timezone and currency settings, but only those fields . We need a method of storing complete events while only giving consumers a subset of the event fields. Problem How can I simply consume only a few data items from a large event? Solution Create an Event Processor that inspects each event, pulls out the fields of interest, and passes new, smaller events downstream for further processing. Implementation As an example, in the streaming database ksqlDB , we can use a SELECT statement to easily transform a rich event stream into a stream of simpler events. Assume that we have an event stream called products , where each event contains a huge number of fields. We are only interested in four fields: producer_id , category , sku , and price . We can prune down the events to just those fields with the following query: CREATE OR REPLACE STREAM product_summaries AS SELECT product_id, category, sku, price FROM products; Or we can perform an equivalent transformation using the Apache Kafka\u00ae client library Kafka Streams , perhaps as part of a larger processing pipeline: builder.stream(\"products\", Consumed.with(Serdes.Long(), productSerde)) .mapValues( (product) -> { ProductSummary summary = new ProductSummary(); summary.setCategory(product.getCategory()); summary.setSku(product.getSku()); summary.setPrice(product.getPrice()); return summary; }) .to(\"product_summaries\", Produced.with(Serdes.Long(), productSummarySerde)); Considerations Since filtering the content creates a new stream, it's worth considering how the new stream will be partitioned, as discussed in the Partitioned Placement pattern. By default, the new stream will inherit the same partitioning key as its source, but we can repartition the data to suit our new use case (for example, by specifying a PARTITION BY clause in ksqlDB). In the example above, our third-party product feed might be partitioned by the vendor's unique product_id , but for this use case, it might make more sense to partition the filtered events by their category . See the ksqlDB documentation for details. References This pattern is derived from Content Filter in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. For filtering out entire events from a stream, consider the Event Filter pattern.","title":"Content Filter"},{"location":"event-processing/content-filter/#content-filter","text":"Events in an Event Processing Application can often be very large. We tend to capture data exactly as it arrives, and then process it, rather than processing it first and only storing the results. So the event that we want to consume often contains much more information than we actually need for the task in hand. For example, we might pull in a product feed from a third-party API and store that data exactly as it was received. Later, we might ask the question, \"How many products are in each product category?\" and find that every event contains 100 fields, when we're really only interested in counting one. At the very least, this is inefficient; the network, memory, and serialization costs are 100x higher than they need to be. But manually inspecting the data actually becomes painful -- hunting through 100 fields to find and check the one that we care about. Equally important, we may have security and data privacy concerns to address. Imagine that we have a stream of data representing users' personal details and site preferences. If the marketing department wants to get more information about our global customer base, we might be able to share the users' timezone and currency settings, but only those fields . We need a method of storing complete events while only giving consumers a subset of the event fields.","title":"Content Filter"},{"location":"event-processing/content-filter/#problem","text":"How can I simply consume only a few data items from a large event?","title":"Problem"},{"location":"event-processing/content-filter/#solution","text":"Create an Event Processor that inspects each event, pulls out the fields of interest, and passes new, smaller events downstream for further processing.","title":"Solution"},{"location":"event-processing/content-filter/#implementation","text":"As an example, in the streaming database ksqlDB , we can use a SELECT statement to easily transform a rich event stream into a stream of simpler events. Assume that we have an event stream called products , where each event contains a huge number of fields. We are only interested in four fields: producer_id , category , sku , and price . We can prune down the events to just those fields with the following query: CREATE OR REPLACE STREAM product_summaries AS SELECT product_id, category, sku, price FROM products; Or we can perform an equivalent transformation using the Apache Kafka\u00ae client library Kafka Streams , perhaps as part of a larger processing pipeline: builder.stream(\"products\", Consumed.with(Serdes.Long(), productSerde)) .mapValues( (product) -> { ProductSummary summary = new ProductSummary(); summary.setCategory(product.getCategory()); summary.setSku(product.getSku()); summary.setPrice(product.getPrice()); return summary; }) .to(\"product_summaries\", Produced.with(Serdes.Long(), productSummarySerde));","title":"Implementation"},{"location":"event-processing/content-filter/#considerations","text":"Since filtering the content creates a new stream, it's worth considering how the new stream will be partitioned, as discussed in the Partitioned Placement pattern. By default, the new stream will inherit the same partitioning key as its source, but we can repartition the data to suit our new use case (for example, by specifying a PARTITION BY clause in ksqlDB). In the example above, our third-party product feed might be partitioned by the vendor's unique product_id , but for this use case, it might make more sense to partition the filtered events by their category . See the ksqlDB documentation for details.","title":"Considerations"},{"location":"event-processing/content-filter/#references","text":"This pattern is derived from Content Filter in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. For filtering out entire events from a stream, consider the Event Filter pattern.","title":"References"},{"location":"event-processing/dead-letter-stream/","text":"Dead Letter Stream Event Processing Applications may encounter invalid data as they operate over the infinite stream of events. Errors may include invalid data formats, nonsensical, missing or corrupt values, technical failures, or other unexpected scenarios. Problem How can an event processing application handle processing failures without terminating, or becoming stuck, when a message cannot be read? Solution When the event processing application cannot process an event for an unrecoverable reason, the problematic event is published to a \u201cdead letter stream\u201d. This stream stores the event allowing it to be logged, reprocessed later, or otherwise acted upon. Additional contextual information can be provided in the \"dead letter event\" to ease fault resolution later, such as details on why its processing failed. Implementation Java Basic Kafka Consumer while (keepConsuming) { try { final ConsumerRecords<K, V> records = consumer.poll(Duration.ofSeconds(1)); try { eventProcessor.process(records); } catch (Exception ex) { deadEventReporter.report(/*Error Details*/); } } catch (SerializationException se) { deadEventReporter.report(/*Error Details*/); } } Python Basic Kafka Consumer while True: try: event = consumer.poll(1000) except SerializerError as e: deadEventReporter.report(e) break if msg.error(): deadEventReporter.report(msg.error()) continue if msg is None: continue eventProcessor.process(msg) Considerations What should real-world applications do with the events in the dead letter stream? Reprocessing events automatically will often lead to reorderings and hence the potential for corruption to downstream systems if the stream contains events that represent changing states of the same underlying entity, such as orders being booked, processed, or shipped. Manual reprocessing can be useful, but is often viewed more as an error log in many real-world implementations. References This pattern is derived from Dead Letter Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Confluent\u2019s Schema Registry provides data governance capabilities including \u201cschema on write\u201d enforcement which can help insulate downstream consumers from unexpected event formats. Kafka Streams provides the ability to register a customer Serde to handle corrupt records allowing for creation of Dead Event data which could be published to a Dead Event Stream. See this Confluent Kafka Streams FAQ for details. ksqlDB emits a stream of records, called the Processing Log , which is analogous to a Dead Letter Stream. The Processing Log contains structured events including processing errors and additional debugging details for ksqlDB applications. See this Kafka Tutorial for more information.","title":"Dead Letter Stream"},{"location":"event-processing/dead-letter-stream/#dead-letter-stream","text":"Event Processing Applications may encounter invalid data as they operate over the infinite stream of events. Errors may include invalid data formats, nonsensical, missing or corrupt values, technical failures, or other unexpected scenarios.","title":"Dead Letter Stream"},{"location":"event-processing/dead-letter-stream/#problem","text":"How can an event processing application handle processing failures without terminating, or becoming stuck, when a message cannot be read?","title":"Problem"},{"location":"event-processing/dead-letter-stream/#solution","text":"When the event processing application cannot process an event for an unrecoverable reason, the problematic event is published to a \u201cdead letter stream\u201d. This stream stores the event allowing it to be logged, reprocessed later, or otherwise acted upon. Additional contextual information can be provided in the \"dead letter event\" to ease fault resolution later, such as details on why its processing failed.","title":"Solution"},{"location":"event-processing/dead-letter-stream/#implementation","text":"Java Basic Kafka Consumer while (keepConsuming) { try { final ConsumerRecords<K, V> records = consumer.poll(Duration.ofSeconds(1)); try { eventProcessor.process(records); } catch (Exception ex) { deadEventReporter.report(/*Error Details*/); } } catch (SerializationException se) { deadEventReporter.report(/*Error Details*/); } } Python Basic Kafka Consumer while True: try: event = consumer.poll(1000) except SerializerError as e: deadEventReporter.report(e) break if msg.error(): deadEventReporter.report(msg.error()) continue if msg is None: continue eventProcessor.process(msg)","title":"Implementation"},{"location":"event-processing/dead-letter-stream/#considerations","text":"What should real-world applications do with the events in the dead letter stream? Reprocessing events automatically will often lead to reorderings and hence the potential for corruption to downstream systems if the stream contains events that represent changing states of the same underlying entity, such as orders being booked, processed, or shipped. Manual reprocessing can be useful, but is often viewed more as an error log in many real-world implementations.","title":"Considerations"},{"location":"event-processing/dead-letter-stream/#references","text":"This pattern is derived from Dead Letter Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Confluent\u2019s Schema Registry provides data governance capabilities including \u201cschema on write\u201d enforcement which can help insulate downstream consumers from unexpected event formats. Kafka Streams provides the ability to register a customer Serde to handle corrupt records allowing for creation of Dead Event data which could be published to a Dead Event Stream. See this Confluent Kafka Streams FAQ for details. ksqlDB emits a stream of records, called the Processing Log , which is analogous to a Dead Letter Stream. The Processing Log contains structured events including processing errors and additional debugging details for ksqlDB applications. See this Kafka Tutorial for more information.","title":"References"},{"location":"event-processing/event-chunking/","text":"Event Chunking Sometimes compression can reduce message size, but there are various use cases that entail large message payloads where compression may not be enough. Often these use cases are related to image, video, or audio processing: image recognition, video analytics, audio analytics, etc. Problem How do I handle use cases where the event payload is too large to move through the event streaming platform as a single event? Solution Instead of storing the entire event as a single event in the event streaming platform, break it into chunks (an approach called \"chunking\") so that the large event is sent across as multiple smaller events. The producer can do the chunking when writing events into the event streaming platform. Downstream clients consume the chunks and, when all the smaller chunks have been received, recombine (\"unchunk\") them to restore the original event. Implementation Use metadata to track each chunk so that they can be associated to their respective parent event: Association between any given chunk and its parent event The chunk\u2019s position in the parent event The total number of chunks of the parent event Considerations Chunking places additional burden on client applications. First, implementing the chunking and unchunking logic requires more application development. Second, the consumer application needs to be able to cache the chunks as it waits to receive all the smaller chunks that comprise the original event. This, in turn, can have implications on memory fragmentation and longer garbage collection (GC). Mitigating this depends on the programming language: in Java, for example, the JVM heap size and GC can be tuned. Client applications that are not aware of the protocol used for chunking events may not be able to reconstruct the original event accurately. References To handle large events, an alternative approach that may be preferred is Claim Check","title":"Event Chunking"},{"location":"event-processing/event-chunking/#event-chunking","text":"Sometimes compression can reduce message size, but there are various use cases that entail large message payloads where compression may not be enough. Often these use cases are related to image, video, or audio processing: image recognition, video analytics, audio analytics, etc.","title":"Event Chunking"},{"location":"event-processing/event-chunking/#problem","text":"How do I handle use cases where the event payload is too large to move through the event streaming platform as a single event?","title":"Problem"},{"location":"event-processing/event-chunking/#solution","text":"Instead of storing the entire event as a single event in the event streaming platform, break it into chunks (an approach called \"chunking\") so that the large event is sent across as multiple smaller events. The producer can do the chunking when writing events into the event streaming platform. Downstream clients consume the chunks and, when all the smaller chunks have been received, recombine (\"unchunk\") them to restore the original event.","title":"Solution"},{"location":"event-processing/event-chunking/#implementation","text":"Use metadata to track each chunk so that they can be associated to their respective parent event: Association between any given chunk and its parent event The chunk\u2019s position in the parent event The total number of chunks of the parent event","title":"Implementation"},{"location":"event-processing/event-chunking/#considerations","text":"Chunking places additional burden on client applications. First, implementing the chunking and unchunking logic requires more application development. Second, the consumer application needs to be able to cache the chunks as it waits to receive all the smaller chunks that comprise the original event. This, in turn, can have implications on memory fragmentation and longer garbage collection (GC). Mitigating this depends on the programming language: in Java, for example, the JVM heap size and GC can be tuned. Client applications that are not aware of the protocol used for chunking events may not be able to reconstruct the original event accurately.","title":"Considerations"},{"location":"event-processing/event-chunking/#references","text":"To handle large events, an alternative approach that may be preferred is Claim Check","title":"References"},{"location":"event-processing/event-filter/","text":"Event Filter Event Processing Applications may need to operate over a subset of Events in an Event Stream . Problem How can an application select only the relevant events (or discard uninteresting events) from an Event Stream? Solution Implementation As an example, the streaming database ksqlDB lets us create a filtered Event Stream using familiar SQL syntax: CREATE STREAM payments_only WITH (kafka_topic = 'transactions-topic') AS SELECT * FROM all_transactions WHERE type = 'purchase'; The Kafka Streams client library of Apache Kafka\u00ae provides a filter operator in its DSL. This operator filters out events that do not match a given predicate: builder .stream(\"transactions-topic\") .filter((key, transaction) -> transaction.type == \"purchase\") .to(\"payments-topic\"); References This pattern is derived from Message Filter in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. See the tutorial How to filter a stream of events for detailed examples of filtering event streams.","title":"Event Filter"},{"location":"event-processing/event-filter/#event-filter","text":"Event Processing Applications may need to operate over a subset of Events in an Event Stream .","title":"Event Filter"},{"location":"event-processing/event-filter/#problem","text":"How can an application select only the relevant events (or discard uninteresting events) from an Event Stream?","title":"Problem"},{"location":"event-processing/event-filter/#solution","text":"","title":"Solution"},{"location":"event-processing/event-filter/#implementation","text":"As an example, the streaming database ksqlDB lets us create a filtered Event Stream using familiar SQL syntax: CREATE STREAM payments_only WITH (kafka_topic = 'transactions-topic') AS SELECT * FROM all_transactions WHERE type = 'purchase'; The Kafka Streams client library of Apache Kafka\u00ae provides a filter operator in its DSL. This operator filters out events that do not match a given predicate: builder .stream(\"transactions-topic\") .filter((key, transaction) -> transaction.type == \"purchase\") .to(\"payments-topic\");","title":"Implementation"},{"location":"event-processing/event-filter/#references","text":"This pattern is derived from Message Filter in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. See the tutorial How to filter a stream of events for detailed examples of filtering event streams.","title":"References"},{"location":"event-processing/event-mapper/","text":"Event Mapper Traditional applications, which work with data at rest, and Event Processing Applications , which work with data in motion, may need to share data via an Event Streaming Platform . These applications need a common mechanism to convert data from Events to domain objects and vice versa. Problem How can I move data between domain objects in an application\u2019s internal data model and Events in an Event Streaming Platform, while keeping the two independent of each other? Solution An Event Mapper provides independence between the traditional application and the Event Streaming Platform, so that neither is aware of the other (and ideally, neither is even aware of the event mapper itself). Create an Event Mapper, or use an existing one, to map the domain model (or the application's internal data model) to the data formats accepted by the Event Streaming Platform, and vice versa. The Event Mapper reads the domain model and converts it into outgoing Events, which are sent to the Event Streaming Platform. Conversely, an Event Mapper can be used to create or update domain objects based on incoming Events. Implementation In this example, we use the Java producer client of Apache Kafka\u00ae to implement an Event Mapper that constructs an Event ( PublicationEvent ) from the domain model object ( Publication ) before the event is written to an Event Stream (called a \"topic\" in Kafka). private final IMapper domainToEventMapper = mapperFactory.buildMapper(Publication.class); private final Producer<String, PublicationEvent> producer = ... public void newPublication(String author, String title) { Publication newPub = new Publication(author, title); producer.send(author /* event key */, domainToEventMapper.map(newPub)); We can implement the reverse operation in a second Event Mapper that converts PublicationEvent Events back into domain object updates: private final IMapper eventToDomainMapper = mapperFactory.buildMapper(Publication.class); private final Consumer<String, PublicationEvent> consumer = ... public void updatePublication(PublicationEvent pubEvent) { Publication newPub = eventToDomainMapper.map(pubEvent); domainStore.update(newPub); Considerations The Event Mapper may optionally validate the schema of the converted objects. For details, see the Schema Validator pattern. References This pattern is derived from Messaging Mapper in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. See also the Event Serializer and Event Deserializer patterns.","title":"Event Mapper"},{"location":"event-processing/event-mapper/#event-mapper","text":"Traditional applications, which work with data at rest, and Event Processing Applications , which work with data in motion, may need to share data via an Event Streaming Platform . These applications need a common mechanism to convert data from Events to domain objects and vice versa.","title":"Event Mapper"},{"location":"event-processing/event-mapper/#problem","text":"How can I move data between domain objects in an application\u2019s internal data model and Events in an Event Streaming Platform, while keeping the two independent of each other?","title":"Problem"},{"location":"event-processing/event-mapper/#solution","text":"An Event Mapper provides independence between the traditional application and the Event Streaming Platform, so that neither is aware of the other (and ideally, neither is even aware of the event mapper itself). Create an Event Mapper, or use an existing one, to map the domain model (or the application's internal data model) to the data formats accepted by the Event Streaming Platform, and vice versa. The Event Mapper reads the domain model and converts it into outgoing Events, which are sent to the Event Streaming Platform. Conversely, an Event Mapper can be used to create or update domain objects based on incoming Events.","title":"Solution"},{"location":"event-processing/event-mapper/#implementation","text":"In this example, we use the Java producer client of Apache Kafka\u00ae to implement an Event Mapper that constructs an Event ( PublicationEvent ) from the domain model object ( Publication ) before the event is written to an Event Stream (called a \"topic\" in Kafka). private final IMapper domainToEventMapper = mapperFactory.buildMapper(Publication.class); private final Producer<String, PublicationEvent> producer = ... public void newPublication(String author, String title) { Publication newPub = new Publication(author, title); producer.send(author /* event key */, domainToEventMapper.map(newPub)); We can implement the reverse operation in a second Event Mapper that converts PublicationEvent Events back into domain object updates: private final IMapper eventToDomainMapper = mapperFactory.buildMapper(Publication.class); private final Consumer<String, PublicationEvent> consumer = ... public void updatePublication(PublicationEvent pubEvent) { Publication newPub = eventToDomainMapper.map(pubEvent); domainStore.update(newPub);","title":"Implementation"},{"location":"event-processing/event-mapper/#considerations","text":"The Event Mapper may optionally validate the schema of the converted objects. For details, see the Schema Validator pattern.","title":"Considerations"},{"location":"event-processing/event-mapper/#references","text":"This pattern is derived from Messaging Mapper in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. See also the Event Serializer and Event Deserializer patterns.","title":"References"},{"location":"event-processing/event-processing-application/","text":"Event Processing Application Once our data -- such as financial transactions, tracking information for shipments, IoT sensor measurements, etc. -- is set in motion as streams of events on an Event Streaming Platform, we want to put that data to use and create value from it. Event Processors are the building blocks for achieving this, but they solve only a specific part or step of a use case. Problem How can we build a full-fledged application for data in motion, an application that creates, reads, processes, and/or queries Event Streams to solve a use case end-to-end? Solution We build an Event Processing Application by composing one or more Event Processors into an interconnected processing topology for Event Streams and Tables . Here, the continuous output streams of one processor are the continuous input streams to one or more downstream processors. The combined functionality of the application then covers our use case end-to-end, or at least covers as much of the use case as we want. (The question of how many applications should implement a use case is an important design decision, which we are not covering here.) The event processors -- which make up the larger Event Processing Application -- are typically distributed, running across multiple instances, to allow for elastic, parallel, fault-tolerant processing of data in motion at scale. For example, an application can read a stream of customer payments from an Event Store in an Event Streaming Platform , then filter payments for certain customers, and then aggregate those payments per country and per week. The processing mode is stream processing; that is, data is continuously processed 24/7. As soon as new Events are available, they are processed and propagated through the topology of Event Processors . Implementation Apache Kafka\u00ae is the most popular Event Streaming Platform . There are several options for building Event Processing Applications when using Kafka. We'll cover two here. ksqlDB ksqlDB is a streaming database with which we can build Event Processing Applications using SQL syntax. It has first-class support for Streams and Tables . When we create Tables and Streams in ksqlDB, Kafka topics are used as the storage layer behind the scenes. In the example below, the ksqlDB table movies is backed by a Kafka topic of the same name. CREATE TABLE movies (ID INT PRIMARY KEY, title VARCHAR, release_year INT) WITH (kafka_topic='movies', partitions=1, value_format='avro'); CREATE STREAM ratings (MOVIE_ID INT KEY, rating DOUBLE) WITH (kafka_topic='ratings', partitions=1, value_format='avro'); As we would expect, we can add new Events using INSERT : INSERT INTO movies (id, title, release_year) VALUES (294, 'Die Hard', 1998); INSERT INTO ratings (movie_id, rating) VALUES (294, 8.2); We can also perform stream processing using ksqlDB's SQL syntax. In the following example, the command CREATE STREAM .. AS SELECT .. continuously joins the ratings stream and the movies table to create a new stream of enriched ratings. CREATE STREAM rated_movies WITH (kafka_topic='rated_movies', value_format='avro') AS SELECT ratings.movie_id as id, title, rating FROM ratings LEFT JOIN movies ON ratings.movie_id = movies.id; Kafka Streams With the Kafka Streams client library of Apache Kafka, we can implement an event processing application in Java, Scala, or other JVM languages. Here is a Kafka Streams example similar to the ksqlDB example above: KStream<Integer, Rating> ratings = builder.table(<blabla>); KTable<Integer, Movie> movies = builder.stream(<blabla>); MovieRatingJoiner joiner = new MovieRatingJoiner(); KStream <Integer, EnrichedRating> enrichedRatings = ratings.join(movies, joiner); See the tutorial How to join a stream and a lookup table for a full example of using Kafka Streams to build an Event Processing Application. Considerations When building an Event Processing Application, it's recommended to confine the application to one problem domain. While it's possible to use any number of Event Processors in the application, the Event Processors should be closely related in most cases (similar to how one would design a microservice). Event Processing Applications themselves can also be composed. This is a common design pattern for implementing event-driven architectures, which are powered by a fleet of applications and microservices. In this design, the output of one application forms the input to one or more downstream applications. This is conceptually similar to the topology of Event Processors , as described above. A key difference in practice, however, is that different applications are often built by different teams inside an organization. For example, a customer-facing application built by the Payments team is continuously feeding data via Event Streams to an application built by the Anti-Fraud team, and to another application built by the Data Science team. References The Event Streaming Platform pattern provides a higher-level overview of how Event Processing Applications are used across the Event Streaming Platform. The tutorial How to join a stream and a lookup table provides a step-by-step example of event processing using SQL. The tutorial How to sum a stream of events shows how to apply an aggregate function over an Event Stream .","title":"Event Processing Application"},{"location":"event-processing/event-processing-application/#event-processing-application","text":"Once our data -- such as financial transactions, tracking information for shipments, IoT sensor measurements, etc. -- is set in motion as streams of events on an Event Streaming Platform, we want to put that data to use and create value from it. Event Processors are the building blocks for achieving this, but they solve only a specific part or step of a use case.","title":"Event Processing Application"},{"location":"event-processing/event-processing-application/#problem","text":"How can we build a full-fledged application for data in motion, an application that creates, reads, processes, and/or queries Event Streams to solve a use case end-to-end?","title":"Problem"},{"location":"event-processing/event-processing-application/#solution","text":"We build an Event Processing Application by composing one or more Event Processors into an interconnected processing topology for Event Streams and Tables . Here, the continuous output streams of one processor are the continuous input streams to one or more downstream processors. The combined functionality of the application then covers our use case end-to-end, or at least covers as much of the use case as we want. (The question of how many applications should implement a use case is an important design decision, which we are not covering here.) The event processors -- which make up the larger Event Processing Application -- are typically distributed, running across multiple instances, to allow for elastic, parallel, fault-tolerant processing of data in motion at scale. For example, an application can read a stream of customer payments from an Event Store in an Event Streaming Platform , then filter payments for certain customers, and then aggregate those payments per country and per week. The processing mode is stream processing; that is, data is continuously processed 24/7. As soon as new Events are available, they are processed and propagated through the topology of Event Processors .","title":"Solution"},{"location":"event-processing/event-processing-application/#implementation","text":"Apache Kafka\u00ae is the most popular Event Streaming Platform . There are several options for building Event Processing Applications when using Kafka. We'll cover two here.","title":"Implementation"},{"location":"event-processing/event-processing-application/#ksqldb","text":"ksqlDB is a streaming database with which we can build Event Processing Applications using SQL syntax. It has first-class support for Streams and Tables . When we create Tables and Streams in ksqlDB, Kafka topics are used as the storage layer behind the scenes. In the example below, the ksqlDB table movies is backed by a Kafka topic of the same name. CREATE TABLE movies (ID INT PRIMARY KEY, title VARCHAR, release_year INT) WITH (kafka_topic='movies', partitions=1, value_format='avro'); CREATE STREAM ratings (MOVIE_ID INT KEY, rating DOUBLE) WITH (kafka_topic='ratings', partitions=1, value_format='avro'); As we would expect, we can add new Events using INSERT : INSERT INTO movies (id, title, release_year) VALUES (294, 'Die Hard', 1998); INSERT INTO ratings (movie_id, rating) VALUES (294, 8.2); We can also perform stream processing using ksqlDB's SQL syntax. In the following example, the command CREATE STREAM .. AS SELECT .. continuously joins the ratings stream and the movies table to create a new stream of enriched ratings. CREATE STREAM rated_movies WITH (kafka_topic='rated_movies', value_format='avro') AS SELECT ratings.movie_id as id, title, rating FROM ratings LEFT JOIN movies ON ratings.movie_id = movies.id;","title":"ksqlDB"},{"location":"event-processing/event-processing-application/#kafka-streams","text":"With the Kafka Streams client library of Apache Kafka, we can implement an event processing application in Java, Scala, or other JVM languages. Here is a Kafka Streams example similar to the ksqlDB example above: KStream<Integer, Rating> ratings = builder.table(<blabla>); KTable<Integer, Movie> movies = builder.stream(<blabla>); MovieRatingJoiner joiner = new MovieRatingJoiner(); KStream <Integer, EnrichedRating> enrichedRatings = ratings.join(movies, joiner); See the tutorial How to join a stream and a lookup table for a full example of using Kafka Streams to build an Event Processing Application.","title":"Kafka Streams"},{"location":"event-processing/event-processing-application/#considerations","text":"When building an Event Processing Application, it's recommended to confine the application to one problem domain. While it's possible to use any number of Event Processors in the application, the Event Processors should be closely related in most cases (similar to how one would design a microservice). Event Processing Applications themselves can also be composed. This is a common design pattern for implementing event-driven architectures, which are powered by a fleet of applications and microservices. In this design, the output of one application forms the input to one or more downstream applications. This is conceptually similar to the topology of Event Processors , as described above. A key difference in practice, however, is that different applications are often built by different teams inside an organization. For example, a customer-facing application built by the Payments team is continuously feeding data via Event Streams to an application built by the Anti-Fraud team, and to another application built by the Data Science team.","title":"Considerations"},{"location":"event-processing/event-processing-application/#references","text":"The Event Streaming Platform pattern provides a higher-level overview of how Event Processing Applications are used across the Event Streaming Platform. The tutorial How to join a stream and a lookup table provides a step-by-step example of event processing using SQL. The tutorial How to sum a stream of events shows how to apply an aggregate function over an Event Stream .","title":"References"},{"location":"event-processing/event-processor/","text":"Event Processor Once our data -- such as financial transactions, tracking information for shipments, IoT sensor measurements, etc. -- is set in motion as streams of events on an Event Streaming Platform , we want to put it to use and create value from it. How do we do this? Problem How can we process Events in an Event Streaming Platform ? Solution We build an Event Processor, which is a component that reads Events and processes them, possibly writing new Events as the result of its processing. An Event Processor may act as an Event Source and/or Event Sink ; in practice, both are common. An Event Processor can be distributed, which means that it has multiple instances running across different machines. In this case, the processing of Events happens concurrently across these instances. An important characteristic of an Event Processor is that it should allow for composition with other Event Processors. In practice, we rarely use a single Event Processor in isolation. Instead, we compose and connect one or more Event Processors (via Event Streams ) inside an Event Processing Application that fully implements one particular use case end-to-end, or that implements a subset of the overall business logic limited to the bounded context of a particular domain (for example, in a microservices architecture). An Event Processor performs a specific task within the Event Processing Application. Think of the Event Processor as one processing node, or processing step, of a larger processing topology. Examples are the mapping of an event type to a domain object, filtering only the important events out of an Event Stream , enriching an event stream with additional data by joining it to another stream or database table, triggering alerts, or creating new events for consumption by other applications. Implementation There are multiple ways to create an Event Processing Application using Event Processors. We will look at two. ksqlDB The streaming database ksqlDB provides a familiar SQL syntax, which we can use to create Event Processing Applications. ksqlDB parses SQL commands and constructs and manages the Event Processors that we define as part of an Event Processing Application. In the following example, we create a ksqlDB query that reads data from the readings Event Stream and \"cleans\" the Event values. The query publishes the clean readings to a new stream called clean_readings . Here, this query acts as an event processing application comprising multiple interconnected Event Processors. CREATE STREAM clean_readings AS SELECT sensor, reading, UCASE(location) AS location FROM readings EMIT CHANGES; With ksqlDB, we can view each section of the command as the construction of a different Event Processor: CREATE STREAM defines the new output Event Stream to which this application will produce Events. SELECT ... is a mapping function, taking each input Event and \"cleaning\" it as defined. In this example, cleaning simply means uppercasing the location field in each input reading. FROM ... is a source Event Processor that defines the input Event Stream for the overall application. EMIT CHANGES is ksqlDB syntax that defines our query as continuously running, and specifies that incremental changes will be produced as the query runs perpetually. Kafka Streams The Kafka Streams DSL provides abstractions for Event Streams and Tables , as well as stateful and stateless transformation functions ( map , filter , and others). Each of these functions can act as an Event Processor in the larger Event Processing Application that we build with the Kafka Streams library. builder .stream(\"readings\"); .mapValues((key, value)-> new Reading(value.sensor, value.reading, value.location.toUpperCase()) .to(\"clean\"); In the above example, we use the Kafka Streams StreamsBuilder to construct the stream processing topology. First, we create an input stream, using the stream function. This creates an Event Stream from the designated Apache Kafka\u00ae topic. Next, we transform the Events, using the mapValues function. This function accepts an Event and returns a new Event with any desired transformations to the original Event's values. Finally, we write the transformed Events to a destination Kafka topic, using the to function. This function terminates our stream processing topology. Considerations While it could be tempting to build a \"multi-purpose\" Event Processor, it's important to design processors in a composable way. By building processors as discrete units, it is easier to reason about what each processor does, and by extension, what the Event Processing Application does. References See the Event Processing Application pattern. Event Processing Applications are composed of Event Processors. In Kafka Streams , a processor is a node in the processor topology, representing a step to transform Events. See the blog post How Real-Time Stream Processing Works with ksqlDB, Animated . The blog post Intro to Apache Kafka: How Kafka Works provides details about the core Kafka concepts, such as Events and topics.","title":"Event Processor"},{"location":"event-processing/event-processor/#event-processor","text":"Once our data -- such as financial transactions, tracking information for shipments, IoT sensor measurements, etc. -- is set in motion as streams of events on an Event Streaming Platform , we want to put it to use and create value from it. How do we do this?","title":"Event Processor"},{"location":"event-processing/event-processor/#problem","text":"How can we process Events in an Event Streaming Platform ?","title":"Problem"},{"location":"event-processing/event-processor/#solution","text":"We build an Event Processor, which is a component that reads Events and processes them, possibly writing new Events as the result of its processing. An Event Processor may act as an Event Source and/or Event Sink ; in practice, both are common. An Event Processor can be distributed, which means that it has multiple instances running across different machines. In this case, the processing of Events happens concurrently across these instances. An important characteristic of an Event Processor is that it should allow for composition with other Event Processors. In practice, we rarely use a single Event Processor in isolation. Instead, we compose and connect one or more Event Processors (via Event Streams ) inside an Event Processing Application that fully implements one particular use case end-to-end, or that implements a subset of the overall business logic limited to the bounded context of a particular domain (for example, in a microservices architecture). An Event Processor performs a specific task within the Event Processing Application. Think of the Event Processor as one processing node, or processing step, of a larger processing topology. Examples are the mapping of an event type to a domain object, filtering only the important events out of an Event Stream , enriching an event stream with additional data by joining it to another stream or database table, triggering alerts, or creating new events for consumption by other applications.","title":"Solution"},{"location":"event-processing/event-processor/#implementation","text":"There are multiple ways to create an Event Processing Application using Event Processors. We will look at two.","title":"Implementation"},{"location":"event-processing/event-processor/#ksqldb","text":"The streaming database ksqlDB provides a familiar SQL syntax, which we can use to create Event Processing Applications. ksqlDB parses SQL commands and constructs and manages the Event Processors that we define as part of an Event Processing Application. In the following example, we create a ksqlDB query that reads data from the readings Event Stream and \"cleans\" the Event values. The query publishes the clean readings to a new stream called clean_readings . Here, this query acts as an event processing application comprising multiple interconnected Event Processors. CREATE STREAM clean_readings AS SELECT sensor, reading, UCASE(location) AS location FROM readings EMIT CHANGES; With ksqlDB, we can view each section of the command as the construction of a different Event Processor: CREATE STREAM defines the new output Event Stream to which this application will produce Events. SELECT ... is a mapping function, taking each input Event and \"cleaning\" it as defined. In this example, cleaning simply means uppercasing the location field in each input reading. FROM ... is a source Event Processor that defines the input Event Stream for the overall application. EMIT CHANGES is ksqlDB syntax that defines our query as continuously running, and specifies that incremental changes will be produced as the query runs perpetually.","title":"ksqlDB"},{"location":"event-processing/event-processor/#kafka-streams","text":"The Kafka Streams DSL provides abstractions for Event Streams and Tables , as well as stateful and stateless transformation functions ( map , filter , and others). Each of these functions can act as an Event Processor in the larger Event Processing Application that we build with the Kafka Streams library. builder .stream(\"readings\"); .mapValues((key, value)-> new Reading(value.sensor, value.reading, value.location.toUpperCase()) .to(\"clean\"); In the above example, we use the Kafka Streams StreamsBuilder to construct the stream processing topology. First, we create an input stream, using the stream function. This creates an Event Stream from the designated Apache Kafka\u00ae topic. Next, we transform the Events, using the mapValues function. This function accepts an Event and returns a new Event with any desired transformations to the original Event's values. Finally, we write the transformed Events to a destination Kafka topic, using the to function. This function terminates our stream processing topology.","title":"Kafka Streams"},{"location":"event-processing/event-processor/#considerations","text":"While it could be tempting to build a \"multi-purpose\" Event Processor, it's important to design processors in a composable way. By building processors as discrete units, it is easier to reason about what each processor does, and by extension, what the Event Processing Application does.","title":"Considerations"},{"location":"event-processing/event-processor/#references","text":"See the Event Processing Application pattern. Event Processing Applications are composed of Event Processors. In Kafka Streams , a processor is a node in the processor topology, representing a step to transform Events. See the blog post How Real-Time Stream Processing Works with ksqlDB, Animated . The blog post Intro to Apache Kafka: How Kafka Works provides details about the core Kafka concepts, such as Events and topics.","title":"References"},{"location":"event-processing/event-router/","text":"Event Router Event Streams may contain a subset of Events which need to be processed in isolation. For example, an inventory check system may be distributed across multiple physical systems, and the target system may depend on the category of the item being checked. Problem How can we isolate Events into a dedicated Event Stream based on an attribute of the Events? Solution Implementation With the streaming database ksqlDB , we can continuously route Events to a different Event Stream. We use the CREATE STREAM syntax with an appropriate WHERE filter: CREATE STREAM payments ...; CREATE STREAM payments_france AS SELECT * FROM payments WHERE country = 'france'; CREATE STREAM payments_spain AS SELECT * FROM payments WHERE country = 'spain'; With the Apache Kafka\u00ae client library Kafka Streams , use a TopicNameExtractor to route Events to different Event Streams (called \"topics\" in Kafka). The TopicNameExtractor has one method to implement, extract() , which accepts three parameters: The event key The event value The RecordContext , which provides access to headers, partitions, and other contextual information about the event We can use any of the given parameters to generate and return the desired destination topic name for the given Event. Kafka Streams will complete the routing. CountryTopicExtractor implements TopicNameExtractor<String, String> { String extract(String key, String value, RecordContext recordContext) { switch (value.country) { case \"france\": return \"france-topic\"; case \"spain\": return \"spain-topic\"; } } } KStream<String, String> myStream = builder.stream(...); myStream.mapValues(..).to(new CountryTopicExtractor()); Considerations Event Routers should not modify the Event itself, and instead should provide only proper routing to the desired destinations. If an Event Router needs to attach additional information or context to an Event, consider using the Event Envelope pattern. References This pattern is derived from Message Router in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. See the tutorial How to dynamically choose the output topic at runtime for a full example of dynamically routing Events at runtime.","title":"Event Router"},{"location":"event-processing/event-router/#event-router","text":"Event Streams may contain a subset of Events which need to be processed in isolation. For example, an inventory check system may be distributed across multiple physical systems, and the target system may depend on the category of the item being checked.","title":"Event Router"},{"location":"event-processing/event-router/#problem","text":"How can we isolate Events into a dedicated Event Stream based on an attribute of the Events?","title":"Problem"},{"location":"event-processing/event-router/#solution","text":"","title":"Solution"},{"location":"event-processing/event-router/#implementation","text":"With the streaming database ksqlDB , we can continuously route Events to a different Event Stream. We use the CREATE STREAM syntax with an appropriate WHERE filter: CREATE STREAM payments ...; CREATE STREAM payments_france AS SELECT * FROM payments WHERE country = 'france'; CREATE STREAM payments_spain AS SELECT * FROM payments WHERE country = 'spain'; With the Apache Kafka\u00ae client library Kafka Streams , use a TopicNameExtractor to route Events to different Event Streams (called \"topics\" in Kafka). The TopicNameExtractor has one method to implement, extract() , which accepts three parameters: The event key The event value The RecordContext , which provides access to headers, partitions, and other contextual information about the event We can use any of the given parameters to generate and return the desired destination topic name for the given Event. Kafka Streams will complete the routing. CountryTopicExtractor implements TopicNameExtractor<String, String> { String extract(String key, String value, RecordContext recordContext) { switch (value.country) { case \"france\": return \"france-topic\"; case \"spain\": return \"spain-topic\"; } } } KStream<String, String> myStream = builder.stream(...); myStream.mapValues(..).to(new CountryTopicExtractor());","title":"Implementation"},{"location":"event-processing/event-router/#considerations","text":"Event Routers should not modify the Event itself, and instead should provide only proper routing to the desired destinations. If an Event Router needs to attach additional information or context to an Event, consider using the Event Envelope pattern.","title":"Considerations"},{"location":"event-processing/event-router/#references","text":"This pattern is derived from Message Router in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. See the tutorial How to dynamically choose the output topic at runtime for a full example of dynamically routing Events at runtime.","title":"References"},{"location":"event-processing/event-splitter/","text":"Event Splitter One Event may actually contain multiple child Events, each of which may need to be processed in a different way. Problem How can an Event be split into multiple Events for distinct processing? Solution Split the original Event into multiple child Events. Then publish one Event for each of the child Events. Implementation Many event processing technologies support this operation. The streaming database ksqlDB provides an EXPLODE() table function, which takes an array and outputs one value for each of the elements of the array. The example below processes each input Event, un-nesting the array and generating new Events for each element, with new column names. SELECT EXPLODE(TOTAL)->TOTALTYPE AS TOTAL_TYPE, EXPLODE(TOTAL)->TOTALAMOUNT AS TOTAL_AMOUNT, EXPLODE(TOTAL)->ID AS CUSTOMER_ID FROM my_stream EMIT CHANGES; The Apache Kafka\u00ae client library Kafka Streams has an analogous method, called flatMap() . The example below processes each input Event and generates new Events, with new keys and values. KStream<Long, String> myStream = ...; KStream<String, Integer> splitStream = myStream.flatMap( (eventKey, eventValue) -> { List<KeyValue<String, Integer>> result = new LinkedList<>(); result.add(KeyValue.pair(eventValue.toUpperCase(), 1000)); result.add(KeyValue.pair(eventValue.toLowerCase(), 9000)); return result; } ); Or, as my grandmother used to say: There once was a man from Manhattan, With Events that he needed to flatten. He cooked up a scheme To call flatMap on stream , Then he wrote it all down as a pattern. Considerations If you have child Events that must be routed to different Event Streams, see the Event Router pattern, used to route Events to different locations. For capacity planning and sizing, consider that splitting the original Event into N child Events leads to write amplification, increasing the volume of Events that must be managed by the Event Streaming Platform . A use case may require that you track the lineage of parent and child Events. If so, ensure that the child Events include a data field containing a reference to the original parent Event (for example, a unique identifier). References This pattern is derived from Splitter in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf.","title":"Event Splitter"},{"location":"event-processing/event-splitter/#event-splitter","text":"One Event may actually contain multiple child Events, each of which may need to be processed in a different way.","title":"Event Splitter"},{"location":"event-processing/event-splitter/#problem","text":"How can an Event be split into multiple Events for distinct processing?","title":"Problem"},{"location":"event-processing/event-splitter/#solution","text":"Split the original Event into multiple child Events. Then publish one Event for each of the child Events.","title":"Solution"},{"location":"event-processing/event-splitter/#implementation","text":"Many event processing technologies support this operation. The streaming database ksqlDB provides an EXPLODE() table function, which takes an array and outputs one value for each of the elements of the array. The example below processes each input Event, un-nesting the array and generating new Events for each element, with new column names. SELECT EXPLODE(TOTAL)->TOTALTYPE AS TOTAL_TYPE, EXPLODE(TOTAL)->TOTALAMOUNT AS TOTAL_AMOUNT, EXPLODE(TOTAL)->ID AS CUSTOMER_ID FROM my_stream EMIT CHANGES; The Apache Kafka\u00ae client library Kafka Streams has an analogous method, called flatMap() . The example below processes each input Event and generates new Events, with new keys and values. KStream<Long, String> myStream = ...; KStream<String, Integer> splitStream = myStream.flatMap( (eventKey, eventValue) -> { List<KeyValue<String, Integer>> result = new LinkedList<>(); result.add(KeyValue.pair(eventValue.toUpperCase(), 1000)); result.add(KeyValue.pair(eventValue.toLowerCase(), 9000)); return result; } ); Or, as my grandmother used to say: There once was a man from Manhattan, With Events that he needed to flatten. He cooked up a scheme To call flatMap on stream , Then he wrote it all down as a pattern.","title":"Implementation"},{"location":"event-processing/event-splitter/#considerations","text":"If you have child Events that must be routed to different Event Streams, see the Event Router pattern, used to route Events to different locations. For capacity planning and sizing, consider that splitting the original Event into N child Events leads to write amplification, increasing the volume of Events that must be managed by the Event Streaming Platform . A use case may require that you track the lineage of parent and child Events. If so, ensure that the child Events include a data field containing a reference to the original parent Event (for example, a unique identifier).","title":"Considerations"},{"location":"event-processing/event-splitter/#references","text":"This pattern is derived from Splitter in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf.","title":"References"},{"location":"event-processing/event-streaming-api/","text":"Event Streaming API Applications that connect to the Event Streaming Platform need to do so in a consistent and reliable way. Problem How can my application connect to an Event Streaming Platform to send and receive Events ? Solution The Event Streaming Platform provides an Application Programming Interface (API) that allows applications to reliably communicate across the platform. The API provides a logical and well-documented protocol, which defines the message structure and data exchange methods. Higher-level libraries implement these protocols, so that a variety of technologies and programming languages can be used to interface with the platform. These higher-level libraries allow the application to focus on business logic and leave the details of platform communication to the API. References This pattern is derived from Message Endpoint in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. The Kafka Protocol Guide provides details about the wire protocol implemented in Kafka. The Kafka API documentation contains information about the variety of APIs available for reading from, writing to, and administering Kafka.","title":"Event Streaming API"},{"location":"event-processing/event-streaming-api/#event-streaming-api","text":"Applications that connect to the Event Streaming Platform need to do so in a consistent and reliable way.","title":"Event Streaming API"},{"location":"event-processing/event-streaming-api/#problem","text":"How can my application connect to an Event Streaming Platform to send and receive Events ?","title":"Problem"},{"location":"event-processing/event-streaming-api/#solution","text":"The Event Streaming Platform provides an Application Programming Interface (API) that allows applications to reliably communicate across the platform. The API provides a logical and well-documented protocol, which defines the message structure and data exchange methods. Higher-level libraries implement these protocols, so that a variety of technologies and programming languages can be used to interface with the platform. These higher-level libraries allow the application to focus on business logic and leave the details of platform communication to the API.","title":"Solution"},{"location":"event-processing/event-streaming-api/#references","text":"This pattern is derived from Message Endpoint in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. The Kafka Protocol Guide provides details about the wire protocol implemented in Kafka. The Kafka API documentation contains information about the variety of APIs available for reading from, writing to, and administering Kafka.","title":"References"},{"location":"event-processing/event-translator/","text":"Event Translator Event Streaming Platforms will connect a variety of systems over time, and it may not be feasible to use common data formats across all of these systems. Problem How can systems that use different data formats communicate with each other via Events ? Solution An Event Translator converts a data format into a standard format that is familiar to downstream Event Processors . This can take the form of field manipulation -- for example, mapping one Event schema to another Event schema. Another common form is different serialization types -- for example, translating Apache Avro\u2122 to JSON or Protocol Buffers (Protobuf) to Avro. Implementation With the streaming database ksqlDB , we can create Event Streams using SQL statements: CREATE STREAM translated_stream AS SELECT fieldX AS fieldC, field.Y AS fieldA, field.Z AS fieldB FROM untranslated_stream Considerations In some cases, translations will be unidirectional if data is lost. For example, translating XML to JSON will often lose information, meaning that the original form cannot be recreated. References This pattern is derived from Event Translator in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf.","title":"Event Translator"},{"location":"event-processing/event-translator/#event-translator","text":"Event Streaming Platforms will connect a variety of systems over time, and it may not be feasible to use common data formats across all of these systems.","title":"Event Translator"},{"location":"event-processing/event-translator/#problem","text":"How can systems that use different data formats communicate with each other via Events ?","title":"Problem"},{"location":"event-processing/event-translator/#solution","text":"An Event Translator converts a data format into a standard format that is familiar to downstream Event Processors . This can take the form of field manipulation -- for example, mapping one Event schema to another Event schema. Another common form is different serialization types -- for example, translating Apache Avro\u2122 to JSON or Protocol Buffers (Protobuf) to Avro.","title":"Solution"},{"location":"event-processing/event-translator/#implementation","text":"With the streaming database ksqlDB , we can create Event Streams using SQL statements: CREATE STREAM translated_stream AS SELECT fieldX AS fieldC, field.Y AS fieldA, field.Z AS fieldB FROM untranslated_stream","title":"Implementation"},{"location":"event-processing/event-translator/#considerations","text":"In some cases, translations will be unidirectional if data is lost. For example, translating XML to JSON will often lose information, meaning that the original form cannot be recreated.","title":"Considerations"},{"location":"event-processing/event-translator/#references","text":"This pattern is derived from Event Translator in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf.","title":"References"},{"location":"event-processing/idempotent-reader/","text":"Idempotent Reader In ideal circumstances, Events are only written once into an Event Stream . Under normal operations, all consumers of the stream will also only read and process each event once. However, depending on the behavior and configuration of the Event Source , there may be failures that create duplicate events. When this happens, we need a strategy for dealing with the duplicates. An idempotent reader must take two causes of duplicate events into consideration: Operational Failures : Intermittent network and system failures are unavoidable in distributed systems. In the case of a machine failure or a brief network outage, an Event Source may produce the same event multiple times due to retries. Similarly, an Event Sink may consume and process the same event multiple times due to intermittent offset updating failures. The Event Streaming Platform should automatically guard against these operational failures by providing strong delivery and processing guarantees, such as those found in Apache Kafka\u00ae transactions. Incorrect Application Logic : An Event Source could mistakenly produce the same event multiple times, populating the Event Stream with multiple distinct events from the perspective of the Event Streaming Platform . For example, imagine a bug in the Event Source that writes a customer payment event twice, instead of just once. The Event Streaming Platform knows nothing of the business logic, so it cannot differentiate between the two events and instead considers them as two distinct payment events. Problem How can an application deal with duplicate Events when reading from an Event Stream? Solution This can be addressed using exactly-once semantics (EOS), including native support for transactions and support for idempotent clients. EOS allows Event Streaming Applications to process data without loss or duplication, ensuring that computed results are always accurate. Idempotent Writing by the Event Source is the first step in solving this problem. Idempotent Writing provides strong, exactly-once delivery guarantees of the producer's Events, and removes operational failures as a cause of written duplicate Events. On the reading side, in Event Processors and Event Sinks , an Idempotent Reader can be configured to read only committed transactions. This prevents events within incomplete transactions from being read, providing the reader isolation from operational writer failures. Keep in mind that idempotence means that the reader's business logic must be able to process the same consumed event multiple times, where multiple reads have the same effect as a single read of the event. For example, if the reader manages a counter (i.e., a state) for the events it has read, then reading the same event multiple times should only increment the counter once. Duplicates caused by incorrect application logic from upstream sources are best resolved by fixing the application's logic (i.e., fixing the root cause). In cases where this is not possible, such as when events are generated outside of our control, the next best option is to embed a tracking ID into the event. A tracking ID should be a field that is unique to the logical event, such as an event key or request ID. The consumer can then read the tracking ID, cross-reference it against an internal state store of IDs it has already processed, and discard the event if necessary. Implementation To configure an Idempotent Reader to read only committed transactions, set the following parameter: isolation.level=\"read_committed\" In your Kafka Streams application, to handle operational failures, you can enable EOS . Within a single transaction, a Kafka Streams application using EOS will atomically update its consumer offsets, its state stores including their changelog topics, its repartition topics, and its output topics. To enable EOS, configure your application with the following parameter: processing.guarantee=\"exactly_once\" In the streaming database ksqlDB , you can also enable EOS by configuring the same parameter as shown above. To handle incorrect application logic, again, first try to eliminate the source of duplication from the code. If that is not an option, you can assign a tracking ID to each event based off of the contents of the event, enabling consumers to detect duplicates for themselves. This requires that each consumer application maintain an internal state store for tracking the events' unique IDs, and this store will vary in size depending on the event count and the period for which the consumer must guard against duplicates. This option requires both additional disk usage and processing power for inserting and validating events. For a subset of business cases, it may also be possible to design the consumer processing logic to be idempotent. For example, in a simple ETL where a database is the Event Sink , the duplicate events can be deduplicated by the database during an upsert on the event ID as primary key. Idempotent business logic also enables your application to read from Event Streams that aren't strictly free of duplicates, or from historic streams that may not have had EOS available when they were created. Considerations A solution that requires EOS guarantees must enable EOS at all stages of the pipeline, not just on the reader. An Idempotent Reader is therefore typically combined with an Idempotent Writer and transactional processing. References This pattern is derived from Idempotent Receiver in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. Blog post about exactly-once semantics in Apache Kafka Tutorial on How to maintain message ordering and no message duplication","title":"Idempotent Reader"},{"location":"event-processing/idempotent-reader/#idempotent-reader","text":"In ideal circumstances, Events are only written once into an Event Stream . Under normal operations, all consumers of the stream will also only read and process each event once. However, depending on the behavior and configuration of the Event Source , there may be failures that create duplicate events. When this happens, we need a strategy for dealing with the duplicates. An idempotent reader must take two causes of duplicate events into consideration: Operational Failures : Intermittent network and system failures are unavoidable in distributed systems. In the case of a machine failure or a brief network outage, an Event Source may produce the same event multiple times due to retries. Similarly, an Event Sink may consume and process the same event multiple times due to intermittent offset updating failures. The Event Streaming Platform should automatically guard against these operational failures by providing strong delivery and processing guarantees, such as those found in Apache Kafka\u00ae transactions. Incorrect Application Logic : An Event Source could mistakenly produce the same event multiple times, populating the Event Stream with multiple distinct events from the perspective of the Event Streaming Platform . For example, imagine a bug in the Event Source that writes a customer payment event twice, instead of just once. The Event Streaming Platform knows nothing of the business logic, so it cannot differentiate between the two events and instead considers them as two distinct payment events.","title":"Idempotent Reader"},{"location":"event-processing/idempotent-reader/#problem","text":"How can an application deal with duplicate Events when reading from an Event Stream?","title":"Problem"},{"location":"event-processing/idempotent-reader/#solution","text":"This can be addressed using exactly-once semantics (EOS), including native support for transactions and support for idempotent clients. EOS allows Event Streaming Applications to process data without loss or duplication, ensuring that computed results are always accurate. Idempotent Writing by the Event Source is the first step in solving this problem. Idempotent Writing provides strong, exactly-once delivery guarantees of the producer's Events, and removes operational failures as a cause of written duplicate Events. On the reading side, in Event Processors and Event Sinks , an Idempotent Reader can be configured to read only committed transactions. This prevents events within incomplete transactions from being read, providing the reader isolation from operational writer failures. Keep in mind that idempotence means that the reader's business logic must be able to process the same consumed event multiple times, where multiple reads have the same effect as a single read of the event. For example, if the reader manages a counter (i.e., a state) for the events it has read, then reading the same event multiple times should only increment the counter once. Duplicates caused by incorrect application logic from upstream sources are best resolved by fixing the application's logic (i.e., fixing the root cause). In cases where this is not possible, such as when events are generated outside of our control, the next best option is to embed a tracking ID into the event. A tracking ID should be a field that is unique to the logical event, such as an event key or request ID. The consumer can then read the tracking ID, cross-reference it against an internal state store of IDs it has already processed, and discard the event if necessary.","title":"Solution"},{"location":"event-processing/idempotent-reader/#implementation","text":"To configure an Idempotent Reader to read only committed transactions, set the following parameter: isolation.level=\"read_committed\" In your Kafka Streams application, to handle operational failures, you can enable EOS . Within a single transaction, a Kafka Streams application using EOS will atomically update its consumer offsets, its state stores including their changelog topics, its repartition topics, and its output topics. To enable EOS, configure your application with the following parameter: processing.guarantee=\"exactly_once\" In the streaming database ksqlDB , you can also enable EOS by configuring the same parameter as shown above. To handle incorrect application logic, again, first try to eliminate the source of duplication from the code. If that is not an option, you can assign a tracking ID to each event based off of the contents of the event, enabling consumers to detect duplicates for themselves. This requires that each consumer application maintain an internal state store for tracking the events' unique IDs, and this store will vary in size depending on the event count and the period for which the consumer must guard against duplicates. This option requires both additional disk usage and processing power for inserting and validating events. For a subset of business cases, it may also be possible to design the consumer processing logic to be idempotent. For example, in a simple ETL where a database is the Event Sink , the duplicate events can be deduplicated by the database during an upsert on the event ID as primary key. Idempotent business logic also enables your application to read from Event Streams that aren't strictly free of duplicates, or from historic streams that may not have had EOS available when they were created.","title":"Implementation"},{"location":"event-processing/idempotent-reader/#considerations","text":"A solution that requires EOS guarantees must enable EOS at all stages of the pipeline, not just on the reader. An Idempotent Reader is therefore typically combined with an Idempotent Writer and transactional processing.","title":"Considerations"},{"location":"event-processing/idempotent-reader/#references","text":"This pattern is derived from Idempotent Receiver in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. Blog post about exactly-once semantics in Apache Kafka Tutorial on How to maintain message ordering and no message duplication","title":"References"},{"location":"event-processing/idempotent-writer/","text":"Idempotent Writer A writer produces Events that are written into an Event Stream , and under stable conditions, each Event is recorded only once. However, in the case of an operational failure or a brief network outage, an Event Source may need to retry writes. This may result in multiple copies of the same Event ending up in the Event Stream, as the first write may have actually succeeded even though the producer did not receive the acknowledgement from the Event Streaming Platform . This type of duplication is a common failure scenario in practice and one of the perils of distributed systems. Problem How can an Event Streaming Platform ensure that an Event Source does not write the same Event more than once? Solution Generally speaking, this can be addressed by native support for idempotent clients. This means that a writer may try to produce an Event more than once, but the Event Streaming Platform detects and discards duplicate write attempts for the same Event. Implementation To make an Apache Kafka\u00ae producer idempotent, configure your producer with the following setting: enable.idempotence=true The Kafka producer tags each batch of Events that it sends to the Kafka cluster with a sequence number. Brokers in the cluster use this sequence number to enforce deduplication of Events sent from this specific producer. Each batch's sequence number is persisted so that even if the leader broker fails, the new leader broker will also know if a given batch is a duplicate. To enable exactly-once processing guarantees in ksqlDB or Kafka Streams, configure the application with the following setting, which includes enabling idempotence in the embedded producer: processing.guarantee=exactly_once Considerations Enabling idempotency for a Kafka producer not only ensures that duplicate Events are fenced out from the topic, it also ensures that they are written in order. This is because the brokers accept a batch of Events only if its sequence number is exactly one greater than that of the last committed batch; otherwise, it results in an out-of-sequence error. Exactly-once semantics (EOS) allow Event Streaming Applications to process data without loss or duplication. This ensures that computed results are always consistent and accurate, even for stateful computations such as joins, aggregations , and windowing . Any solution that requires EOS guarantees must enable EOS at all stages of the pipeline, not just on the writer. An Idempotent Writer is therefore typically combined with an Idempotent Reader and transactional processing. References Blog post about exactly-once semantics in Apache Kafka Tutorial on How to maintain message ordering and no message duplication","title":"Idempotent Writer"},{"location":"event-processing/idempotent-writer/#idempotent-writer","text":"A writer produces Events that are written into an Event Stream , and under stable conditions, each Event is recorded only once. However, in the case of an operational failure or a brief network outage, an Event Source may need to retry writes. This may result in multiple copies of the same Event ending up in the Event Stream, as the first write may have actually succeeded even though the producer did not receive the acknowledgement from the Event Streaming Platform . This type of duplication is a common failure scenario in practice and one of the perils of distributed systems.","title":"Idempotent Writer"},{"location":"event-processing/idempotent-writer/#problem","text":"How can an Event Streaming Platform ensure that an Event Source does not write the same Event more than once?","title":"Problem"},{"location":"event-processing/idempotent-writer/#solution","text":"Generally speaking, this can be addressed by native support for idempotent clients. This means that a writer may try to produce an Event more than once, but the Event Streaming Platform detects and discards duplicate write attempts for the same Event.","title":"Solution"},{"location":"event-processing/idempotent-writer/#implementation","text":"To make an Apache Kafka\u00ae producer idempotent, configure your producer with the following setting: enable.idempotence=true The Kafka producer tags each batch of Events that it sends to the Kafka cluster with a sequence number. Brokers in the cluster use this sequence number to enforce deduplication of Events sent from this specific producer. Each batch's sequence number is persisted so that even if the leader broker fails, the new leader broker will also know if a given batch is a duplicate. To enable exactly-once processing guarantees in ksqlDB or Kafka Streams, configure the application with the following setting, which includes enabling idempotence in the embedded producer: processing.guarantee=exactly_once","title":"Implementation"},{"location":"event-processing/idempotent-writer/#considerations","text":"Enabling idempotency for a Kafka producer not only ensures that duplicate Events are fenced out from the topic, it also ensures that they are written in order. This is because the brokers accept a batch of Events only if its sequence number is exactly one greater than that of the last committed batch; otherwise, it results in an out-of-sequence error. Exactly-once semantics (EOS) allow Event Streaming Applications to process data without loss or duplication. This ensures that computed results are always consistent and accurate, even for stateful computations such as joins, aggregations , and windowing . Any solution that requires EOS guarantees must enable EOS at all stages of the pipeline, not just on the writer. An Idempotent Writer is therefore typically combined with an Idempotent Reader and transactional processing.","title":"Considerations"},{"location":"event-processing/idempotent-writer/#references","text":"Blog post about exactly-once semantics in Apache Kafka Tutorial on How to maintain message ordering and no message duplication","title":"References"},{"location":"event-sink/event-sink-connector/","text":"Event Sink Connector Connecting external systems to the Event Streaming Platform allows for advanced and specialized integrations. Problem How can we connect applications or external systems, like databases, to an Event Streaming Platform so that it can receive Events ? Solution Event Sink Connector is a specific implementation of an Event Sink . Use an Event Sink Connector to transfer Events from the Event Stream into the specific external system. Implementation CREATE SINK CONNECTOR JDBC_SINK_POSTGRES_01 WITH ( 'connector.class' = 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url' = 'jdbc:postgresql://postgres:5432/', 'connection.user' = 'postgres', 'connection.password' = 'postgres', 'topics' = 'TEMPERATURE_READINGS_TIMESTAMP_MT', 'auto.create' = 'true', 'auto.evolve' = 'true' ); When connecting a system like a relational database to Apache Kafka\u00ae, the most common option is to use Kafka Connect . The connector reads events from the Event Streaming Platform , performs any necessary transformations, and writes the Events to the specified Event Sink . Considerations There are many Event Sink Connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event sink, event sink connector, and the event streaming platform. References For an example of using Kafka Connect as an Event Sink Connector, see Timezone conversion and Kafka Connect JDBC sink with ksqlDB . This pattern is derived from Channel Adapter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Sink Connector"},{"location":"event-sink/event-sink-connector/#event-sink-connector","text":"Connecting external systems to the Event Streaming Platform allows for advanced and specialized integrations.","title":"Event Sink Connector"},{"location":"event-sink/event-sink-connector/#problem","text":"How can we connect applications or external systems, like databases, to an Event Streaming Platform so that it can receive Events ?","title":"Problem"},{"location":"event-sink/event-sink-connector/#solution","text":"Event Sink Connector is a specific implementation of an Event Sink . Use an Event Sink Connector to transfer Events from the Event Stream into the specific external system.","title":"Solution"},{"location":"event-sink/event-sink-connector/#implementation","text":"CREATE SINK CONNECTOR JDBC_SINK_POSTGRES_01 WITH ( 'connector.class' = 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url' = 'jdbc:postgresql://postgres:5432/', 'connection.user' = 'postgres', 'connection.password' = 'postgres', 'topics' = 'TEMPERATURE_READINGS_TIMESTAMP_MT', 'auto.create' = 'true', 'auto.evolve' = 'true' ); When connecting a system like a relational database to Apache Kafka\u00ae, the most common option is to use Kafka Connect . The connector reads events from the Event Streaming Platform , performs any necessary transformations, and writes the Events to the specified Event Sink .","title":"Implementation"},{"location":"event-sink/event-sink-connector/#considerations","text":"There are many Event Sink Connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event sink, event sink connector, and the event streaming platform.","title":"Considerations"},{"location":"event-sink/event-sink-connector/#references","text":"For an example of using Kafka Connect as an Event Sink Connector, see Timezone conversion and Kafka Connect JDBC sink with ksqlDB . This pattern is derived from Channel Adapter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-sink/event-sink/","text":"Event Sink Various components in an Event Streaming Platform will read or receive Events . An Event Sink is the generalization of these components, which can include Event Processing Applications , cloud services, databases, IoT sensors, mainframes, and more. Conceptually, an Event Sink is the opposite of an Event Source . In practice, however, components such as an Event Processing Application can act as both an Event Source and an Event Sink. Problem How can we read (or consume / subscribe to) Events in an Event Streaming Platform ? Solution Use an Event Sink, which typically acts as a client in an Event Streaming Platform . Examples are an Event Sink Connector (which continuously exports Event Streams from the Event Streaming Platform into an external system such as a cloud service or a relational database) or an Event Processing Application such as a Kafka Streams application and the streaming database ksqlDB . Implementation ksqlDB example: Reading events from an existing Apache Kafka\u00ae topic into a ksqlDB event stream for further processing. CREATE STREAM clicks (ip_address VARCHAR, url VARCHAR, timestamp VARCHAR) WITH (KAFKA_TOPIC = 'clicks-topic', VALUE_FORMAT = 'json', TIMESTAMP = 'timestamp', TIMESTAMP_FORMAT = 'yyyy-MM-dd''T''HH:mm:ssXXX'); Generic Kafka Consumer application: See Getting Started with Apache Kafka and Java for a full example: consumer.subscribe(Collections.singletonList(\"stream\")); while (keepConsuming) { final ConsumerRecords<String, EventRecord> consumerRecords = consumer.poll(Duration.ofSeconds(1)); recordsHandler.process(consumerRecords); } References The Kafka Streams library of Apache Kafka is another popular choice of developers to implement elastic applications and microservices that read, process, and write events. See Filter a stream of events for a first example.","title":"Event Sink"},{"location":"event-sink/event-sink/#event-sink","text":"Various components in an Event Streaming Platform will read or receive Events . An Event Sink is the generalization of these components, which can include Event Processing Applications , cloud services, databases, IoT sensors, mainframes, and more. Conceptually, an Event Sink is the opposite of an Event Source . In practice, however, components such as an Event Processing Application can act as both an Event Source and an Event Sink.","title":"Event Sink"},{"location":"event-sink/event-sink/#problem","text":"How can we read (or consume / subscribe to) Events in an Event Streaming Platform ?","title":"Problem"},{"location":"event-sink/event-sink/#solution","text":"Use an Event Sink, which typically acts as a client in an Event Streaming Platform . Examples are an Event Sink Connector (which continuously exports Event Streams from the Event Streaming Platform into an external system such as a cloud service or a relational database) or an Event Processing Application such as a Kafka Streams application and the streaming database ksqlDB .","title":"Solution"},{"location":"event-sink/event-sink/#implementation","text":"ksqlDB example: Reading events from an existing Apache Kafka\u00ae topic into a ksqlDB event stream for further processing. CREATE STREAM clicks (ip_address VARCHAR, url VARCHAR, timestamp VARCHAR) WITH (KAFKA_TOPIC = 'clicks-topic', VALUE_FORMAT = 'json', TIMESTAMP = 'timestamp', TIMESTAMP_FORMAT = 'yyyy-MM-dd''T''HH:mm:ssXXX'); Generic Kafka Consumer application: See Getting Started with Apache Kafka and Java for a full example: consumer.subscribe(Collections.singletonList(\"stream\")); while (keepConsuming) { final ConsumerRecords<String, EventRecord> consumerRecords = consumer.poll(Duration.ofSeconds(1)); recordsHandler.process(consumerRecords); }","title":"Implementation"},{"location":"event-sink/event-sink/#references","text":"The Kafka Streams library of Apache Kafka is another popular choice of developers to implement elastic applications and microservices that read, process, and write events. See Filter a stream of events for a first example.","title":"References"},{"location":"event-source/database-write-aside/","text":"Database Write Aside Applications which write directly to a database may want to produce an associated Event to the Event Streaming Platform for each write operation allowing downstream Event Processing Applications to be notified and consume the Event . Problem How do I update a value in a database and create an associated event with the least amount of effort? Solution Write to a database, then write to Kafka. Perform the write to Kafka as the last step in a database transaction to ensure an atomic dual commit (aborting the transaction if the write to Kafka fails). Implementation //Enable transactions db.setAutoCommit(false); try{ //insert into the DB sql = db.prepareStatement(\"insert into mydb.events values (?)\"); sql.setString(event.toString()); sql.executeUpdate(); //insert into Kafka producer.send(event.key(), event.value()); //commit to the DB db.commit(); } catch (SQLException e ) { db.rollback(); } Considerations In its default form, this pattern guarantees dual-write for most use cases. However, should the database transaction fail at commit time (say, because the database server has crashed) the write to Kafka cannot be rolled back unless transactions have been enabled. For many use cases, this eventuality will be tolerable as the dual-write can be retried once the failure is fixed, and most event consumers will implement idempotence anyway. However, application programmers need to be aware that there is no firm guarantee. Transactional messaging systems like Kafka can be used to provide stronger guarantees so long as all event consumers have the transactions feature enabled. References See Database Write Through for an alternative example of writing database changes to an Event Stream","title":"Database Write Aside"},{"location":"event-source/database-write-aside/#database-write-aside","text":"Applications which write directly to a database may want to produce an associated Event to the Event Streaming Platform for each write operation allowing downstream Event Processing Applications to be notified and consume the Event .","title":"Database Write Aside"},{"location":"event-source/database-write-aside/#problem","text":"How do I update a value in a database and create an associated event with the least amount of effort?","title":"Problem"},{"location":"event-source/database-write-aside/#solution","text":"Write to a database, then write to Kafka. Perform the write to Kafka as the last step in a database transaction to ensure an atomic dual commit (aborting the transaction if the write to Kafka fails).","title":"Solution"},{"location":"event-source/database-write-aside/#implementation","text":"//Enable transactions db.setAutoCommit(false); try{ //insert into the DB sql = db.prepareStatement(\"insert into mydb.events values (?)\"); sql.setString(event.toString()); sql.executeUpdate(); //insert into Kafka producer.send(event.key(), event.value()); //commit to the DB db.commit(); } catch (SQLException e ) { db.rollback(); }","title":"Implementation"},{"location":"event-source/database-write-aside/#considerations","text":"In its default form, this pattern guarantees dual-write for most use cases. However, should the database transaction fail at commit time (say, because the database server has crashed) the write to Kafka cannot be rolled back unless transactions have been enabled. For many use cases, this eventuality will be tolerable as the dual-write can be retried once the failure is fixed, and most event consumers will implement idempotence anyway. However, application programmers need to be aware that there is no firm guarantee. Transactional messaging systems like Kafka can be used to provide stronger guarantees so long as all event consumers have the transactions feature enabled.","title":"Considerations"},{"location":"event-source/database-write-aside/#references","text":"See Database Write Through for an alternative example of writing database changes to an Event Stream","title":"References"},{"location":"event-source/database-write-through/","text":"Database Write Through For architectural or legacy purposes, data-centric applications may write directly to a database. Event Processing Applications will need to reliably consume data from these systems using Events on Event Streams . Problem How do we update a value in a database and create an associated Event with at-least-once guarantees? Solution Some applications write directly to a database table, therefore the database table is the Event Source . We can deploy a Change Data Capture (CDC) solution to continuously capture writes (inserts, updates, deletes) to that database table and produce them as Events onto an Event Stream . The events in stream can then be consumed by Event Processing Applications . Additionally, the Event Stream can be read into a Projection Table, for example with ksqlDB , so that it can be queried by other applications. Implementation Kafka connectors provide the ability to scalably and reliably move data between Apache Kafka\u00ae and other data systems. There are many types of connectors that can connect to many types of databases. For example, using the JDBC Source connector for Kafka Connect allows for streaming of changes from a source database to Kafka topics. { \"connector.class\": \"MySqlCdcSource\", \"name\": \"MySqlCdcSourceConnector_0\", \"kafka.api.key\": \"****************\", \"kafka.api.secret\": \"******************************\", \"database.hostname\": \"database-2.<host-ID>.us-west-2.rds.amazonaws.com\", \"database.port\": \"3306\", \"database.user\": \"admin\", \"database.password\": \"**********\", \"database.server.name\": \"mysql\", \"database.whitelist\": \"employee\", \"table.includelist\":\"employees.departments\", \"snapshot.mode\": \"initial\", \"output.data.format\": \"AVRO\", \"tasks.max\": \"1\" } The above shows an example configuration to deploy a MySQL Source CDC Connector (Debezium) streaming data from a MySQL database to Kafka topics. The configuration determines the database to connect to ( database.hostname ) along with the database elements to source into Kafka ( table.includelist ). The output.data.format configuration instructs Kafka Connect which format to use when writing records (in this case the Apache Avro format). Confluent Cloud CLI can deploy a connector on the command line from a configuration, for example: ccloud connector create --config <connector-config-file.json> Considerations This pattern is a specialization of the Event Source Connector that guarantees that all state changes represented in an Event Source are captured in an Event Streaming Platform . The processing guarantees (cf. \"Guaranteed Delivery\") to choose from \u2014 e.g., at-least-once, exactly-once \u2014 for the CDC data flow depend on the CDC and Database technology utilized. There is a certain delay until changes in the source database table are available in the CDC-ingested event stream. The amount of the delay depends on a variety of factors, including the features and configuration of the Event Source Connector. In many typical scenarios the delay is less than a few seconds. Events typically require the row key to be used as the Kafka event key (aka record/message key), which is the only way to ensure all Events for the same DB table row go to the same Kafka topic-partition and are thus totally ordered. They also typically model deletes as tombstone events, i.e. an event with a non-null key and a null value. By ensuring totally ordered events for each row, consumers see an eventually-consistent representation of these events for each row. References See Oracle CDC Source Connector for details of a premium CDC connector for Oracle DB. See Integrate External Systems to Kafka on Confluent documentation for information on source connectors. Kafka Connect Deep Dive - JDBC Source Connector blog post See Database Write Aside for an alternative example of writing database changes to an Event Stream No More Silos: Integrating Databases and Apache Kafka blog post by Robin Moffatt","title":"Database Write Through"},{"location":"event-source/database-write-through/#database-write-through","text":"For architectural or legacy purposes, data-centric applications may write directly to a database. Event Processing Applications will need to reliably consume data from these systems using Events on Event Streams .","title":"Database Write Through"},{"location":"event-source/database-write-through/#problem","text":"How do we update a value in a database and create an associated Event with at-least-once guarantees?","title":"Problem"},{"location":"event-source/database-write-through/#solution","text":"Some applications write directly to a database table, therefore the database table is the Event Source . We can deploy a Change Data Capture (CDC) solution to continuously capture writes (inserts, updates, deletes) to that database table and produce them as Events onto an Event Stream . The events in stream can then be consumed by Event Processing Applications . Additionally, the Event Stream can be read into a Projection Table, for example with ksqlDB , so that it can be queried by other applications.","title":"Solution"},{"location":"event-source/database-write-through/#implementation","text":"Kafka connectors provide the ability to scalably and reliably move data between Apache Kafka\u00ae and other data systems. There are many types of connectors that can connect to many types of databases. For example, using the JDBC Source connector for Kafka Connect allows for streaming of changes from a source database to Kafka topics. { \"connector.class\": \"MySqlCdcSource\", \"name\": \"MySqlCdcSourceConnector_0\", \"kafka.api.key\": \"****************\", \"kafka.api.secret\": \"******************************\", \"database.hostname\": \"database-2.<host-ID>.us-west-2.rds.amazonaws.com\", \"database.port\": \"3306\", \"database.user\": \"admin\", \"database.password\": \"**********\", \"database.server.name\": \"mysql\", \"database.whitelist\": \"employee\", \"table.includelist\":\"employees.departments\", \"snapshot.mode\": \"initial\", \"output.data.format\": \"AVRO\", \"tasks.max\": \"1\" } The above shows an example configuration to deploy a MySQL Source CDC Connector (Debezium) streaming data from a MySQL database to Kafka topics. The configuration determines the database to connect to ( database.hostname ) along with the database elements to source into Kafka ( table.includelist ). The output.data.format configuration instructs Kafka Connect which format to use when writing records (in this case the Apache Avro format). Confluent Cloud CLI can deploy a connector on the command line from a configuration, for example: ccloud connector create --config <connector-config-file.json>","title":"Implementation"},{"location":"event-source/database-write-through/#considerations","text":"This pattern is a specialization of the Event Source Connector that guarantees that all state changes represented in an Event Source are captured in an Event Streaming Platform . The processing guarantees (cf. \"Guaranteed Delivery\") to choose from \u2014 e.g., at-least-once, exactly-once \u2014 for the CDC data flow depend on the CDC and Database technology utilized. There is a certain delay until changes in the source database table are available in the CDC-ingested event stream. The amount of the delay depends on a variety of factors, including the features and configuration of the Event Source Connector. In many typical scenarios the delay is less than a few seconds. Events typically require the row key to be used as the Kafka event key (aka record/message key), which is the only way to ensure all Events for the same DB table row go to the same Kafka topic-partition and are thus totally ordered. They also typically model deletes as tombstone events, i.e. an event with a non-null key and a null value. By ensuring totally ordered events for each row, consumers see an eventually-consistent representation of these events for each row.","title":"Considerations"},{"location":"event-source/database-write-through/#references","text":"See Oracle CDC Source Connector for details of a premium CDC connector for Oracle DB. See Integrate External Systems to Kafka on Confluent documentation for information on source connectors. Kafka Connect Deep Dive - JDBC Source Connector blog post See Database Write Aside for an alternative example of writing database changes to an Event Stream No More Silos: Integrating Databases and Apache Kafka blog post by Robin Moffatt","title":"References"},{"location":"event-source/event-gateway/","text":"Event Gateway One of the key benefits of adopting an Event -first architecture is to foster collaboration. Our aim is to ensure that Team A can produce data, Team B can process it and Team C can report on it, with only one thing coupling the teams together - the data itself. Teams shouldn't have to agree on shared libraries, synchronized release schedules, or common tooling. Data becomes the one true interface. In reality though, each team will still have to communicate with the Event Store itself. How do we maximize access? How do we ensure that every team can use the event store, without insisting they choose from a shortlist of supported languages? How do we accommodate the team that insists on using Idris + ? + Or Haskell, or Rust, or Erlang, or whatever other language we didn't plan for... Problem How does an an Event Streaming Platform provide access to the widest-possible range of users? Solution Provide an event gateway via a standardized, well-supported interface that gives access to the widest possible range of users. Implementation Confluent provides a broad set of REST APIs that allow any language or CLI to access the event store using HTTP(S). Further, it provides support to produce and consume Apache Kafka\u00ae data, formatted as JSON, Protobuf, Avro or even raw base64-encoded bytes. As a simple example, we can post JSON-encoded events to a topic called sales using curl : curl -X POST \\ -H \"Content-Type: application/vnd.kafka.json.v2+json\" \\ --data '{\"records\":[{\"key\":\"alice\",\"value\":{\"tickets\":5}},{\"key\":\"bob\",\"value\":{\"tickets\":10}}]}' \\ http://localhost:8082/topics/sales { \"offsets\": [ { \"partition\": 0, \"offset\": 0, \"error_code\": null, \"error\": null }, { \"partition\": 0, \"offset\": 1, \"error_code\": null, \"error\": null } ], \"key_schema_id\": null, \"value_schema_id\": null } Considerations In a perfect world, every Event Streaming Platform (and every relational database) would have first-class support for every language. Realistically some languages will be better accommodated than others, but we can still ensure every language has access to every important feature through a standards-based interface. References The Confluent REST APIs documentation","title":"Event Gateway"},{"location":"event-source/event-gateway/#event-gateway","text":"One of the key benefits of adopting an Event -first architecture is to foster collaboration. Our aim is to ensure that Team A can produce data, Team B can process it and Team C can report on it, with only one thing coupling the teams together - the data itself. Teams shouldn't have to agree on shared libraries, synchronized release schedules, or common tooling. Data becomes the one true interface. In reality though, each team will still have to communicate with the Event Store itself. How do we maximize access? How do we ensure that every team can use the event store, without insisting they choose from a shortlist of supported languages? How do we accommodate the team that insists on using Idris + ? + Or Haskell, or Rust, or Erlang, or whatever other language we didn't plan for...","title":"Event Gateway"},{"location":"event-source/event-gateway/#problem","text":"How does an an Event Streaming Platform provide access to the widest-possible range of users?","title":"Problem"},{"location":"event-source/event-gateway/#solution","text":"Provide an event gateway via a standardized, well-supported interface that gives access to the widest possible range of users.","title":"Solution"},{"location":"event-source/event-gateway/#implementation","text":"Confluent provides a broad set of REST APIs that allow any language or CLI to access the event store using HTTP(S). Further, it provides support to produce and consume Apache Kafka\u00ae data, formatted as JSON, Protobuf, Avro or even raw base64-encoded bytes. As a simple example, we can post JSON-encoded events to a topic called sales using curl : curl -X POST \\ -H \"Content-Type: application/vnd.kafka.json.v2+json\" \\ --data '{\"records\":[{\"key\":\"alice\",\"value\":{\"tickets\":5}},{\"key\":\"bob\",\"value\":{\"tickets\":10}}]}' \\ http://localhost:8082/topics/sales { \"offsets\": [ { \"partition\": 0, \"offset\": 0, \"error_code\": null, \"error\": null }, { \"partition\": 0, \"offset\": 1, \"error_code\": null, \"error\": null } ], \"key_schema_id\": null, \"value_schema_id\": null }","title":"Implementation"},{"location":"event-source/event-gateway/#considerations","text":"In a perfect world, every Event Streaming Platform (and every relational database) would have first-class support for every language. Realistically some languages will be better accommodated than others, but we can still ensure every language has access to every important feature through a standards-based interface.","title":"Considerations"},{"location":"event-source/event-gateway/#references","text":"The Confluent REST APIs documentation","title":"References"},{"location":"event-source/event-source-connector/","text":"Event Source Connector Event Processing Applications may want to consume data from existing data systems, which are not themselves Event Sources . Problem How can we connect cloud services and traditional systems, like relational databases, to an Event Streaming Platform , converting their data at rest to data in motion with Events ? Solution Generally speaking, we need to find a way to extract data as Events from the origin system. For relational databases, for example, a common technique is to use Change Data Capture , where changes to database tables\u2014such as INSERTs, UPDATES, DELETEs\u2014are captured as Events , which can then be ingested into another system. The components that perform this extraction and ingestion of Events are typically called \"connectors\". The connectors turn the origin system into an Event Source , then generate Events from that data, and finally sends these Events to the Event Streaming Platform . Implementation When connecting a cloud services and traditional systems to Apache Kafka\u00ae , the most common solution is to use Kafka Connect . There are hundreds of ready-to-use connectors available on Confluent Hub , including blob stores like AWS S3, cloud services like Salesforce and Snowflake, relational databases, data warehouses, traditional message queues, flat files, and more. Confluent also provides many fully managed Kafka connectors in the cloud. There are several options to deploy such connectors. For example, the streaming database ksqlDB provides an ability to manage Kafka connectors with SQL statements. CREATE SOURCE CONNECTOR `jdbc-connector` WITH( \"connector.class\"='io.confluent.connect.jdbc.JdbcSourceConnector', \"connection.url\"='jdbc:postgresql://localhost:5432/my.db', \"mode\"='bulk', \"topic.prefix\"='jdbc-', \"table.whitelist\"='users', \"key\"='username'); Considerations End-to-end data delivery guarantees (such as exactly-once delivery or at-least-once delivery) depend primarily on three factors: (1) the capabilities of the origin Event Source, such as a cloud service or relational database; (2) the capabilities of the Event Source Connector, and (3) the capabilities of the destination Event Streaming Platform, such as Apache Kafka or Confluent. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication, and authorization, etc. between Event Source, Event Source Connector, and the destination Event Streaming Platform. References This pattern is derived from Channel Adapter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full Kafka Connect example","title":"Event Source Connector"},{"location":"event-source/event-source-connector/#event-source-connector","text":"Event Processing Applications may want to consume data from existing data systems, which are not themselves Event Sources .","title":"Event Source Connector"},{"location":"event-source/event-source-connector/#problem","text":"How can we connect cloud services and traditional systems, like relational databases, to an Event Streaming Platform , converting their data at rest to data in motion with Events ?","title":"Problem"},{"location":"event-source/event-source-connector/#solution","text":"Generally speaking, we need to find a way to extract data as Events from the origin system. For relational databases, for example, a common technique is to use Change Data Capture , where changes to database tables\u2014such as INSERTs, UPDATES, DELETEs\u2014are captured as Events , which can then be ingested into another system. The components that perform this extraction and ingestion of Events are typically called \"connectors\". The connectors turn the origin system into an Event Source , then generate Events from that data, and finally sends these Events to the Event Streaming Platform .","title":"Solution"},{"location":"event-source/event-source-connector/#implementation","text":"When connecting a cloud services and traditional systems to Apache Kafka\u00ae , the most common solution is to use Kafka Connect . There are hundreds of ready-to-use connectors available on Confluent Hub , including blob stores like AWS S3, cloud services like Salesforce and Snowflake, relational databases, data warehouses, traditional message queues, flat files, and more. Confluent also provides many fully managed Kafka connectors in the cloud. There are several options to deploy such connectors. For example, the streaming database ksqlDB provides an ability to manage Kafka connectors with SQL statements. CREATE SOURCE CONNECTOR `jdbc-connector` WITH( \"connector.class\"='io.confluent.connect.jdbc.JdbcSourceConnector', \"connection.url\"='jdbc:postgresql://localhost:5432/my.db', \"mode\"='bulk', \"topic.prefix\"='jdbc-', \"table.whitelist\"='users', \"key\"='username');","title":"Implementation"},{"location":"event-source/event-source-connector/#considerations","text":"End-to-end data delivery guarantees (such as exactly-once delivery or at-least-once delivery) depend primarily on three factors: (1) the capabilities of the origin Event Source, such as a cloud service or relational database; (2) the capabilities of the Event Source Connector, and (3) the capabilities of the destination Event Streaming Platform, such as Apache Kafka or Confluent. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication, and authorization, etc. between Event Source, Event Source Connector, and the destination Event Streaming Platform.","title":"Considerations"},{"location":"event-source/event-source-connector/#references","text":"This pattern is derived from Channel Adapter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full Kafka Connect example","title":"References"},{"location":"event-source/event-source/","text":"Event Source Various components in an Event Streaming Platform will generate Events . An Event Source is the generalization of these components, which can include Event Processing Applications , cloud services, databases, IoT sensors, mainframes, and more. Conceptually, an event source is the opposite of an Event Sink . In practice, however, components such as an event processing application can act as both an event source and an event sink. Problem How can we create Events in an Event Streaming Platform ? Solution Use an Event Source, which typically acts as a client in an Event Streaming Platform . Examples are an Event Source Connector (which continuously imports data as Event Streams into the Event Streaming Platform from an external system such as a cloud service or a relational database), or an Event Processing Application , such as a Kafka Streams application or the streaming database ksqlDB . Implementation Normally, an actual component such as an application for Apache Kafka\u00ae would be writing Events into an Event Stream via a client library, API, gateway, etc. We can also write events directly using SQL syntax; the streaming database ksqlDB , for example, provides an INSERT statement. CREATE STREAM users (username VARCHAR, name VARCHAR, phone VARCHAR) with (kafka_topic='users-topic', value_format='json'); INSERT INTO users (username, name, phone) VALUES ('awilson', 'Allison', '+1 555-555-1234'); References ksqlDB The event streaming database, purpose-built for stream processing applications. How to build client applications for writing events into an Event Stream .","title":"Event Source"},{"location":"event-source/event-source/#event-source","text":"Various components in an Event Streaming Platform will generate Events . An Event Source is the generalization of these components, which can include Event Processing Applications , cloud services, databases, IoT sensors, mainframes, and more. Conceptually, an event source is the opposite of an Event Sink . In practice, however, components such as an event processing application can act as both an event source and an event sink.","title":"Event Source"},{"location":"event-source/event-source/#problem","text":"How can we create Events in an Event Streaming Platform ?","title":"Problem"},{"location":"event-source/event-source/#solution","text":"Use an Event Source, which typically acts as a client in an Event Streaming Platform . Examples are an Event Source Connector (which continuously imports data as Event Streams into the Event Streaming Platform from an external system such as a cloud service or a relational database), or an Event Processing Application , such as a Kafka Streams application or the streaming database ksqlDB .","title":"Solution"},{"location":"event-source/event-source/#implementation","text":"Normally, an actual component such as an application for Apache Kafka\u00ae would be writing Events into an Event Stream via a client library, API, gateway, etc. We can also write events directly using SQL syntax; the streaming database ksqlDB , for example, provides an INSERT statement. CREATE STREAM users (username VARCHAR, name VARCHAR, phone VARCHAR) with (kafka_topic='users-topic', value_format='json'); INSERT INTO users (username, name, phone) VALUES ('awilson', 'Allison', '+1 555-555-1234');","title":"Implementation"},{"location":"event-source/event-source/#references","text":"ksqlDB The event streaming database, purpose-built for stream processing applications. How to build client applications for writing events into an Event Stream .","title":"References"},{"location":"event-source/schema-validator/","text":"Schema Validator In an Event Streaming Platform , Event Sources , which create and write Events , are decoupled from Event Sinks and Event Processing Applications , which read and process the events. Ensuring interoperability between the producers and the consumers of events requires that they agree on the data schemas for the events. This is an important aspect of putting Data Contracts in place for data governance. Problem How can I enforce a defined schema for all Events sent to an Event Stream ? Solution Validate that an Event conforms to the defined schema(s) of an Event Stream prior to writing the event to the stream. You can perform this schema validation in two ways: On the server side, the Event Streaming Platform that receives the Event can validate the event. The Event Streaming Platform can reject the event if it fails schema validation and thus violates the Data Contract . On the client side, the Event Source that creates the Event can validate the event. For example, an Event Source Connector can validate events prior to ingesting them into the Event Streaming Platform , or an Event Processing Application can use the schema validation functionality provided by a serialization library that supports schemas (such as the Confluent serializers and deserializers for Kafka). Implementation With Confluent, schema validation is fully supported with a per-environment managed Schema Registry . Use the Confluent Cloud UI to enable Schema Registry in your cloud provider of choice. Schemas can be managed per topic using either the Confluent Cloud UI or the Confluent Cloud CLI . The following command creates a schema using the CLI: ccloud schema-registry schema create --subject employees-value --schema employees.json --type AVRO Considerations Schema Validator is a data governance implementation of \"Schema on Write\", which enforces data conformance prior to event publication. An alternative strategy is Schema On Read , where data formats are not enforced on write, but consuming Event Processing Applications are required to validate data formats as they read each event. Server-side schema validation is preferable when you want to enforce this pattern centrally inside an organization. In contrast, client-side validation assumes the cooperation of client applications and their developers, which may or may not be acceptable (for example, in regulated industries). Schema validation results in a load increase, because it impacts the write path of every event. Client-side validation primarily impacts the load of client applications. Server-side schema validation increases the load on the event streaming platform, whereas client applications are less affected (here, the main impact is dealing with rejected events; see Dead Letter Stream ). References See the Schema Compatibility pattern for information on how schemas can evolve over time while continuing to be verified. Learn more about how to Manage and Validate Schemas with Confluent and Kafka .","title":"Schema Validator"},{"location":"event-source/schema-validator/#schema-validator","text":"In an Event Streaming Platform , Event Sources , which create and write Events , are decoupled from Event Sinks and Event Processing Applications , which read and process the events. Ensuring interoperability between the producers and the consumers of events requires that they agree on the data schemas for the events. This is an important aspect of putting Data Contracts in place for data governance.","title":"Schema Validator"},{"location":"event-source/schema-validator/#problem","text":"How can I enforce a defined schema for all Events sent to an Event Stream ?","title":"Problem"},{"location":"event-source/schema-validator/#solution","text":"Validate that an Event conforms to the defined schema(s) of an Event Stream prior to writing the event to the stream. You can perform this schema validation in two ways: On the server side, the Event Streaming Platform that receives the Event can validate the event. The Event Streaming Platform can reject the event if it fails schema validation and thus violates the Data Contract . On the client side, the Event Source that creates the Event can validate the event. For example, an Event Source Connector can validate events prior to ingesting them into the Event Streaming Platform , or an Event Processing Application can use the schema validation functionality provided by a serialization library that supports schemas (such as the Confluent serializers and deserializers for Kafka).","title":"Solution"},{"location":"event-source/schema-validator/#implementation","text":"With Confluent, schema validation is fully supported with a per-environment managed Schema Registry . Use the Confluent Cloud UI to enable Schema Registry in your cloud provider of choice. Schemas can be managed per topic using either the Confluent Cloud UI or the Confluent Cloud CLI . The following command creates a schema using the CLI: ccloud schema-registry schema create --subject employees-value --schema employees.json --type AVRO","title":"Implementation"},{"location":"event-source/schema-validator/#considerations","text":"Schema Validator is a data governance implementation of \"Schema on Write\", which enforces data conformance prior to event publication. An alternative strategy is Schema On Read , where data formats are not enforced on write, but consuming Event Processing Applications are required to validate data formats as they read each event. Server-side schema validation is preferable when you want to enforce this pattern centrally inside an organization. In contrast, client-side validation assumes the cooperation of client applications and their developers, which may or may not be acceptable (for example, in regulated industries). Schema validation results in a load increase, because it impacts the write path of every event. Client-side validation primarily impacts the load of client applications. Server-side schema validation increases the load on the event streaming platform, whereas client applications are less affected (here, the main impact is dealing with rejected events; see Dead Letter Stream ).","title":"Considerations"},{"location":"event-source/schema-validator/#references","text":"See the Schema Compatibility pattern for information on how schemas can evolve over time while continuing to be verified. Learn more about how to Manage and Validate Schemas with Confluent and Kafka .","title":"References"},{"location":"event-storage/compacted-event-stream/","text":"Compacted Event Stream Event Streams often represent keyed snapshots of state, similar to a Table in a relational database. That is, the Events contain a primary key (identifier) and data that represents the latest information of the business entity related to the Event, such as the latest balance per customer account. Event Processing Applications will need to process these Events to determine the current state of the business entity. However, processing the entire Event Stream history is often not practical. Problem How can a (keyed) Table be stored in an Event Stream forever, using the minimum amount of space? Solution Remove events from the stream that represent outdated information and have been superseded by new events. The table's current data (i.e., its latest state) is represented by the remaining events in the stream. This approach bounds the storage space of the table's event stream to \u0398(number of unique keys currently in table) , rather than \u0398(total number of change events for table) . In practice, the number of unique keys (e.g., unique customer IDs) is typically much smaller than the number of table changes (e.g., total number of changes across all customer profiles). A compacted event stream thus reduces the storage space significantly in most cases. Implementation Apache Kafka\u00ae provides this functionality natively through its Topic Compaction feature. A stream (topic in Kafka) is scanned periodically to remove any old events that have been superseded by newer events that have the same key, such as as the same customer ID. Note that compaction is an asynchronous process in Kafka, so a compacted stream may contain some superseded events, which are waiting to be compacted away. To create a compacted event stream called customer-profiles with Kafka: $ kafka-topics --create \\ --bootstrap-server <bootstrap-url> \\ --replication-factor 3 \\ --partitions 3 \\ --topic customer-profiles \\ --config cleanup.policy=compact Created topic customer-profiles. The kafka-topics command can also verify the current topic's configuration: $ kafka-topics --bootstrap-server localhost:9092 --topic customer-profiles --describe Topic: customer-profiles PartitionCount: 3 ReplicationFactor: 1 Configs: cleanup.policy=compact,segment.bytes=1073741824 Topic: customer-profiles Partition: 0 Leader: 0 Replicas: 0 Isr: 0 Offline: Topic: customer-profiles Partition: 1 Leader: 0 Replicas: 0 Isr: 0 Offline: Topic: customer-profiles Partition: 2 Leader: 0 Replicas: 0 Isr: 0 Offline: Considerations Compacted event streams allow for some optimizations: First, they allow the Event Streaming Platform to limit the storage growth of the stream in a data-specific way, rather than removing events universally after a pre-configured period of time. Second, having smaller streams allows for faster recovery and system migration strategies. It is important to understand that compaction, on purpose, removes historical data from an event stream by removing superseded events as defined above. In many use cases, however, historical data should not be removed, such as for a stream of financial transactions, where every single transaction needs to be recorded and stored. Here, if the storage of the stream is the primary concern, use an Infinite Retention Event Stream instead of a compacted stream. References Compacted event streams are highly related to the State Table pattern. Compacted event streams work a bit like simple Log Structured Merge Trees . Cleanup policy configuration of Kafka topics.","title":"Compacted Event Stream"},{"location":"event-storage/compacted-event-stream/#compacted-event-stream","text":"Event Streams often represent keyed snapshots of state, similar to a Table in a relational database. That is, the Events contain a primary key (identifier) and data that represents the latest information of the business entity related to the Event, such as the latest balance per customer account. Event Processing Applications will need to process these Events to determine the current state of the business entity. However, processing the entire Event Stream history is often not practical.","title":"Compacted Event Stream"},{"location":"event-storage/compacted-event-stream/#problem","text":"How can a (keyed) Table be stored in an Event Stream forever, using the minimum amount of space?","title":"Problem"},{"location":"event-storage/compacted-event-stream/#solution","text":"Remove events from the stream that represent outdated information and have been superseded by new events. The table's current data (i.e., its latest state) is represented by the remaining events in the stream. This approach bounds the storage space of the table's event stream to \u0398(number of unique keys currently in table) , rather than \u0398(total number of change events for table) . In practice, the number of unique keys (e.g., unique customer IDs) is typically much smaller than the number of table changes (e.g., total number of changes across all customer profiles). A compacted event stream thus reduces the storage space significantly in most cases.","title":"Solution"},{"location":"event-storage/compacted-event-stream/#implementation","text":"Apache Kafka\u00ae provides this functionality natively through its Topic Compaction feature. A stream (topic in Kafka) is scanned periodically to remove any old events that have been superseded by newer events that have the same key, such as as the same customer ID. Note that compaction is an asynchronous process in Kafka, so a compacted stream may contain some superseded events, which are waiting to be compacted away. To create a compacted event stream called customer-profiles with Kafka: $ kafka-topics --create \\ --bootstrap-server <bootstrap-url> \\ --replication-factor 3 \\ --partitions 3 \\ --topic customer-profiles \\ --config cleanup.policy=compact Created topic customer-profiles. The kafka-topics command can also verify the current topic's configuration: $ kafka-topics --bootstrap-server localhost:9092 --topic customer-profiles --describe Topic: customer-profiles PartitionCount: 3 ReplicationFactor: 1 Configs: cleanup.policy=compact,segment.bytes=1073741824 Topic: customer-profiles Partition: 0 Leader: 0 Replicas: 0 Isr: 0 Offline: Topic: customer-profiles Partition: 1 Leader: 0 Replicas: 0 Isr: 0 Offline: Topic: customer-profiles Partition: 2 Leader: 0 Replicas: 0 Isr: 0 Offline:","title":"Implementation"},{"location":"event-storage/compacted-event-stream/#considerations","text":"Compacted event streams allow for some optimizations: First, they allow the Event Streaming Platform to limit the storage growth of the stream in a data-specific way, rather than removing events universally after a pre-configured period of time. Second, having smaller streams allows for faster recovery and system migration strategies. It is important to understand that compaction, on purpose, removes historical data from an event stream by removing superseded events as defined above. In many use cases, however, historical data should not be removed, such as for a stream of financial transactions, where every single transaction needs to be recorded and stored. Here, if the storage of the stream is the primary concern, use an Infinite Retention Event Stream instead of a compacted stream.","title":"Considerations"},{"location":"event-storage/compacted-event-stream/#references","text":"Compacted event streams are highly related to the State Table pattern. Compacted event streams work a bit like simple Log Structured Merge Trees . Cleanup policy configuration of Kafka topics.","title":"References"},{"location":"event-storage/event-store/","text":"Event Store When considering an architecture based on an Event Streaming Platform , the first fundamental question is, \"How do we store our events?\" This isn't as obvious as it first sounds, as we have to consider persistence, query performance, write throughput, availability, auditing and many other concerns. This decision will affect all the ones that follow. Problem How can events be stored such that they form a reliable source of truth for applications? Solution Incoming events are stored in an Event Stream , implemented as an append-only log. By choosing this data structure, we can guarantee constant-time (\u0398(1)) writes, lock-free concurrent reads, and straightforward replication across multiple machines. Implementation Apache Kafka\u00ae is an event store that maintains a persistent, append-only stream \u2014 a topic \u2014 for each kind of event we need to store. These topics are: Write-efficient - an append-only log is one of the fastest, cheapest data structures to write to. Read efficient - multiple readers (cf. Event Processor ) can consume the same stream without blocking. Durable - all events are written to storage (e.g., local disk, network storage device), either synchronously (for maximum reliability) or asynchronously (for maximum throughput). Events can be as long-lived as needed, and even stored forever. Highly-available - each event is written to multiple storage devices and replicated across multiple machines, and in the case of failure one of the redundant machines takes over. Auditable - every change is captured and persisted. Every result can be traced back to its source event(s). Considerations It's worth briefly contrasting Apache Kafka\u00ae with message queues and relational databases. While queues also concern themselves with a stream of events, they often consider events as short-lived, independent messages. A message may only exist in memory, or it may be durable enough for data to survive server restarts, but in general they aren't intended to hold on to events for months or even years. Further, their querying capabilities may be limited to simple filtering, offloading more complex queries like joins and aggregations to the application level. In contrast, relational databases are very good at maintaining a persistent state of the world in perpetuity, and answering arbitrary questions about it, but they often fall short on auditing - answering which events led up to the current state - and on liveness - what new events do we need to consider. They are predominantly designed for use cases that operate on data at rest, whereas an Event Store is designed from the ground up for data in motion and event streaming. By beginning with a fundamental data-structure for event capture, and building on that to provide long-term persistence and arbitrary analysis capabilities, Apache Kafka\u00ae provides an ideal choice of event store for modern, data-driven architectures. References See also: Geo-Replication . Using logs to build a solid data infrastructure What Is Apache Kafka? Kafka: The Definitive Guide free Ebook.","title":"Event Store"},{"location":"event-storage/event-store/#event-store","text":"When considering an architecture based on an Event Streaming Platform , the first fundamental question is, \"How do we store our events?\" This isn't as obvious as it first sounds, as we have to consider persistence, query performance, write throughput, availability, auditing and many other concerns. This decision will affect all the ones that follow.","title":"Event Store"},{"location":"event-storage/event-store/#problem","text":"How can events be stored such that they form a reliable source of truth for applications?","title":"Problem"},{"location":"event-storage/event-store/#solution","text":"Incoming events are stored in an Event Stream , implemented as an append-only log. By choosing this data structure, we can guarantee constant-time (\u0398(1)) writes, lock-free concurrent reads, and straightforward replication across multiple machines.","title":"Solution"},{"location":"event-storage/event-store/#implementation","text":"Apache Kafka\u00ae is an event store that maintains a persistent, append-only stream \u2014 a topic \u2014 for each kind of event we need to store. These topics are: Write-efficient - an append-only log is one of the fastest, cheapest data structures to write to. Read efficient - multiple readers (cf. Event Processor ) can consume the same stream without blocking. Durable - all events are written to storage (e.g., local disk, network storage device), either synchronously (for maximum reliability) or asynchronously (for maximum throughput). Events can be as long-lived as needed, and even stored forever. Highly-available - each event is written to multiple storage devices and replicated across multiple machines, and in the case of failure one of the redundant machines takes over. Auditable - every change is captured and persisted. Every result can be traced back to its source event(s).","title":"Implementation"},{"location":"event-storage/event-store/#considerations","text":"It's worth briefly contrasting Apache Kafka\u00ae with message queues and relational databases. While queues also concern themselves with a stream of events, they often consider events as short-lived, independent messages. A message may only exist in memory, or it may be durable enough for data to survive server restarts, but in general they aren't intended to hold on to events for months or even years. Further, their querying capabilities may be limited to simple filtering, offloading more complex queries like joins and aggregations to the application level. In contrast, relational databases are very good at maintaining a persistent state of the world in perpetuity, and answering arbitrary questions about it, but they often fall short on auditing - answering which events led up to the current state - and on liveness - what new events do we need to consider. They are predominantly designed for use cases that operate on data at rest, whereas an Event Store is designed from the ground up for data in motion and event streaming. By beginning with a fundamental data-structure for event capture, and building on that to provide long-term persistence and arbitrary analysis capabilities, Apache Kafka\u00ae provides an ideal choice of event store for modern, data-driven architectures.","title":"Considerations"},{"location":"event-storage/event-store/#references","text":"See also: Geo-Replication . Using logs to build a solid data infrastructure What Is Apache Kafka? Kafka: The Definitive Guide free Ebook.","title":"References"},{"location":"event-storage/infinite-retention-event-stream/","text":"Infinite Retention Event Stream Many use cases demand that Events in an Event Stream will be stored for forever so that the dataset is available in its entirety. Problem How can we ensure that events in a stream are retained forever? Solution The solution for infinite retention depends on the specific Event Streaming Platform . Some platforms support infinite retention \"out of the box\", requiring no action on behalf of the end users. If an Event Streaming Platform does not support infinite storage, infinite retention can be partially achieved with an Event Sink Connector pattern which offloads Events into permanent external storage. Implementation When using Confluent Cloud , infinite retention is built into the Event Streaming Platform ( availability may be limited based on cluster type and cloud provider ). Users of the platform can benefit from infinite storage without any changes to their client applications or operations. For on-premises Event Streaming Platforms , Confluent Platform adds the ability for infinite retention by extending Apache Kafka with Tiered Storage . Tiered storage separates the compute and storage layers, allowing the operator to scale either of those independently as needed. Newly arrived Events are considered \"hot\", but as time moves on, they become \"colder\" and migrate to more cost-effective external storage like an AWS S3 bucket. As cloud-native object stores can effectively scale to infinite size, the Kafka cluster can act as the system of record for infinite Event Streams . Considerations Infinite Retention Streams are typically used to store entire datasets which will be used by many subscribers. For example, storing the canonical customer dataset in an Infinite Retention Event Stream makes it available to any other system, regardless of their database technology. The customer's dataset can be easily imported or reimported as a whole. Compacted Event Streams are often used as a form of Infinite Retention Event Stream. However compacted streams are not infinite. Instead, they retain only the most recent Events for each key, meaning their contents matches the dataset held in an equivalent CRUD database table. References The blog post Infinite Storage in Confluent goes describes the tiered storage approach in more detail. An Event Sink Connector can be used to implement an infinite retention event stream by loading Event into permanent external storage.","title":"Infinite Retention Event Stream"},{"location":"event-storage/infinite-retention-event-stream/#infinite-retention-event-stream","text":"Many use cases demand that Events in an Event Stream will be stored for forever so that the dataset is available in its entirety.","title":"Infinite Retention Event Stream"},{"location":"event-storage/infinite-retention-event-stream/#problem","text":"How can we ensure that events in a stream are retained forever?","title":"Problem"},{"location":"event-storage/infinite-retention-event-stream/#solution","text":"The solution for infinite retention depends on the specific Event Streaming Platform . Some platforms support infinite retention \"out of the box\", requiring no action on behalf of the end users. If an Event Streaming Platform does not support infinite storage, infinite retention can be partially achieved with an Event Sink Connector pattern which offloads Events into permanent external storage.","title":"Solution"},{"location":"event-storage/infinite-retention-event-stream/#implementation","text":"When using Confluent Cloud , infinite retention is built into the Event Streaming Platform ( availability may be limited based on cluster type and cloud provider ). Users of the platform can benefit from infinite storage without any changes to their client applications or operations. For on-premises Event Streaming Platforms , Confluent Platform adds the ability for infinite retention by extending Apache Kafka with Tiered Storage . Tiered storage separates the compute and storage layers, allowing the operator to scale either of those independently as needed. Newly arrived Events are considered \"hot\", but as time moves on, they become \"colder\" and migrate to more cost-effective external storage like an AWS S3 bucket. As cloud-native object stores can effectively scale to infinite size, the Kafka cluster can act as the system of record for infinite Event Streams .","title":"Implementation"},{"location":"event-storage/infinite-retention-event-stream/#considerations","text":"Infinite Retention Streams are typically used to store entire datasets which will be used by many subscribers. For example, storing the canonical customer dataset in an Infinite Retention Event Stream makes it available to any other system, regardless of their database technology. The customer's dataset can be easily imported or reimported as a whole. Compacted Event Streams are often used as a form of Infinite Retention Event Stream. However compacted streams are not infinite. Instead, they retain only the most recent Events for each key, meaning their contents matches the dataset held in an equivalent CRUD database table.","title":"Considerations"},{"location":"event-storage/infinite-retention-event-stream/#references","text":"The blog post Infinite Storage in Confluent goes describes the tiered storage approach in more detail. An Event Sink Connector can be used to implement an infinite retention event stream by loading Event into permanent external storage.","title":"References"},{"location":"event-storage/limited-retention-event-stream/","text":"Limited Retention Event Stream Many use cases allow for Events to be removed from an Event Stream in order to preserve space and prevent the reading of stale data. Problem How can we remove events from an Event Stream based on a criteria, such as event age or Event Stream size? Solution The solution for limited retention will depend on the Event Streaming Platform . Most platforms will allow for deletion of events using an API or with an automated process configured within the platform itself. Because Event Streams are modeled as immutable event logs in the Event Store , events will be removed from the beginning of an Event Stream moving forward (i.e., oldest events are being removed first). Event Processing Applications that are reading from the stream will not be given the deleted events. Implementation Apache Kafka\u00ae implements a Limited Retention Event Stream by default. With Kafka, Event Streams are modeled as Topics . Kafka provides two types of retention policy, which can be configured on a per-topic basis or as a default for new topics. Time-Based Retention With time-based retention, events will be removed from the topic after the event timestamp indicates an event is older than the configured log retention time. On Kafka this is configured with the log.retention.hours setting, which can be set as a default to apply to all topics or on a per-topic basis. Additionally, Kafka respects a log.retention.minutes and log.retention.ms settings to define shorter retention periods. The following example sets the retention period of a topic to one year: log.retention.hours=8760 For more guidance and configuring time-based data retention, see the Kafka Broker Configurations documentation for default settings, or the Modifying Topics section for modifying existing topics. Size-Based Retention With size-based retention, events will begin to be removed from the topic once the total size of the topic violates the configured maximum size. Kafka supports a log.retention.bytes configure. For example, to configure the maximum size of a topic to 100GB you could set the configuration as follows: log.retention.bytes=107374127424 For more guidance and configuring size-based data retention, see the Kafka Broker Configurations documentation for default settings, or the Modifying Topics section for modifying existing topics. How Events Are Being Removed From an Event Stream For either method of configuring retention, Kafka does not immediately remove events one by one when they violate the configured retention settings. To understand how they are removed, we first need to explain that Kafka topics are further broken down into topic partitions (see Partitioned Placement ). Partitions themselves are further divided into files called segments . Segments represent a sequence of the events in a particular partition, and these segments are what is being removed once a violation of the retention policy has occurred. We can further fine-tune the removal algorithm with additional settings such as log.retention.check.interval.ms and segment configuration, such as log.segment.bytes . Considerations We should account for failure scenarios when configuring event streams with limited retention. Our applications should have enough time to read and process events before they are being removed, which means the configured retention must cover the time frames of potential outages experienced by our applications. For example, if an Operations team has an SLA of 2 business days (Mon to Fri) for a non-mission-critical Kafka use case, then retention should be set to at least 4 days to cover for incidents that happen right before or during the weekends. Similarly, if we want to allow applications to reprocess the same events multiple times (often called the ability to reprocess historical data ) for use cases such as A/B testing or machine learning, then we must increase the retention settings accordingly. That's why it is common in practice to configure Kafka topics with long retention periods (months or years). References This pattern is similar to Message Expiration in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Apache Kafka 101: Introduction provides a primer on \"What is Kafka, and how does it work?\" A related pattern is the Infinite Retention Event Stream pattern which details Event Streams that stores events indefinitely.","title":"Limited Retention Event Stream"},{"location":"event-storage/limited-retention-event-stream/#limited-retention-event-stream","text":"Many use cases allow for Events to be removed from an Event Stream in order to preserve space and prevent the reading of stale data.","title":"Limited Retention Event Stream"},{"location":"event-storage/limited-retention-event-stream/#problem","text":"How can we remove events from an Event Stream based on a criteria, such as event age or Event Stream size?","title":"Problem"},{"location":"event-storage/limited-retention-event-stream/#solution","text":"The solution for limited retention will depend on the Event Streaming Platform . Most platforms will allow for deletion of events using an API or with an automated process configured within the platform itself. Because Event Streams are modeled as immutable event logs in the Event Store , events will be removed from the beginning of an Event Stream moving forward (i.e., oldest events are being removed first). Event Processing Applications that are reading from the stream will not be given the deleted events.","title":"Solution"},{"location":"event-storage/limited-retention-event-stream/#implementation","text":"Apache Kafka\u00ae implements a Limited Retention Event Stream by default. With Kafka, Event Streams are modeled as Topics . Kafka provides two types of retention policy, which can be configured on a per-topic basis or as a default for new topics.","title":"Implementation"},{"location":"event-storage/limited-retention-event-stream/#time-based-retention","text":"With time-based retention, events will be removed from the topic after the event timestamp indicates an event is older than the configured log retention time. On Kafka this is configured with the log.retention.hours setting, which can be set as a default to apply to all topics or on a per-topic basis. Additionally, Kafka respects a log.retention.minutes and log.retention.ms settings to define shorter retention periods. The following example sets the retention period of a topic to one year: log.retention.hours=8760 For more guidance and configuring time-based data retention, see the Kafka Broker Configurations documentation for default settings, or the Modifying Topics section for modifying existing topics.","title":"Time-Based Retention"},{"location":"event-storage/limited-retention-event-stream/#size-based-retention","text":"With size-based retention, events will begin to be removed from the topic once the total size of the topic violates the configured maximum size. Kafka supports a log.retention.bytes configure. For example, to configure the maximum size of a topic to 100GB you could set the configuration as follows: log.retention.bytes=107374127424 For more guidance and configuring size-based data retention, see the Kafka Broker Configurations documentation for default settings, or the Modifying Topics section for modifying existing topics.","title":"Size-Based Retention"},{"location":"event-storage/limited-retention-event-stream/#how-events-are-being-removed-from-an-event-stream","text":"For either method of configuring retention, Kafka does not immediately remove events one by one when they violate the configured retention settings. To understand how they are removed, we first need to explain that Kafka topics are further broken down into topic partitions (see Partitioned Placement ). Partitions themselves are further divided into files called segments . Segments represent a sequence of the events in a particular partition, and these segments are what is being removed once a violation of the retention policy has occurred. We can further fine-tune the removal algorithm with additional settings such as log.retention.check.interval.ms and segment configuration, such as log.segment.bytes .","title":"How Events Are Being Removed From an Event Stream"},{"location":"event-storage/limited-retention-event-stream/#considerations","text":"We should account for failure scenarios when configuring event streams with limited retention. Our applications should have enough time to read and process events before they are being removed, which means the configured retention must cover the time frames of potential outages experienced by our applications. For example, if an Operations team has an SLA of 2 business days (Mon to Fri) for a non-mission-critical Kafka use case, then retention should be set to at least 4 days to cover for incidents that happen right before or during the weekends. Similarly, if we want to allow applications to reprocess the same events multiple times (often called the ability to reprocess historical data ) for use cases such as A/B testing or machine learning, then we must increase the retention settings accordingly. That's why it is common in practice to configure Kafka topics with long retention periods (months or years).","title":"Considerations"},{"location":"event-storage/limited-retention-event-stream/#references","text":"This pattern is similar to Message Expiration in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Apache Kafka 101: Introduction provides a primer on \"What is Kafka, and how does it work?\" A related pattern is the Infinite Retention Event Stream pattern which details Event Streams that stores events indefinitely.","title":"References"},{"location":"event-stream/event-broker/","text":"Event Broker In a software architecture, loosely-coupled components allow services and applications to change with minimal impact on their dependent systems and applications. On the side of the organization, this loose coupling also allows different development teams to efficiently work independently of each other. Problem How can we decouple Event Sources from Event Sinks , given that both may include cloud services, systems such as relational databases, and applications and microservices? Solution We can use the Event Broker of an Event Streaming Platform to provide this decoupling. Typically, multiple event brokers are deployed as a distributed cluster to ensure elasticity, scalability, and fault-tolerance during operations. Event brokers collaborate on receiving and durably storing Events (write operations) as well as serving events (read operations) into Event Streams from one or many clients in parallel. Clients that produce events are called Event Sources and are decoupled and isolated, through the brokers, from clients that consume the events, which are called Event Sinks . Typically, the technical architecture follows the design of \"dumb brokers, smart clients.\" Here, the broker intentionally limits its client-facing functionality to achieve the best performance and scalability. This means that additional work must be performed by the broker's clients. For example, unlike in traditional messaging brokers, it is the responsibility of an event sink (a consumer) to track its individual progress of reading and processing from an event stream. Implementation Apache Kafka\u00ae is an open-source, distributed Event Streaming Platform , which implements the Event Broker pattern. Kafka runs as a highly scalable and fault-tolerant cluster of brokers. Many Event Processing Applications can produce, consume, and process Events from the cluster in parallel, with strong guarantees such as transactions, using a fully decoupled and yet coordinated architecture. Additionally, Kafka's protocol provides strong backwards compatibility and forwards compatibility guarantees between the server-side brokers and their client applications that produce, consume, and process events. For example, client applications using a new version of Kafka can work with a cluster of brokers running an older version of Kafka. Similarly, older client applications continue to work even when the cluster of brokers is upgraded to a newer version of Kafka (and Kafka also supports in-place version upgrades of clusters). This is another example of decoupling the various components in a Kafka-based architecture, resulting in even better flexibility during design and operations. Considerations In contrast to traditional message brokers, event brokers provide a distributed, durable, and fault-tolerant storage layer. This has several important benefits. For instance, client applications can initiate and resume event production and consumption independently from each other. Similarly, the client applications don't need to be connected to the brokers perpetually in order to not miss any events. When an application is taken offline for maintenance and subsequently restarted, it will then automatically resume its consumption and processing of an event stream exactly at the point where it stopped before. The strong guarantees provided by the brokers in the Event Streaming Platform ensure that applications do not suffer from duplicate data or from data loss (for example, missing out on events that were written during the maintenance window) in these situations, even in the face of failures such as machine or network outages. Another benefit is that client applications can \"rewind the time\" and re-consume historical data in event streams as often as needed. This is useful in many situations, including A/B testing, auditing and compliance, and training and retraining models for machine learning, as well as when fixing unexpected application errors and bugs that occurred in production. References This pattern is derived from Message Broker in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf.","title":"Event Broker"},{"location":"event-stream/event-broker/#event-broker","text":"In a software architecture, loosely-coupled components allow services and applications to change with minimal impact on their dependent systems and applications. On the side of the organization, this loose coupling also allows different development teams to efficiently work independently of each other.","title":"Event Broker"},{"location":"event-stream/event-broker/#problem","text":"How can we decouple Event Sources from Event Sinks , given that both may include cloud services, systems such as relational databases, and applications and microservices?","title":"Problem"},{"location":"event-stream/event-broker/#solution","text":"We can use the Event Broker of an Event Streaming Platform to provide this decoupling. Typically, multiple event brokers are deployed as a distributed cluster to ensure elasticity, scalability, and fault-tolerance during operations. Event brokers collaborate on receiving and durably storing Events (write operations) as well as serving events (read operations) into Event Streams from one or many clients in parallel. Clients that produce events are called Event Sources and are decoupled and isolated, through the brokers, from clients that consume the events, which are called Event Sinks . Typically, the technical architecture follows the design of \"dumb brokers, smart clients.\" Here, the broker intentionally limits its client-facing functionality to achieve the best performance and scalability. This means that additional work must be performed by the broker's clients. For example, unlike in traditional messaging brokers, it is the responsibility of an event sink (a consumer) to track its individual progress of reading and processing from an event stream.","title":"Solution"},{"location":"event-stream/event-broker/#implementation","text":"Apache Kafka\u00ae is an open-source, distributed Event Streaming Platform , which implements the Event Broker pattern. Kafka runs as a highly scalable and fault-tolerant cluster of brokers. Many Event Processing Applications can produce, consume, and process Events from the cluster in parallel, with strong guarantees such as transactions, using a fully decoupled and yet coordinated architecture. Additionally, Kafka's protocol provides strong backwards compatibility and forwards compatibility guarantees between the server-side brokers and their client applications that produce, consume, and process events. For example, client applications using a new version of Kafka can work with a cluster of brokers running an older version of Kafka. Similarly, older client applications continue to work even when the cluster of brokers is upgraded to a newer version of Kafka (and Kafka also supports in-place version upgrades of clusters). This is another example of decoupling the various components in a Kafka-based architecture, resulting in even better flexibility during design and operations.","title":"Implementation"},{"location":"event-stream/event-broker/#considerations","text":"In contrast to traditional message brokers, event brokers provide a distributed, durable, and fault-tolerant storage layer. This has several important benefits. For instance, client applications can initiate and resume event production and consumption independently from each other. Similarly, the client applications don't need to be connected to the brokers perpetually in order to not miss any events. When an application is taken offline for maintenance and subsequently restarted, it will then automatically resume its consumption and processing of an event stream exactly at the point where it stopped before. The strong guarantees provided by the brokers in the Event Streaming Platform ensure that applications do not suffer from duplicate data or from data loss (for example, missing out on events that were written during the maintenance window) in these situations, even in the face of failures such as machine or network outages. Another benefit is that client applications can \"rewind the time\" and re-consume historical data in event streams as often as needed. This is useful in many situations, including A/B testing, auditing and compliance, and training and retraining models for machine learning, as well as when fixing unexpected application errors and bugs that occurred in production.","title":"Considerations"},{"location":"event-stream/event-broker/#references","text":"This pattern is derived from Message Broker in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf.","title":"References"},{"location":"event-stream/event-stream/","text":"Event Stream Event Processing Applications need to communicate, and ideally the communication is based on Events . The applications need a standard mechanism to use for this communication. Problem How can Event Processors and applications communicate with each other using event streaming? Solution { \"name\": \"elasticsearch-ksqldb\", \"config\": { \"connector.class\": \"io.confluent.connect.elasticsearch.ElasticsearchSinkConnector\", \"consumer.interceptor.classes\": \"io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor\", \"topics\": \"WIKIPEDIABOT\", \"topic.index.map\": \"WIKIPEDIABOT:wikipediabot\", \"connection.url\": \"http://elasticsearch:9200\", \"type.name\": \"_doc\", \"key.ignore\": true, \"key.converter.schema.registry.url\": \"https://schemaregistry:8085\", \"value.converter\": \"io.confluent.connect.avro.AvroConverter\", \"value.converter.schema.registry.url\": \"https://schemaregistry:8085\", \"value.converter.schema.registry.ssl.truststore.location\": \"/etc/kafka/secrets/kafka.client.truststore.jks\", \"value.converter.schema.registry.ssl.truststore.password\": \"confluent\", \"value.converter.basic.auth.credentials.source\": \"USER_INFO\", \"value.converter.basic.auth.user.info\": \"connectorSA:connectorSA\", \"consumer.override.sasl.jaas.config\": \"org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required username=\\\"connectorSA\\\" password=\\\"connectorSA\\\" metadataServerUrls=\\\"https://kafka1:8091,https://kafka2:8092\\\";\", \"schema.ignore\": true } } Connect the Event Processing Applications with an Event Stream. Event Sources produce events to the Event Stream, and Event Processors and Event Sinks consume them. Event Streams are named, allowing communication over a specific stream of events. Notice how Event Streams decouple the source and sink applications, which communicate indirectly and asynchronously with each other through events. Additionally, event data formats are often validated, in order to govern the communication between applications. Generally speaking, an Event Stream records the history of what has happened in the world as a sequence of events (think: a sequence of facts). Examples of streams would be a sales ledger or the sequence of moves in a chess match. This history is an ordered sequence or chain of events, so we know which event happened before another event and can infer causality (for example, \u201cWhite moved the e2 pawn to e4; then Black moved the e7 pawn to e5\u201d). A stream thus represents both the past and the present: as we go from today to tomorrow -- or from one millisecond to the next -- new events are constantly being appended to the history. Conceptually, a stream provides immutable data. It supports only inserting (appending) new events, and existing events cannot be changed. Streams are persistent, durable, and fault-tolerant. Unlike traditional message queues, events stored in streams can be read as often as needed by Event Sinks and Event Processing Applications, and they are not deleted after consumption. Instead, retention policies control how events are retained. Events in a stream can be keyed , and we can have many events for one key. For a stream of payments of all customers, the customer ID might be the key (cf. related patterns such as Partitioned Parallelism ). Implementation In Apache Kafka\u00ae , Event Streams are called topics . Kafka allows you to define policies which dictate how events are retained, using time or size limitations or retaining events forever . Kafka consumers (Event Sinks and Event Processing Applications) are able to decide where in an event stream to begin reading. They can choose to begin reading from the oldest or newest event, or seek to a specific location in the topic, using the event's timestamp or position (called the offset ). The streaming database ksqlDB supports Event Streams using a familiar SQL syntax. The following example creates a stream of events named riderLocations , representing locations of riders in a car-sharing service. The data format is JSON. CREATE STREAM riderLocations (profileId VARCHAR, latitude DOUBLE, longitude DOUBLE) WITH (kafka_topic='locations', value_format='json'); New events can be written to the riderLocations stream using the INSERT syntax: INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('c2309eec', 37.7877, -122.4205); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('18f4ea86', 37.3903, -122.0643); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('4ab5cbad', 37.3952, -122.0813); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('8b6eae59', 37.3944, -122.0813); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('4a7c7b41', 37.4049, -122.0822); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('4ddad000', 37.7857, -122.4011); A push query , also known as a streaming query, can be run continuously over the stream using a SELECT command with the EMIT CHANGES clause. As new events arrive, this query will emit new results that match the WHERE conditionals. The following query looks for riders in close proximity to Mountain View, California, in the United States. -- Mountain View lat, long: 37.4133, -122.1162 SELECT * FROM riderLocations WHERE GEO_DISTANCE(latitude, longitude, 37.4133, -122.1162) <= 5 EMIT CHANGES; References This pattern is derived from Message Channel in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. See the Kafka Storage & Processing Fundamentals page for essential details on Kafka","title":"Event Stream"},{"location":"event-stream/event-stream/#event-stream","text":"Event Processing Applications need to communicate, and ideally the communication is based on Events . The applications need a standard mechanism to use for this communication.","title":"Event Stream"},{"location":"event-stream/event-stream/#problem","text":"How can Event Processors and applications communicate with each other using event streaming?","title":"Problem"},{"location":"event-stream/event-stream/#solution","text":"{ \"name\": \"elasticsearch-ksqldb\", \"config\": { \"connector.class\": \"io.confluent.connect.elasticsearch.ElasticsearchSinkConnector\", \"consumer.interceptor.classes\": \"io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor\", \"topics\": \"WIKIPEDIABOT\", \"topic.index.map\": \"WIKIPEDIABOT:wikipediabot\", \"connection.url\": \"http://elasticsearch:9200\", \"type.name\": \"_doc\", \"key.ignore\": true, \"key.converter.schema.registry.url\": \"https://schemaregistry:8085\", \"value.converter\": \"io.confluent.connect.avro.AvroConverter\", \"value.converter.schema.registry.url\": \"https://schemaregistry:8085\", \"value.converter.schema.registry.ssl.truststore.location\": \"/etc/kafka/secrets/kafka.client.truststore.jks\", \"value.converter.schema.registry.ssl.truststore.password\": \"confluent\", \"value.converter.basic.auth.credentials.source\": \"USER_INFO\", \"value.converter.basic.auth.user.info\": \"connectorSA:connectorSA\", \"consumer.override.sasl.jaas.config\": \"org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required username=\\\"connectorSA\\\" password=\\\"connectorSA\\\" metadataServerUrls=\\\"https://kafka1:8091,https://kafka2:8092\\\";\", \"schema.ignore\": true } } Connect the Event Processing Applications with an Event Stream. Event Sources produce events to the Event Stream, and Event Processors and Event Sinks consume them. Event Streams are named, allowing communication over a specific stream of events. Notice how Event Streams decouple the source and sink applications, which communicate indirectly and asynchronously with each other through events. Additionally, event data formats are often validated, in order to govern the communication between applications. Generally speaking, an Event Stream records the history of what has happened in the world as a sequence of events (think: a sequence of facts). Examples of streams would be a sales ledger or the sequence of moves in a chess match. This history is an ordered sequence or chain of events, so we know which event happened before another event and can infer causality (for example, \u201cWhite moved the e2 pawn to e4; then Black moved the e7 pawn to e5\u201d). A stream thus represents both the past and the present: as we go from today to tomorrow -- or from one millisecond to the next -- new events are constantly being appended to the history. Conceptually, a stream provides immutable data. It supports only inserting (appending) new events, and existing events cannot be changed. Streams are persistent, durable, and fault-tolerant. Unlike traditional message queues, events stored in streams can be read as often as needed by Event Sinks and Event Processing Applications, and they are not deleted after consumption. Instead, retention policies control how events are retained. Events in a stream can be keyed , and we can have many events for one key. For a stream of payments of all customers, the customer ID might be the key (cf. related patterns such as Partitioned Parallelism ).","title":"Solution"},{"location":"event-stream/event-stream/#implementation","text":"In Apache Kafka\u00ae , Event Streams are called topics . Kafka allows you to define policies which dictate how events are retained, using time or size limitations or retaining events forever . Kafka consumers (Event Sinks and Event Processing Applications) are able to decide where in an event stream to begin reading. They can choose to begin reading from the oldest or newest event, or seek to a specific location in the topic, using the event's timestamp or position (called the offset ). The streaming database ksqlDB supports Event Streams using a familiar SQL syntax. The following example creates a stream of events named riderLocations , representing locations of riders in a car-sharing service. The data format is JSON. CREATE STREAM riderLocations (profileId VARCHAR, latitude DOUBLE, longitude DOUBLE) WITH (kafka_topic='locations', value_format='json'); New events can be written to the riderLocations stream using the INSERT syntax: INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('c2309eec', 37.7877, -122.4205); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('18f4ea86', 37.3903, -122.0643); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('4ab5cbad', 37.3952, -122.0813); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('8b6eae59', 37.3944, -122.0813); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('4a7c7b41', 37.4049, -122.0822); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('4ddad000', 37.7857, -122.4011); A push query , also known as a streaming query, can be run continuously over the stream using a SELECT command with the EMIT CHANGES clause. As new events arrive, this query will emit new results that match the WHERE conditionals. The following query looks for riders in close proximity to Mountain View, California, in the United States. -- Mountain View lat, long: 37.4133, -122.1162 SELECT * FROM riderLocations WHERE GEO_DISTANCE(latitude, longitude, 37.4133, -122.1162) <= 5 EMIT CHANGES;","title":"Implementation"},{"location":"event-stream/event-stream/#references","text":"This pattern is derived from Message Channel in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. See the Kafka Storage & Processing Fundamentals page for essential details on Kafka","title":"References"},{"location":"event-stream/event-streaming-platform/","text":"Event Streaming Platform Companies are rarely built on a single data store and a single application to interact with it. Typically, a company may have hundreds or thousands of applications, databases, data warehouses, or other data stores. The company's data is spread across these resources, and the interconnection between them is immensely complicated. In larger enterprises, multiple lines of business can complicate the situation even further. Modern software architectures, such as microservices and SaaS applications, also add complexity, as engineers are tasked with weaving the entire infrastructure together cohesively. Furthermore, companies can no longer survive without reacting to Events within their business in real time. Customers and business partners expect immediate reactions and rich interactive applications. Today, data is in motion, and engineering teams need to model applications to process business requirements as streams of events, not as data at rest, sitting idly in a traditional data store. Problem What architecture can we use to model everything within our business as streams of events, creating a modern, fault-tolerant, and scalable platform for building modern applications? Solution We can design business processes and applications around Event Streams . Everything, from sales, orders, trades, and customer experiences to sensor readings and database updates, is modeled as an Event . Events are written to the Event Streaming Platform once, allowing distributed functions within the business to react in real time. Systems external to the Event Streaming Platform are integrated using Event Sources and Event Sinks . Business logic is built within Event Processing Applications , which are composed of Event Processors that read events from and write events to Event Streams. Implementation Apache Kafka\u00ae is the most popular Event Streaming Platform, designed to address the business requirements of a modern distributed architecture. You can use Kafka to read, write, process, query, and react to Event Streams in a way that's horizontally scalable, fault-tolerant, and simple to use. Kafka is built upon many of the patterns described in Event Streaming Patterns . Fundamentals Data in Kafka is exchanged as events, which represent facts about something that has occurred. Examples of events include orders, payments, activities, and measurements. In Kafka, events are sometimes also referred to as records or messages , and they contain data and metadata describing the event. Events are written to, stored in, and read from Event Streams . In Kafka, these streams are called topics . Topics have names and generally contain \"related\" records of a particular use case, such as customer payments. Topics are modeled as durable, distributed, append-only logs in the Event Store . For more details about Kafka topics, see the Apache Kafka 101 course . Applications that write events to topics are called Producers . Producers come in many forms and represent the Event Source pattern. Reading events is performed by Consumers , which represent Event Sinks . Consumers typically operate in a distributed, coordinated fashion to increase scale and fault tolerance. Event Processing Applications act as both event sources and event sinks. Applications which produce and consume events as described above are referred to as \"clients.\" These client applications can be authored in a variety of programming languages, including Java , Go , C/C++ , C# (.NET) , Python , Node.JS , and more . Stream Processing Event Processing Applications can be built atop Kafka using a variety of techniques. ksqlDB The streaming database ksqlDB allows you to build Event Processing Applications using SQL syntax. It ships with an API, command line interface (CLI), and GUI. ksqlDB's elastic architecture decouples its distributed compute layer from its distributed storage layer, which uses and tightly integrates with Kafka. Kafka Streams The Kafka client library Kafka Streams allows you to build elastic applications and microservices on the JVM, using languages such as Java and Scala. An application can run in a distributed fashion across multiple instances for better scalability and fault-tolerance. Data Integrations The Kafka Connect framework allows you to scalably and reliably integrate cloud services and data systems external to Kafka into the Event Streaming Platform. Data from these systems is set in motion by being continuously imported and/or exported as Event Streams through Kafka connectors . There are hundreds of ready-to-use Kafka connectors available on Confluent Hub . On-boarding existing data systems into Kafka is often the first step in the journey of adopting an Event Streaming Platform. Source Connectors pull data into Kafka topics from sources such as traditional databases, cloud object storage services, or SaaS products such as Salesforce. Advanced integrations are possible with patterns such as Database Write Through and Database Write Aside . Sink Connectors are the complementary pattern to Source Connectors . While source connectors bring data into the Event Streaming Platform continuously, sinks continuously deliver data from Kafka streams to external cloud services and systems. Common destination systems include cloud data warehouse services, function-based serverless compute services, relational databases, Elasticsearch, and cloud object storage services. Considerations Event Streaming Platforms are distributed computing systems made up of a diverse set of components. Because building and operating such a platform requires significant engineering expertise and resources, many organizations opt for a fully-managed Kafka service such as Confluent Cloud , rather than self-managing the platform, so that they can focus on creating business value. References This pattern is derived from Message Bus in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. Confluent Cloud is a cloud-native service for Apache Kafka\u00ae. The Apache Kafka 101 course provides a primer on what Kafka is and how it works.","title":"Event Streaming Platform"},{"location":"event-stream/event-streaming-platform/#event-streaming-platform","text":"Companies are rarely built on a single data store and a single application to interact with it. Typically, a company may have hundreds or thousands of applications, databases, data warehouses, or other data stores. The company's data is spread across these resources, and the interconnection between them is immensely complicated. In larger enterprises, multiple lines of business can complicate the situation even further. Modern software architectures, such as microservices and SaaS applications, also add complexity, as engineers are tasked with weaving the entire infrastructure together cohesively. Furthermore, companies can no longer survive without reacting to Events within their business in real time. Customers and business partners expect immediate reactions and rich interactive applications. Today, data is in motion, and engineering teams need to model applications to process business requirements as streams of events, not as data at rest, sitting idly in a traditional data store.","title":"Event Streaming Platform"},{"location":"event-stream/event-streaming-platform/#problem","text":"What architecture can we use to model everything within our business as streams of events, creating a modern, fault-tolerant, and scalable platform for building modern applications?","title":"Problem"},{"location":"event-stream/event-streaming-platform/#solution","text":"We can design business processes and applications around Event Streams . Everything, from sales, orders, trades, and customer experiences to sensor readings and database updates, is modeled as an Event . Events are written to the Event Streaming Platform once, allowing distributed functions within the business to react in real time. Systems external to the Event Streaming Platform are integrated using Event Sources and Event Sinks . Business logic is built within Event Processing Applications , which are composed of Event Processors that read events from and write events to Event Streams.","title":"Solution"},{"location":"event-stream/event-streaming-platform/#implementation","text":"Apache Kafka\u00ae is the most popular Event Streaming Platform, designed to address the business requirements of a modern distributed architecture. You can use Kafka to read, write, process, query, and react to Event Streams in a way that's horizontally scalable, fault-tolerant, and simple to use. Kafka is built upon many of the patterns described in Event Streaming Patterns .","title":"Implementation"},{"location":"event-stream/event-streaming-platform/#fundamentals","text":"Data in Kafka is exchanged as events, which represent facts about something that has occurred. Examples of events include orders, payments, activities, and measurements. In Kafka, events are sometimes also referred to as records or messages , and they contain data and metadata describing the event. Events are written to, stored in, and read from Event Streams . In Kafka, these streams are called topics . Topics have names and generally contain \"related\" records of a particular use case, such as customer payments. Topics are modeled as durable, distributed, append-only logs in the Event Store . For more details about Kafka topics, see the Apache Kafka 101 course . Applications that write events to topics are called Producers . Producers come in many forms and represent the Event Source pattern. Reading events is performed by Consumers , which represent Event Sinks . Consumers typically operate in a distributed, coordinated fashion to increase scale and fault tolerance. Event Processing Applications act as both event sources and event sinks. Applications which produce and consume events as described above are referred to as \"clients.\" These client applications can be authored in a variety of programming languages, including Java , Go , C/C++ , C# (.NET) , Python , Node.JS , and more .","title":"Fundamentals"},{"location":"event-stream/event-streaming-platform/#stream-processing","text":"Event Processing Applications can be built atop Kafka using a variety of techniques.","title":"Stream Processing"},{"location":"event-stream/event-streaming-platform/#ksqldb","text":"The streaming database ksqlDB allows you to build Event Processing Applications using SQL syntax. It ships with an API, command line interface (CLI), and GUI. ksqlDB's elastic architecture decouples its distributed compute layer from its distributed storage layer, which uses and tightly integrates with Kafka.","title":"ksqlDB"},{"location":"event-stream/event-streaming-platform/#kafka-streams","text":"The Kafka client library Kafka Streams allows you to build elastic applications and microservices on the JVM, using languages such as Java and Scala. An application can run in a distributed fashion across multiple instances for better scalability and fault-tolerance.","title":"Kafka Streams"},{"location":"event-stream/event-streaming-platform/#data-integrations","text":"The Kafka Connect framework allows you to scalably and reliably integrate cloud services and data systems external to Kafka into the Event Streaming Platform. Data from these systems is set in motion by being continuously imported and/or exported as Event Streams through Kafka connectors . There are hundreds of ready-to-use Kafka connectors available on Confluent Hub . On-boarding existing data systems into Kafka is often the first step in the journey of adopting an Event Streaming Platform. Source Connectors pull data into Kafka topics from sources such as traditional databases, cloud object storage services, or SaaS products such as Salesforce. Advanced integrations are possible with patterns such as Database Write Through and Database Write Aside . Sink Connectors are the complementary pattern to Source Connectors . While source connectors bring data into the Event Streaming Platform continuously, sinks continuously deliver data from Kafka streams to external cloud services and systems. Common destination systems include cloud data warehouse services, function-based serverless compute services, relational databases, Elasticsearch, and cloud object storage services.","title":"Data Integrations"},{"location":"event-stream/event-streaming-platform/#considerations","text":"Event Streaming Platforms are distributed computing systems made up of a diverse set of components. Because building and operating such a platform requires significant engineering expertise and resources, many organizations opt for a fully-managed Kafka service such as Confluent Cloud , rather than self-managing the platform, so that they can focus on creating business value.","title":"Considerations"},{"location":"event-stream/event-streaming-platform/#references","text":"This pattern is derived from Message Bus in Enterprise Integration Patterns , by Gregor Hohpe and Bobby Woolf. Confluent Cloud is a cloud-native service for Apache Kafka\u00ae. The Apache Kafka 101 course provides a primer on what Kafka is and how it works.","title":"References"},{"location":"event-stream/partitioned-parallelism/","text":"Partitioned Parallelism If service goals mandate high throughput, it is useful to be able to distribute event storage, as well as event production and consumption, for parallel processing. Distributing and concurrently processing events enables an application to scale. Problem How can I allocate events across Event Streams and Tables so that they can be concurrently processed by distributed Event Processors ? Solution Use a partitioned event stream, and then assign the events to different partitions of the stream. Essentially, a partition is a unit of parallelism for storing, reading, writing, and processing events. Partitioning enables concurrency and scalability in two main ways: Platform scalability: different Event Brokers can concurrently store and serve Events to Event Processing Applications Application scalability: different Event Processing Applications can process Events concurrently Event partitioning also impacts application semantics: placing events into a given partition guarantees that the ordering of events is preserved per partition (but typically not across different partitions of the same stream). This ordering guarantee is crucial for many use cases; very often, the sequencing of events is important (for example, when processing retail orders, an order must be paid before it can be shipped). Implementation With Apache Kafka\u00ae, streams (called topics ) are created either by an administrator or by a streaming application such as the streaming database ksqlDB . The number of partitions is specified at the time the topic is created. For example: ccloud kafka topic create myTopic --partitions 30 Events are placed into a specific partition according to the partitioning algorithm of the Event Source , such as an Event Processing Application . All events assigned to a given partition have strong ordering guarantees. The common partitioning schemes are: Partitioning based on the event key (such as the customer ID for a stream of customer payments), where events with the same key are stored in the same partition Round-robin partitioning, which provides an even distribution of events per partition Custom partitioning algorithms, tailored to specific use cases In a Kafka-based technology, such as a Kafka Streams application or ksqlDB , the processors can scale by working on a set of partitions concurrently and in a distributed manner. If an event stream's key content changes because of how the query is processing the rows -- for example, to execute a JOIN operation in ksqlDB between two streams of events -- the underlying keys are recalculated, and the events are sent to a new partition in the new topic to perform the computation. (This internal operation is often called distributed data shuffling .) CREATE STREAM stream_name WITH ([...,] PARTITIONS=number_of_partitions) AS SELECT select_expr [, ...] FROM from_stream PARTITION BY new_key_expr [, ...] EMIT CHANGES; Considerations In general, a higher number of stream partitions results in higher throughput. To maximize throughput, you need enough partitions to utilize all distributed instances of an Event Processor (for example, servers in a ksqlDB cluster). Be sure to choose the partition count carefully based on the throughput of Event Sources (such as Kafka producers, including connectors), Event Processors (such as ksqlDB or Kafka Streams applications), and Event Sinks (such as Kafka consumers, including connectors). Also be sure to benchmark performance in the environment. Plan the design of data patterns and key assignments so that events are distributed as evenly as possible across the stream partitions. This will prevent certain stream partitions from being overloaded relative to other stream partitions. See the blog post Streams and Tables in Apache Kafka: Elasticity, Fault Tolerance, and Other Advanced Concepts to learn more about partitions and dealing with partition skew. References The blog post How to choose the number of topics/partitions in a Kafka cluster? provides helpful guidance for selecting partition counts for your topics. For a processing parallelism approach that subdivides the unit of work from a partition down to an event or event key, see the Confluent Parallel Consumer for Kafka .","title":"Partitioned Parallelism"},{"location":"event-stream/partitioned-parallelism/#partitioned-parallelism","text":"If service goals mandate high throughput, it is useful to be able to distribute event storage, as well as event production and consumption, for parallel processing. Distributing and concurrently processing events enables an application to scale.","title":"Partitioned Parallelism"},{"location":"event-stream/partitioned-parallelism/#problem","text":"How can I allocate events across Event Streams and Tables so that they can be concurrently processed by distributed Event Processors ?","title":"Problem"},{"location":"event-stream/partitioned-parallelism/#solution","text":"Use a partitioned event stream, and then assign the events to different partitions of the stream. Essentially, a partition is a unit of parallelism for storing, reading, writing, and processing events. Partitioning enables concurrency and scalability in two main ways: Platform scalability: different Event Brokers can concurrently store and serve Events to Event Processing Applications Application scalability: different Event Processing Applications can process Events concurrently Event partitioning also impacts application semantics: placing events into a given partition guarantees that the ordering of events is preserved per partition (but typically not across different partitions of the same stream). This ordering guarantee is crucial for many use cases; very often, the sequencing of events is important (for example, when processing retail orders, an order must be paid before it can be shipped).","title":"Solution"},{"location":"event-stream/partitioned-parallelism/#implementation","text":"With Apache Kafka\u00ae, streams (called topics ) are created either by an administrator or by a streaming application such as the streaming database ksqlDB . The number of partitions is specified at the time the topic is created. For example: ccloud kafka topic create myTopic --partitions 30 Events are placed into a specific partition according to the partitioning algorithm of the Event Source , such as an Event Processing Application . All events assigned to a given partition have strong ordering guarantees. The common partitioning schemes are: Partitioning based on the event key (such as the customer ID for a stream of customer payments), where events with the same key are stored in the same partition Round-robin partitioning, which provides an even distribution of events per partition Custom partitioning algorithms, tailored to specific use cases In a Kafka-based technology, such as a Kafka Streams application or ksqlDB , the processors can scale by working on a set of partitions concurrently and in a distributed manner. If an event stream's key content changes because of how the query is processing the rows -- for example, to execute a JOIN operation in ksqlDB between two streams of events -- the underlying keys are recalculated, and the events are sent to a new partition in the new topic to perform the computation. (This internal operation is often called distributed data shuffling .) CREATE STREAM stream_name WITH ([...,] PARTITIONS=number_of_partitions) AS SELECT select_expr [, ...] FROM from_stream PARTITION BY new_key_expr [, ...] EMIT CHANGES;","title":"Implementation"},{"location":"event-stream/partitioned-parallelism/#considerations","text":"In general, a higher number of stream partitions results in higher throughput. To maximize throughput, you need enough partitions to utilize all distributed instances of an Event Processor (for example, servers in a ksqlDB cluster). Be sure to choose the partition count carefully based on the throughput of Event Sources (such as Kafka producers, including connectors), Event Processors (such as ksqlDB or Kafka Streams applications), and Event Sinks (such as Kafka consumers, including connectors). Also be sure to benchmark performance in the environment. Plan the design of data patterns and key assignments so that events are distributed as evenly as possible across the stream partitions. This will prevent certain stream partitions from being overloaded relative to other stream partitions. See the blog post Streams and Tables in Apache Kafka: Elasticity, Fault Tolerance, and Other Advanced Concepts to learn more about partitions and dealing with partition skew.","title":"Considerations"},{"location":"event-stream/partitioned-parallelism/#references","text":"The blog post How to choose the number of topics/partitions in a Kafka cluster? provides helpful guidance for selecting partition counts for your topics. For a processing parallelism approach that subdivides the unit of work from a partition down to an event or event key, see the Confluent Parallel Consumer for Kafka .","title":"References"},{"location":"event-stream/schema-compatibility/","text":"Schema Compatibility Schemas are like Data Contracts in that they set the terms that guarantee applications can process the data they receive. A natural behavior of applications and data schemas is that they evolve over time, so it's important to have a policy about how they are allowed to evolve and what compatibility rules are between old and new versions. Problem How do we ensure that schemas can evolve without breaking existing Event Sinks (readers) and Event Sources (writers), including Event Processing Applications ? Solution There are two types of compatibility to consider: backwards compatibility and forwards compatibility. Backwards compatibility ensures that newer readers can update their schema and still consume events written by older writers. The types of backwards compatible changes include: deletion of fields: old writers can continue to include this field, new readers ignore it addition of optional fields with a default value: old writers do not write this field, new readers use the default value Forwards compatibility ensures that newer writers can produce events with an updated schema that can still be read by older readers. The types of forwards compatible changes include: addition of fields: new writers include this field, old readers ignore it deletion of optional fields with a default value: new writers do not write this field, old readers use the default value Implementation Using Avro as the serialization format , if the original schema is: {\"namespace\": \"io.confluent.examples.client\", \"type\": \"record\", \"name\": \"Event\", \"fields\": [ {\"name\": \"field1\", \"type\": \"boolean\", \"default\": true}, {\"name\": \"field2\", \"type\": \"string\"} ] } Examples of compatible changes would be: Removal of a field that had a default : notice field1 is removed {\"namespace\": \"io.confluent.examples.client\", \"type\": \"record\", \"name\": \"Event\", \"fields\": [ {\"name\": \"field2\", \"type\": \"string\"} ] } Addition of a field with a default : notice field3 is added with a default value of 0. {\"namespace\": \"io.confluent.examples.client\", \"type\": \"record\", \"name\": \"Event\", \"fields\": [ {\"name\": \"field1\", \"type\": \"boolean\", \"default\": true}, {\"pame\": \"field2\", \"type\": \"string\"}, {\"pame\": \"field3\", \"type\": \"int\", \"default\": 0} ] } Considerations We can use a fully-managed Schema Registry service with built-in compatibility checking, so that we can centralize our schemas and check compatibility of new schema versions against previous versions. curl -X POST --data @filename.avsc https://<schema-registry>/<subject>/versions Once we updated our schemas and asserted the desired compatibility level, we must be thoughtful about the order of upgrading the applications that use them. In some cases we should upgrade writer applications first (Event Sources, i.e., consumers), in other cases we should upgrade reader applications first (Event Sinks and Event Processors, i.e., producers). See Schema Compatibility Types for more details. References Event Serializer : encode events so that they can be written to disk, transferred across the network, and generally preserved for future readers Schema-on-Read : enable the reader of events to determine which schema to apply to the Event that is processed Schema evolution and compatibility : backward, forward, full Working with schemas : creating, editing, comparing versions Maven plugin to test for schema compatibility during the development cycle","title":"Schema Compatibility"},{"location":"event-stream/schema-compatibility/#schema-compatibility","text":"Schemas are like Data Contracts in that they set the terms that guarantee applications can process the data they receive. A natural behavior of applications and data schemas is that they evolve over time, so it's important to have a policy about how they are allowed to evolve and what compatibility rules are between old and new versions.","title":"Schema Compatibility"},{"location":"event-stream/schema-compatibility/#problem","text":"How do we ensure that schemas can evolve without breaking existing Event Sinks (readers) and Event Sources (writers), including Event Processing Applications ?","title":"Problem"},{"location":"event-stream/schema-compatibility/#solution","text":"There are two types of compatibility to consider: backwards compatibility and forwards compatibility. Backwards compatibility ensures that newer readers can update their schema and still consume events written by older writers. The types of backwards compatible changes include: deletion of fields: old writers can continue to include this field, new readers ignore it addition of optional fields with a default value: old writers do not write this field, new readers use the default value Forwards compatibility ensures that newer writers can produce events with an updated schema that can still be read by older readers. The types of forwards compatible changes include: addition of fields: new writers include this field, old readers ignore it deletion of optional fields with a default value: new writers do not write this field, old readers use the default value","title":"Solution"},{"location":"event-stream/schema-compatibility/#implementation","text":"Using Avro as the serialization format , if the original schema is: {\"namespace\": \"io.confluent.examples.client\", \"type\": \"record\", \"name\": \"Event\", \"fields\": [ {\"name\": \"field1\", \"type\": \"boolean\", \"default\": true}, {\"name\": \"field2\", \"type\": \"string\"} ] } Examples of compatible changes would be: Removal of a field that had a default : notice field1 is removed {\"namespace\": \"io.confluent.examples.client\", \"type\": \"record\", \"name\": \"Event\", \"fields\": [ {\"name\": \"field2\", \"type\": \"string\"} ] } Addition of a field with a default : notice field3 is added with a default value of 0. {\"namespace\": \"io.confluent.examples.client\", \"type\": \"record\", \"name\": \"Event\", \"fields\": [ {\"name\": \"field1\", \"type\": \"boolean\", \"default\": true}, {\"pame\": \"field2\", \"type\": \"string\"}, {\"pame\": \"field3\", \"type\": \"int\", \"default\": 0} ] }","title":"Implementation"},{"location":"event-stream/schema-compatibility/#considerations","text":"We can use a fully-managed Schema Registry service with built-in compatibility checking, so that we can centralize our schemas and check compatibility of new schema versions against previous versions. curl -X POST --data @filename.avsc https://<schema-registry>/<subject>/versions Once we updated our schemas and asserted the desired compatibility level, we must be thoughtful about the order of upgrading the applications that use them. In some cases we should upgrade writer applications first (Event Sources, i.e., consumers), in other cases we should upgrade reader applications first (Event Sinks and Event Processors, i.e., producers). See Schema Compatibility Types for more details.","title":"Considerations"},{"location":"event-stream/schema-compatibility/#references","text":"Event Serializer : encode events so that they can be written to disk, transferred across the network, and generally preserved for future readers Schema-on-Read : enable the reader of events to determine which schema to apply to the Event that is processed Schema evolution and compatibility : backward, forward, full Working with schemas : creating, editing, comparing versions Maven plugin to test for schema compatibility during the development cycle","title":"References"},{"location":"event-stream/schema-evolution/","text":"Schema Evolution Schema Evolution is an important aspect of data management. Similar to how APIs evolve and need to be compatible with all applications that rely on older or newer API versions, schemas also evolve, and likewise need to be compatible with all applications that rely on older or newer schema versions. Problem How can I restructure or add new information to an event, ideally in a way that ensures Schema Compatibility ? Solution One approach in evolving a schema is to evolve the schema \"in place\" (as illustrated above). In this approach, a stream can contain events that use new and previous schema versions. The schema compatibility checks then ensure that Event Processing Applications and Event Sinks can read schemas in both formats. Another approach is to perform dual schema upgrades , also known as creating versioned streams (illustrated above). This approach is especially useful when you need to introduce breaking changes into a stream's schema(s) and the new schema will be incompatible with the previous schema. Here, Event Sources write to two streams: One stream that uses the previous schema version, such as payments-v1 . Another stream that uses the new schema version, such as payments-v2 . Event Processing Applications and Event Sinks then each consume from the stream that uses the schema version with which they are compatible. After upgrading all consumers to the new schema version, you can retire the stream that uses the old schema version. Implementation For \"in-place\" schema evolution, an Event Stream has multiple versions of the same schema, and the different versions are compatible with each other . For example, if a field is removed, then a consumer that was developed to process events without this field will be able to process events that were written with the old schema and contain the field; the consumer will simply ignore that field. In this case, if the original schema was as follows: {\"namespace\": \"io.confluent.examples.client\", \"type\": \"record\", \"name\": \"Event\", \"fields\": [ {\"name\": \"field1\", \"type\": \"boolean\", \"default\": true}, {\"name\": \"field2\", \"type\": \"string\"} ] } Then we could modify the schema to omit field2 , as shown below, and the event stream would then have a mix of both schema types. If new consumers process events written with the old schema, these consumers would ignore field2 . {\"namespace\": \"io.confluent.examples.client\", \"type\": \"record\", \"name\": \"Event\", \"fields\": [ {\"name\": \"field1\", \"type\": \"boolean\", \"default\": true} ] } Dual schema upgrades involve breaking changes, so the processing flow needs to be more explicit: clients must only write to and read from the Event Stream that corresponds to the schema that they can process. For example, an application that can process a v1 schema would read from one stream: CREATE STREAM payments-v1 (transaction_id BIGINT, username VARCHAR) WITH (kafka_topic='payments-v1'); And an application that can process a v2 schema would read from another stream: CREATE STREAM payments-v2 (transaction_id BIGINT, store_id VARCHAR) WITH (kafka_topic='payments-v2'); Considerations Follow Schema Compatibility rules to determine which schema changes are compatible and which are breaking changes. References See also the Event Serializer pattern, in which we encode events so that they can be written to disk, transferred across the network, and generally preserved for future readers See also the Schema-on-Read pattern, which enables the reader of events to determine which schema to apply to the Event that the reader is processing. Schema evolution and compatibility covers backward compatibility, forward compatibility, and full compatibility. Working with schemas covers how to create schemas, edit them, and compare versions. The Schema Registry Maven Plugin allows you to test for schema compatibility during the development cycle.","title":"Schema Evolution"},{"location":"event-stream/schema-evolution/#schema-evolution","text":"Schema Evolution is an important aspect of data management. Similar to how APIs evolve and need to be compatible with all applications that rely on older or newer API versions, schemas also evolve, and likewise need to be compatible with all applications that rely on older or newer schema versions.","title":"Schema Evolution"},{"location":"event-stream/schema-evolution/#problem","text":"How can I restructure or add new information to an event, ideally in a way that ensures Schema Compatibility ?","title":"Problem"},{"location":"event-stream/schema-evolution/#solution","text":"One approach in evolving a schema is to evolve the schema \"in place\" (as illustrated above). In this approach, a stream can contain events that use new and previous schema versions. The schema compatibility checks then ensure that Event Processing Applications and Event Sinks can read schemas in both formats. Another approach is to perform dual schema upgrades , also known as creating versioned streams (illustrated above). This approach is especially useful when you need to introduce breaking changes into a stream's schema(s) and the new schema will be incompatible with the previous schema. Here, Event Sources write to two streams: One stream that uses the previous schema version, such as payments-v1 . Another stream that uses the new schema version, such as payments-v2 . Event Processing Applications and Event Sinks then each consume from the stream that uses the schema version with which they are compatible. After upgrading all consumers to the new schema version, you can retire the stream that uses the old schema version.","title":"Solution"},{"location":"event-stream/schema-evolution/#implementation","text":"For \"in-place\" schema evolution, an Event Stream has multiple versions of the same schema, and the different versions are compatible with each other . For example, if a field is removed, then a consumer that was developed to process events without this field will be able to process events that were written with the old schema and contain the field; the consumer will simply ignore that field. In this case, if the original schema was as follows: {\"namespace\": \"io.confluent.examples.client\", \"type\": \"record\", \"name\": \"Event\", \"fields\": [ {\"name\": \"field1\", \"type\": \"boolean\", \"default\": true}, {\"name\": \"field2\", \"type\": \"string\"} ] } Then we could modify the schema to omit field2 , as shown below, and the event stream would then have a mix of both schema types. If new consumers process events written with the old schema, these consumers would ignore field2 . {\"namespace\": \"io.confluent.examples.client\", \"type\": \"record\", \"name\": \"Event\", \"fields\": [ {\"name\": \"field1\", \"type\": \"boolean\", \"default\": true} ] } Dual schema upgrades involve breaking changes, so the processing flow needs to be more explicit: clients must only write to and read from the Event Stream that corresponds to the schema that they can process. For example, an application that can process a v1 schema would read from one stream: CREATE STREAM payments-v1 (transaction_id BIGINT, username VARCHAR) WITH (kafka_topic='payments-v1'); And an application that can process a v2 schema would read from another stream: CREATE STREAM payments-v2 (transaction_id BIGINT, store_id VARCHAR) WITH (kafka_topic='payments-v2');","title":"Implementation"},{"location":"event-stream/schema-evolution/#considerations","text":"Follow Schema Compatibility rules to determine which schema changes are compatible and which are breaking changes.","title":"Considerations"},{"location":"event-stream/schema-evolution/#references","text":"See also the Event Serializer pattern, in which we encode events so that they can be written to disk, transferred across the network, and generally preserved for future readers See also the Schema-on-Read pattern, which enables the reader of events to determine which schema to apply to the Event that the reader is processing. Schema evolution and compatibility covers backward compatibility, forward compatibility, and full compatibility. Working with schemas covers how to create schemas, edit them, and compare versions. The Schema Registry Maven Plugin allows you to test for schema compatibility during the development cycle.","title":"References"},{"location":"event-stream/test/","text":"{ \"name\": \"elasticsearch-ksqldb\", \"config\": { \"connector.class\": \"io.confluent.connect.elasticsearch.ElasticsearchSinkConnector\", \"consumer.interceptor.classes\": \"io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor\", \"topics\": \"WIKIPEDIABOT\", \"topic.index.map\": \"WIKIPEDIABOT:wikipediabot\", \"connection.url\": \"http://elasticsearch:9200\", \"type.name\": \"_doc\", \"key.ignore\": true, \"key.converter.schema.registry.url\": \"https://schemaregistry:8085\", \"value.converter\": \"io.confluent.connect.avro.AvroConverter\", \"value.converter.schema.registry.url\": \"https://schemaregistry:8085\", \"value.converter.schema.registry.ssl.truststore.location\": \"/etc/kafka/secrets/kafka.client.truststore.jks\", \"value.converter.schema.registry.ssl.truststore.password\": \"confluent\", \"value.converter.basic.auth.credentials.source\": \"USER_INFO\", \"value.converter.basic.auth.user.info\": \"connectorSA:connectorSA\", \"consumer.override.sasl.jaas.config\": \"org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required username=\\\"connectorSA\\\" password=\\\"connectorSA\\\" metadataServerUrls=\\\"https://kafka1:8091,https://kafka2:8092\\\";\", \"schema.ignore\": true } }","title":"Test"},{"location":"stream-processing/event-aggregator/","text":"Event Aggregator Combining multiple events into a single encompassing event\u2014e.g., to compute totals, averages, etc.\u2014is a common task in event streaming and streaming analytics. Problem How can multiple related events be aggregated to produce a new event? Solution We use an Event Grouper followed by an event aggregator. The grouper prepares the input events as needed for the subsequent aggregation step, e.g. by grouping the events based on the data field by which the aggregation is computed (such as a customer ID) and/or by grouping the events into time windows (such as 5-minute windows). The aggregator then computes the desired aggregation for each group of events, e.g., by computing the average or sum of each 5-minute window. Implementation For example, we can use the streaming database ksqlDB and Apache Kafka\u00ae to perform an aggregation. We'll start by creating a stream in ksqlDB called orders , based on an existing Kafka topic of the same name: CREATE STREAM orders (order_id INT, item_id INT, total_units DOUBLE) WITH (KAFKA_TOPIC='orders', VALUE_FORMAT='AVRO'); Then we'll create a table containing the aggregated events from that stream: CREATE TABLE item_stats AS SELECT item_id, COUNT(*) AS total_orders, AVG(total_units) AS avg_units FROM orders WINDOW TUMBLING (SIZE 1 HOUR) GROUP BY item_id EMIT CHANGES; This table will be continuously updated whenever new events arrive in the orders stream. Considerations In event streaming, a key technical challenge is that\u2014with few exceptions\u2014it is generally not possible to tell whether the input data is \"complete\" at a given point in time. For this reason, stream processing technologies such as the streaming database ksqlDB and the Kafka Streams client library of Apache Kafka employ techniques such as slack time 1 and grace periods (see GRACE PERIOD clause in ksqlDB) or watermarks to define cutoff points after which an Event Processor will discard any late-arriving input events from its processing. See the Suppressed Event Aggregator pattern for additional information. References Related patterns: Suppressed Event Aggregator More detailed examples of event aggregation can be seen in the following tutorials: How to sum a stream of events and How to count a stream of events . Footnotes 1 Slack time: Beyond Analytics: The Evolution of Stream Processing Systems (SIGMOD 2020) , Aurora: a new model and architecture for data stream management (VLDB Journal 2003)","title":"Event Aggregator"},{"location":"stream-processing/event-aggregator/#event-aggregator","text":"Combining multiple events into a single encompassing event\u2014e.g., to compute totals, averages, etc.\u2014is a common task in event streaming and streaming analytics.","title":"Event Aggregator"},{"location":"stream-processing/event-aggregator/#problem","text":"How can multiple related events be aggregated to produce a new event?","title":"Problem"},{"location":"stream-processing/event-aggregator/#solution","text":"We use an Event Grouper followed by an event aggregator. The grouper prepares the input events as needed for the subsequent aggregation step, e.g. by grouping the events based on the data field by which the aggregation is computed (such as a customer ID) and/or by grouping the events into time windows (such as 5-minute windows). The aggregator then computes the desired aggregation for each group of events, e.g., by computing the average or sum of each 5-minute window.","title":"Solution"},{"location":"stream-processing/event-aggregator/#implementation","text":"For example, we can use the streaming database ksqlDB and Apache Kafka\u00ae to perform an aggregation. We'll start by creating a stream in ksqlDB called orders , based on an existing Kafka topic of the same name: CREATE STREAM orders (order_id INT, item_id INT, total_units DOUBLE) WITH (KAFKA_TOPIC='orders', VALUE_FORMAT='AVRO'); Then we'll create a table containing the aggregated events from that stream: CREATE TABLE item_stats AS SELECT item_id, COUNT(*) AS total_orders, AVG(total_units) AS avg_units FROM orders WINDOW TUMBLING (SIZE 1 HOUR) GROUP BY item_id EMIT CHANGES; This table will be continuously updated whenever new events arrive in the orders stream.","title":"Implementation"},{"location":"stream-processing/event-aggregator/#considerations","text":"In event streaming, a key technical challenge is that\u2014with few exceptions\u2014it is generally not possible to tell whether the input data is \"complete\" at a given point in time. For this reason, stream processing technologies such as the streaming database ksqlDB and the Kafka Streams client library of Apache Kafka employ techniques such as slack time 1 and grace periods (see GRACE PERIOD clause in ksqlDB) or watermarks to define cutoff points after which an Event Processor will discard any late-arriving input events from its processing. See the Suppressed Event Aggregator pattern for additional information.","title":"Considerations"},{"location":"stream-processing/event-aggregator/#references","text":"Related patterns: Suppressed Event Aggregator More detailed examples of event aggregation can be seen in the following tutorials: How to sum a stream of events and How to count a stream of events .","title":"References"},{"location":"stream-processing/event-aggregator/#footnotes","text":"1 Slack time: Beyond Analytics: The Evolution of Stream Processing Systems (SIGMOD 2020) , Aurora: a new model and architecture for data stream management (VLDB Journal 2003)","title":"Footnotes"},{"location":"stream-processing/event-grouper/","text":"Event Grouper An event grouper is a specialized form of an Event Processor that groups events together by a common field, such as a customer ID, and/or by event timestamps (often called windowing or time-based windowing ). Problem How can we group individual but related events from the same Event Stream or Table , so that they can subsequently be processed as a whole? Solution For time-based grouping a.k.a. time-based windowing , we use an Event Processor that groups the related events into windows based on their event timestamps. Most window types have a pre-defined window size, such as 10 minutes or 24 hours. An exception is session windows, where the size of each window varies depending on the time characteristics of the grouped events. For field-based grouping, we use an Event Processor that groups events by one or more data fields, irrespective of the event timestamps. The two grouping approaches are orthogonal and can be composed. For example, to compute 7-day averages for every customer in a stream of payments, we first group the events in the stream by customer ID and by 7-day windows, and then compute the respective averages for each customer+window grouping. Implementation As an example, the streaming database ksqlDB provides the capability to group related events by a column and group them into \"windows\" where all the related events have a timestamp within the defined time-window. SELECT product-name, COUNT(*), SUM(price) FROM purchases WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY product-name EMIT CHANGES; Considerations When grouping events into time windows, there are various types of groupings possible. Hopping Windows are based on time intervals. They model fixed-sized, possibly overlapping windows. A hopping window is defined by two properties: the window's duration and its advance or \"hop\", interval. Tumbling Windows are a special case of hopping windows. Like hopping windows, tumbling windows are based on time intervals. They model fixed-size, non-overlapping, gap-less windows. A tumbling window is defined by a single property: the window's duration. Session Windows aggregate events into a session, which represents a period of activity separated by a specified gap of inactivity, or \"idleness\". Any records with timestamps that occur within the inactivity gap of existing sessions are merged into the existing session. If a record's timestamp occurs outside of the session gap, a new session is created. See the ksqlDB supported window types and the Kafka Streams supported window types for details and diagrams explaining window types. References The Tumbling Windows in ksqlDB and the Tumbling Windows in Kafka Streams tutorials provide an end-to-end example for calculating an aggregate calculation over a window of events. The Tumbling Windows in ksqlDB and the Tumbling Windows in Kafka Streams tutorials provide an end-to-end example for calculating an aggregate calculation over a window of events. Related full tutorials are Session Windows in ksqlDB and Session Windows in Kafka Streams , as well as Hopping Windows in ksqlDB .","title":"Event Grouper"},{"location":"stream-processing/event-grouper/#event-grouper","text":"An event grouper is a specialized form of an Event Processor that groups events together by a common field, such as a customer ID, and/or by event timestamps (often called windowing or time-based windowing ).","title":"Event Grouper"},{"location":"stream-processing/event-grouper/#problem","text":"How can we group individual but related events from the same Event Stream or Table , so that they can subsequently be processed as a whole?","title":"Problem"},{"location":"stream-processing/event-grouper/#solution","text":"For time-based grouping a.k.a. time-based windowing , we use an Event Processor that groups the related events into windows based on their event timestamps. Most window types have a pre-defined window size, such as 10 minutes or 24 hours. An exception is session windows, where the size of each window varies depending on the time characteristics of the grouped events. For field-based grouping, we use an Event Processor that groups events by one or more data fields, irrespective of the event timestamps. The two grouping approaches are orthogonal and can be composed. For example, to compute 7-day averages for every customer in a stream of payments, we first group the events in the stream by customer ID and by 7-day windows, and then compute the respective averages for each customer+window grouping.","title":"Solution"},{"location":"stream-processing/event-grouper/#implementation","text":"As an example, the streaming database ksqlDB provides the capability to group related events by a column and group them into \"windows\" where all the related events have a timestamp within the defined time-window. SELECT product-name, COUNT(*), SUM(price) FROM purchases WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY product-name EMIT CHANGES;","title":"Implementation"},{"location":"stream-processing/event-grouper/#considerations","text":"When grouping events into time windows, there are various types of groupings possible. Hopping Windows are based on time intervals. They model fixed-sized, possibly overlapping windows. A hopping window is defined by two properties: the window's duration and its advance or \"hop\", interval. Tumbling Windows are a special case of hopping windows. Like hopping windows, tumbling windows are based on time intervals. They model fixed-size, non-overlapping, gap-less windows. A tumbling window is defined by a single property: the window's duration. Session Windows aggregate events into a session, which represents a period of activity separated by a specified gap of inactivity, or \"idleness\". Any records with timestamps that occur within the inactivity gap of existing sessions are merged into the existing session. If a record's timestamp occurs outside of the session gap, a new session is created. See the ksqlDB supported window types and the Kafka Streams supported window types for details and diagrams explaining window types.","title":"Considerations"},{"location":"stream-processing/event-grouper/#references","text":"The Tumbling Windows in ksqlDB and the Tumbling Windows in Kafka Streams tutorials provide an end-to-end example for calculating an aggregate calculation over a window of events. The Tumbling Windows in ksqlDB and the Tumbling Windows in Kafka Streams tutorials provide an end-to-end example for calculating an aggregate calculation over a window of events. Related full tutorials are Session Windows in ksqlDB and Session Windows in Kafka Streams , as well as Hopping Windows in ksqlDB .","title":"References"},{"location":"stream-processing/event-joiner/","text":"Event Joiner Event Streams may need to be joined (i.e. enriched) with a Table or another Event Stream in order to provide more comprehensive details about their Events . Problem How can I enrich an Event Stream or Table with additional context? Solution We can combine Events in a stream with a Table or another Event Stream by performing a join between the two. The join is based on a key shared by the original Event Stream and the other Event Stream or Table. We can also provide a window buffering mechanism based on timestamps, so that we can produce join results when Events from both Event Streams aren't immediately available. Another approach is to join an Event Stream and a Table that contains more static data, resulting in an enriched Event Stream. Implementation With the streaming database ksqlDB , we can create a stream of Events from an existing Kafka topic (in this example, note the similarity to fact tables in data warehouses): CREATE STREAM ratings (MOVIE_ID INT KEY, rating DOUBLE) WITH (KAFKA_TOPIC='ratings'); We can then create a Table from another existing Kafka topic that changes less frequently. This Table serves as our reference data (similar to dimension tables in data warehouses). CREATE TABLE movies (ID INT PRIMARY KEY, title VARCHAR, release_year INT) WITH (KAFKA_TOPIC='movies'); To create a stream of enriched Events, we perform a join between the Event Stream and the Table. SELECT ratings.movie_id AS ID, title, release_year, rating FROM ratings LEFT JOIN movies ON ratings.movie_id = movies.id EMIT CHANGES; Considerations In ksqlDB, joins between an Event Stream and a Table are driven by the Event Stream side of the join. Updates to the Table only update the state of the Table. Only a new Event in the Event Stream will cause a new join result. For example, if we're joining an Event Stream of orders to a Table of customers, a new order will be enriched if there is a customer record in the Table. But if a new customer is added to the Table, that will not trigger the join condition. The ksqlDB documentation contains more information about stream-table join semantics . We can perform an inner or left-outer join between an Event Stream and a Table. Joins are also useful for initiating subsequent processing when two or more corresponding Events arrive on different Event Streams or Tables. References How to join a stream and a lookup table in ksqlDB Joining a stream and a stream in ksqlDB How to join a table and a table in ksqlDB Performing N-way joins in ksqlDB Joining collections in the ksqlDB documentation","title":"Event Joiner"},{"location":"stream-processing/event-joiner/#event-joiner","text":"Event Streams may need to be joined (i.e. enriched) with a Table or another Event Stream in order to provide more comprehensive details about their Events .","title":"Event Joiner"},{"location":"stream-processing/event-joiner/#problem","text":"How can I enrich an Event Stream or Table with additional context?","title":"Problem"},{"location":"stream-processing/event-joiner/#solution","text":"We can combine Events in a stream with a Table or another Event Stream by performing a join between the two. The join is based on a key shared by the original Event Stream and the other Event Stream or Table. We can also provide a window buffering mechanism based on timestamps, so that we can produce join results when Events from both Event Streams aren't immediately available. Another approach is to join an Event Stream and a Table that contains more static data, resulting in an enriched Event Stream.","title":"Solution"},{"location":"stream-processing/event-joiner/#implementation","text":"With the streaming database ksqlDB , we can create a stream of Events from an existing Kafka topic (in this example, note the similarity to fact tables in data warehouses): CREATE STREAM ratings (MOVIE_ID INT KEY, rating DOUBLE) WITH (KAFKA_TOPIC='ratings'); We can then create a Table from another existing Kafka topic that changes less frequently. This Table serves as our reference data (similar to dimension tables in data warehouses). CREATE TABLE movies (ID INT PRIMARY KEY, title VARCHAR, release_year INT) WITH (KAFKA_TOPIC='movies'); To create a stream of enriched Events, we perform a join between the Event Stream and the Table. SELECT ratings.movie_id AS ID, title, release_year, rating FROM ratings LEFT JOIN movies ON ratings.movie_id = movies.id EMIT CHANGES;","title":"Implementation"},{"location":"stream-processing/event-joiner/#considerations","text":"In ksqlDB, joins between an Event Stream and a Table are driven by the Event Stream side of the join. Updates to the Table only update the state of the Table. Only a new Event in the Event Stream will cause a new join result. For example, if we're joining an Event Stream of orders to a Table of customers, a new order will be enriched if there is a customer record in the Table. But if a new customer is added to the Table, that will not trigger the join condition. The ksqlDB documentation contains more information about stream-table join semantics . We can perform an inner or left-outer join between an Event Stream and a Table. Joins are also useful for initiating subsequent processing when two or more corresponding Events arrive on different Event Streams or Tables.","title":"Considerations"},{"location":"stream-processing/event-joiner/#references","text":"How to join a stream and a lookup table in ksqlDB Joining a stream and a stream in ksqlDB How to join a table and a table in ksqlDB Performing N-way joins in ksqlDB Joining collections in the ksqlDB documentation","title":"References"},{"location":"stream-processing/event-stream-merger/","text":"Event Stream Merger An Event Streaming Application may contain multiple Event Stream instances. But in some cases, it may make sense for the application to merge the separate Event Streams into a single Event Stream, without changing the individual Events. While this may seem logically related to a join, this merge is a completely different operation. A join produces results by combining Events with the same key to produce a new Event, possibly of a different type. A merge of Event Streams combines the Events from multiple Event Streams into a single Event Stream, but the individual Events are unchanged and remain independent of each other. Problem How can an application merge separate Event Streams? Solution Implementation For Apache Kafka\u00ae, the Kafka Streams client library provides a merge operator in its DSL. This operator merges two Event Streams into a single Event Stream. We can then take the merged stream and perform any number of operations on it. KStream<String, Event> eventStream = builder.stream(...); KStream<String, Event> eventStreamII = builder.stream(...); KStream<String, Event> allEventsStream = eventStream.merge(eventStreamII); allEventsStream.groupByKey()... Considerations Kafka Streams provides no guarantees on the processing order of records from the underlying Event Streams. In order for multiple Event Streams to be merged, they must use the same key and value types. References How to merge many streams into one stream with Kafka Streams How to merge many streams into one stream with ksqlDB","title":"Event Stream Merger"},{"location":"stream-processing/event-stream-merger/#event-stream-merger","text":"An Event Streaming Application may contain multiple Event Stream instances. But in some cases, it may make sense for the application to merge the separate Event Streams into a single Event Stream, without changing the individual Events. While this may seem logically related to a join, this merge is a completely different operation. A join produces results by combining Events with the same key to produce a new Event, possibly of a different type. A merge of Event Streams combines the Events from multiple Event Streams into a single Event Stream, but the individual Events are unchanged and remain independent of each other.","title":"Event Stream Merger"},{"location":"stream-processing/event-stream-merger/#problem","text":"How can an application merge separate Event Streams?","title":"Problem"},{"location":"stream-processing/event-stream-merger/#solution","text":"","title":"Solution"},{"location":"stream-processing/event-stream-merger/#implementation","text":"For Apache Kafka\u00ae, the Kafka Streams client library provides a merge operator in its DSL. This operator merges two Event Streams into a single Event Stream. We can then take the merged stream and perform any number of operations on it. KStream<String, Event> eventStream = builder.stream(...); KStream<String, Event> eventStreamII = builder.stream(...); KStream<String, Event> allEventsStream = eventStream.merge(eventStreamII); allEventsStream.groupByKey()...","title":"Implementation"},{"location":"stream-processing/event-stream-merger/#considerations","text":"Kafka Streams provides no guarantees on the processing order of records from the underlying Event Streams. In order for multiple Event Streams to be merged, they must use the same key and value types.","title":"Considerations"},{"location":"stream-processing/event-stream-merger/#references","text":"How to merge many streams into one stream with Kafka Streams How to merge many streams into one stream with ksqlDB","title":"References"},{"location":"stream-processing/event-time-processing/","text":"Event-Time Processing Consistent time semantics are of particular importance in stream processing. Many operations in an Event Processor are dependent on time, such as joins, aggregations when computed over a window of time (for example, five-minute averages), and handling out-of-order and \"late\" data. In many systems, developers have a choice between different variants of time for an Event: Event-time, which captures the time at which an Event was originally created by its Event Source . Ingestion-time, which captures the time at which an Event was received on the Event Stream in an Event Streaming Platform . Wall-clock-time or processing-time, which is the time at which a downstream Event Processor happens to process the Event (potentially milliseconds, hours, months, or more after event-time). Depending on the use case, developers need to pick one variant over the others. Problem How can we implement event-time based processing of Events (i.e., processing based on each Event's original timeline)? Solution For event-time processing, the Event Source must include a timestamp in each Event (for example, in a data field or in header metadata) that denotes the time at which the Event was created by the Event Source. Then, on the consuming side, the Event Processing Application needs to extract this timestamp from the Event. This allows the application to process Events based on their original timeline. Implementation ksqlDB In the streaming database ksqlDB , every Event (or record) has a system column named ROWTIME , which represents the timestamp for the Event. This defaults to the time at which the Event was originally created by its Event Source . For example, when we create a ksqlDB STREAM or TABLE from an existing Apache Kafka\u00ae topic, then the timestamp embedded in a Kafka message is extracted and assigned to the Event in ksqlDB. (See also the CreateTime of a Kafka ProducerRecord , and the Kafka message format .) Sometimes, this default behavior of ksqlDB is not what we want. Maybe the Events have a custom data field containing their actual timestamps (for example, some legacy data that has been around for a while was ingested into Kafka only recently, so we can't trust the CreateTime information in the Kafka messages because they are much newer than the original timestamps). To use a timestamp in the Event payload itself, we can add a WITH(TIMESTAMP='some-field') clause when creating a stream or table. This instructs ksqlDB to get the timestamp from the specified field in the record. CREATE STREAM my_event_stream WITH (kafka_topic='events', timestamp='eventTime'); Kafka Streams The Kafka Streams client library of Apache Kafka provides a TimestampExtractor interface for extracting the timestamp from Events. The default implementation retrieves the timestamp from the Kafka message (see the discussion above) as set by the producer of the message. Normally, this setup results in event-time processing, which is what we want. But for those cases where we need to get the timestamp from the event payload, we can create our own TimestampExtractor implementation: class OrderTimestampExtractor implements TimestampExtractor { @Override public long extract(ConsumerRecord<Object, Object> record, long partitionTime) { ElectronicOrder order = (ElectronicOrder)record.value(); return order.getTime(); } Generally speaking, this functionality of custom timestamp assignment makes it easy to integrate data from other applications that are not using Kafka Streams or ksqlDB themselves. Additionally, Kafka has the notion of event-time vs. processing-time (wall-clock-time) vs. ingestion time, similar to ksqlDB. Clients such as Kafka Streams make it possible to select which variant of time we want to work with in our application. Considerations When deciding which time semantics to use, we need to consider the problem domain. In most cases, event-time processing is the recommended option. For example, when re-processing historical Event Streams (such as for A/B testing, for training machine learning models), only event-time processing yields correct results. If we use processing-time (wall-clock time) to process the last four weeks' worth of Events, then an Event Processor will falsely believe that these four weeks of data were created just now in a matter of minutes, which completely breaks the original timeline and temporal distribution of the data and thus leads to incorrect processing results. The difference between event-time and ingestion-time is typically less pronounced, but ingestion-time still suffers from the same conceptual discrepancy between when an Event actually occurred in the real world (event-time) vs. when the Event was received and stored in the Event Streaming Platform (ingestion-time). If, for some reason, there is a significant delay between when an Event is captured and when it is delivered to the Event Streaming Platform, then event-time is the better option. One reason not to use event-time is if we cannot trust the Event Source to provide us with reliable data, including reliable embedded timestamps for Events. In this case, ingestion-time may be the preferred option, if it is not feasible to fix the root cause (unreliable Event sources). References See also the Wall-Clock-Time Processing pattern, which provides further details about using current time (wall-clock time) as the event time. Timestamp assignment in ksqlDB See the tutorial Event-time semantics in ksqlDB for further details about time concepts.","title":"Event-Time Processing"},{"location":"stream-processing/event-time-processing/#event-time-processing","text":"Consistent time semantics are of particular importance in stream processing. Many operations in an Event Processor are dependent on time, such as joins, aggregations when computed over a window of time (for example, five-minute averages), and handling out-of-order and \"late\" data. In many systems, developers have a choice between different variants of time for an Event: Event-time, which captures the time at which an Event was originally created by its Event Source . Ingestion-time, which captures the time at which an Event was received on the Event Stream in an Event Streaming Platform . Wall-clock-time or processing-time, which is the time at which a downstream Event Processor happens to process the Event (potentially milliseconds, hours, months, or more after event-time). Depending on the use case, developers need to pick one variant over the others.","title":"Event-Time Processing"},{"location":"stream-processing/event-time-processing/#problem","text":"How can we implement event-time based processing of Events (i.e., processing based on each Event's original timeline)?","title":"Problem"},{"location":"stream-processing/event-time-processing/#solution","text":"For event-time processing, the Event Source must include a timestamp in each Event (for example, in a data field or in header metadata) that denotes the time at which the Event was created by the Event Source. Then, on the consuming side, the Event Processing Application needs to extract this timestamp from the Event. This allows the application to process Events based on their original timeline.","title":"Solution"},{"location":"stream-processing/event-time-processing/#implementation","text":"","title":"Implementation"},{"location":"stream-processing/event-time-processing/#ksqldb","text":"In the streaming database ksqlDB , every Event (or record) has a system column named ROWTIME , which represents the timestamp for the Event. This defaults to the time at which the Event was originally created by its Event Source . For example, when we create a ksqlDB STREAM or TABLE from an existing Apache Kafka\u00ae topic, then the timestamp embedded in a Kafka message is extracted and assigned to the Event in ksqlDB. (See also the CreateTime of a Kafka ProducerRecord , and the Kafka message format .) Sometimes, this default behavior of ksqlDB is not what we want. Maybe the Events have a custom data field containing their actual timestamps (for example, some legacy data that has been around for a while was ingested into Kafka only recently, so we can't trust the CreateTime information in the Kafka messages because they are much newer than the original timestamps). To use a timestamp in the Event payload itself, we can add a WITH(TIMESTAMP='some-field') clause when creating a stream or table. This instructs ksqlDB to get the timestamp from the specified field in the record. CREATE STREAM my_event_stream WITH (kafka_topic='events', timestamp='eventTime');","title":"ksqlDB"},{"location":"stream-processing/event-time-processing/#kafka-streams","text":"The Kafka Streams client library of Apache Kafka provides a TimestampExtractor interface for extracting the timestamp from Events. The default implementation retrieves the timestamp from the Kafka message (see the discussion above) as set by the producer of the message. Normally, this setup results in event-time processing, which is what we want. But for those cases where we need to get the timestamp from the event payload, we can create our own TimestampExtractor implementation: class OrderTimestampExtractor implements TimestampExtractor { @Override public long extract(ConsumerRecord<Object, Object> record, long partitionTime) { ElectronicOrder order = (ElectronicOrder)record.value(); return order.getTime(); } Generally speaking, this functionality of custom timestamp assignment makes it easy to integrate data from other applications that are not using Kafka Streams or ksqlDB themselves. Additionally, Kafka has the notion of event-time vs. processing-time (wall-clock-time) vs. ingestion time, similar to ksqlDB. Clients such as Kafka Streams make it possible to select which variant of time we want to work with in our application.","title":"Kafka Streams"},{"location":"stream-processing/event-time-processing/#considerations","text":"When deciding which time semantics to use, we need to consider the problem domain. In most cases, event-time processing is the recommended option. For example, when re-processing historical Event Streams (such as for A/B testing, for training machine learning models), only event-time processing yields correct results. If we use processing-time (wall-clock time) to process the last four weeks' worth of Events, then an Event Processor will falsely believe that these four weeks of data were created just now in a matter of minutes, which completely breaks the original timeline and temporal distribution of the data and thus leads to incorrect processing results. The difference between event-time and ingestion-time is typically less pronounced, but ingestion-time still suffers from the same conceptual discrepancy between when an Event actually occurred in the real world (event-time) vs. when the Event was received and stored in the Event Streaming Platform (ingestion-time). If, for some reason, there is a significant delay between when an Event is captured and when it is delivered to the Event Streaming Platform, then event-time is the better option. One reason not to use event-time is if we cannot trust the Event Source to provide us with reliable data, including reliable embedded timestamps for Events. In this case, ingestion-time may be the preferred option, if it is not feasible to fix the root cause (unreliable Event sources).","title":"Considerations"},{"location":"stream-processing/event-time-processing/#references","text":"See also the Wall-Clock-Time Processing pattern, which provides further details about using current time (wall-clock time) as the event time. Timestamp assignment in ksqlDB See the tutorial Event-time semantics in ksqlDB for further details about time concepts.","title":"References"},{"location":"stream-processing/logical-and/","text":"Logical AND Event Streams become more interesting when they're considered together. Often, when two separate Events occur, that triggers a new fact that we want to capture. A product can only be dispatched when there's an order and a successful payment. If someone places a bet and their horse wins, then we transfer money to them. How can we combine information from several different Event Streams and use it to create new Events? Problem How can an application trigger processing when two or more related Events arrive on different Event Streams? Solution Multiple streams of Events can be joined together, similar to joins in a relational database. We watch the Event Streams and remember their most recent Events (for example, via an in-memory cache, or a local or network storage device) for a certain amount of time. Whenever a new Event arrives, we consider it alongside the other recently-captured Events, and look for matches. If we find one, we emit a new Event. For stream-stream joins, it's important to think about what we consider a \"recent\" Event. We can't join brand new Events with arbitrarily-old ones; to join potentially-infinite streams would require potentially-infinite memory. Instead, we decide on a retention period that counts as \"new enough\", and only hold on to Events during that period. This is often just fine -- for example, a payment will usually happen soon after an order is placed. If it doesn't go through within the hour, we can reasonably expect a different process to chase the user for updated payment details. Implementation As an example, imagine a bank that captures logins to its website, and withdrawals from an ATM. The fraud department might be keen to hear if the same user_id logs in to the website in one country, and makes a withdrawal in a different country, within the same day. (This would not necessarily be fraud, but it's certainly suspicious!) To implement this example, we'll use the streaming database ksqlDB . We start with two Event Streams: -- For simplicity's sake, we'll assume that IP addresses -- have already been converted into country codes. CREATE OR REPLACE STREAM logins ( user_id BIGINT, country_code VARCHAR ) WITH ( KAFKA_TOPIC = 'logins_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 3 ); CREATE OR REPLACE STREAM withdrawals ( user_id BIGINT, country_code VARCHAR, amount DECIMAL(10,2), success BOOLEAN ) WITH ( KAFKA_TOPIC = 'withdrawals_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 3 ); We can now join these two Event Streams. Events with the same user_id are considered equal, and we will specifically look at Events that happen WITHIN 1 DAY : CREATE STREAM possible_frauds AS SELECT l.user_id, l.country_code, w.country_code, w.amount, w.success FROM logins l JOIN withdrawals w WITHIN 1 DAY ON l.user_id = w.user_id WHERE l.country_code != w.country_code EMIT CHANGES; Query that stream in one terminal: SELECT * FROM possible_frauds EMIT CHANGES; Insert some data in another terminal: INSERT INTO logins (user_id, country_code) VALUES (1, 'gb'); INSERT INTO logins (user_id, country_code) VALUES (2, 'us'); INSERT INTO logins (user_id, country_code) VALUES (3, 'be'); INSERT INTO logins (user_id, country_code) VALUES (2, 'us'); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (1, 'gb', 10.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (1, 'au', 250.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (2, 'us', 50.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (3, 'be', 20.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (2, 'fr', 20.00, true); This produces a stream of possible fraud cases that need further investigation: +-----------+----------------+----------------+--------+---------+ |L_USER_ID |L_COUNTRY_CODE |W_COUNTRY_CODE |AMOUNT |SUCCESS | +-----------+----------------+----------------+--------+---------+ |1 |gb |au |250.00 |true | |2 |us |fr |20.00 |true | |2 |us |fr |20.00 |true | Considerations Joining Event Streams is fairly simple. The big consideration is how long of a retention period we need, and by extension, the amount of resources that our join will use. Planning that tradeoff requires careful consideration of the specific problem that we're solving. For long retention periods, consider joining an Event Stream to a Projection Table instead. References Joining Streams and Tables in the ksqlDB documentation. See also the Pipeline pattern, used for considering Events in series (rather than in parallel). See also the Projection Table pattern, a memory-efficient way of considering an Event Stream over a potentially-infinite time period. See chapter 14, \"Kafka Streams and KSQL\", of Designing Event-Driven Systems for further discussion.","title":"Logical AND"},{"location":"stream-processing/logical-and/#logical-and","text":"Event Streams become more interesting when they're considered together. Often, when two separate Events occur, that triggers a new fact that we want to capture. A product can only be dispatched when there's an order and a successful payment. If someone places a bet and their horse wins, then we transfer money to them. How can we combine information from several different Event Streams and use it to create new Events?","title":"Logical AND"},{"location":"stream-processing/logical-and/#problem","text":"How can an application trigger processing when two or more related Events arrive on different Event Streams?","title":"Problem"},{"location":"stream-processing/logical-and/#solution","text":"Multiple streams of Events can be joined together, similar to joins in a relational database. We watch the Event Streams and remember their most recent Events (for example, via an in-memory cache, or a local or network storage device) for a certain amount of time. Whenever a new Event arrives, we consider it alongside the other recently-captured Events, and look for matches. If we find one, we emit a new Event. For stream-stream joins, it's important to think about what we consider a \"recent\" Event. We can't join brand new Events with arbitrarily-old ones; to join potentially-infinite streams would require potentially-infinite memory. Instead, we decide on a retention period that counts as \"new enough\", and only hold on to Events during that period. This is often just fine -- for example, a payment will usually happen soon after an order is placed. If it doesn't go through within the hour, we can reasonably expect a different process to chase the user for updated payment details.","title":"Solution"},{"location":"stream-processing/logical-and/#implementation","text":"As an example, imagine a bank that captures logins to its website, and withdrawals from an ATM. The fraud department might be keen to hear if the same user_id logs in to the website in one country, and makes a withdrawal in a different country, within the same day. (This would not necessarily be fraud, but it's certainly suspicious!) To implement this example, we'll use the streaming database ksqlDB . We start with two Event Streams: -- For simplicity's sake, we'll assume that IP addresses -- have already been converted into country codes. CREATE OR REPLACE STREAM logins ( user_id BIGINT, country_code VARCHAR ) WITH ( KAFKA_TOPIC = 'logins_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 3 ); CREATE OR REPLACE STREAM withdrawals ( user_id BIGINT, country_code VARCHAR, amount DECIMAL(10,2), success BOOLEAN ) WITH ( KAFKA_TOPIC = 'withdrawals_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 3 ); We can now join these two Event Streams. Events with the same user_id are considered equal, and we will specifically look at Events that happen WITHIN 1 DAY : CREATE STREAM possible_frauds AS SELECT l.user_id, l.country_code, w.country_code, w.amount, w.success FROM logins l JOIN withdrawals w WITHIN 1 DAY ON l.user_id = w.user_id WHERE l.country_code != w.country_code EMIT CHANGES; Query that stream in one terminal: SELECT * FROM possible_frauds EMIT CHANGES; Insert some data in another terminal: INSERT INTO logins (user_id, country_code) VALUES (1, 'gb'); INSERT INTO logins (user_id, country_code) VALUES (2, 'us'); INSERT INTO logins (user_id, country_code) VALUES (3, 'be'); INSERT INTO logins (user_id, country_code) VALUES (2, 'us'); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (1, 'gb', 10.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (1, 'au', 250.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (2, 'us', 50.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (3, 'be', 20.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (2, 'fr', 20.00, true); This produces a stream of possible fraud cases that need further investigation: +-----------+----------------+----------------+--------+---------+ |L_USER_ID |L_COUNTRY_CODE |W_COUNTRY_CODE |AMOUNT |SUCCESS | +-----------+----------------+----------------+--------+---------+ |1 |gb |au |250.00 |true | |2 |us |fr |20.00 |true | |2 |us |fr |20.00 |true |","title":"Implementation"},{"location":"stream-processing/logical-and/#considerations","text":"Joining Event Streams is fairly simple. The big consideration is how long of a retention period we need, and by extension, the amount of resources that our join will use. Planning that tradeoff requires careful consideration of the specific problem that we're solving. For long retention periods, consider joining an Event Stream to a Projection Table instead.","title":"Considerations"},{"location":"stream-processing/logical-and/#references","text":"Joining Streams and Tables in the ksqlDB documentation. See also the Pipeline pattern, used for considering Events in series (rather than in parallel). See also the Projection Table pattern, a memory-efficient way of considering an Event Stream over a potentially-infinite time period. See chapter 14, \"Kafka Streams and KSQL\", of Designing Event-Driven Systems for further discussion.","title":"References"},{"location":"stream-processing/suppressed-event-aggregator/","text":"Suppressed Event Aggregator An Event Streaming Application can perform continuous aggregation operations like an Event Aggregator . Normally, however, the aggregator will emit \"intermediate\" processing results. That's because an event stream is potentially infinite, so generally we do not know when the input is considered \"complete\". So, with few exceptions, there's not really a point where we can have a \"final\" result. Also, this emit strategy has the benefit of resulting in low latency between the time when a new input event is received and the time when updated processing results are available. In certain situations, however, a single, final result is what we prefer to receive, rather than multiple intermediate results. For example, when we have to feed aggregation results into a system that is natively not compatible with a streaming approach. Here, we need an aggregator that is able to \"suppress\" intermediate results so that only a single, final result is being produced. Problem How can an event aggregator provide a final aggregation result, rather than \"intermediate\" results that keep being updated? Solution Generally speaking, this is possible only for windowed aggregations. That's because, in this case, the aggregator does know when the input for a given window (e.g., a 5-minute window in order to compute 5-minute averages) is considered complete, and thus it can be configured to suppress \"intermediate\" results until the window time passes. How do we do this? First, the input events of the aggregator must be windowed via an Event Grouper , i.e., the events are being grouped into \"windows\" based on their timestamps. Depending on the configured grouping, an event is placed exclusively into a single window, or it can be placed into multiple windows. Then, the event aggregator performs its operation on each window. Only once the window is considered to have \"passed\" (e.g., a 5-minute window starting at 09:00am and ending at 09:05am) will the aggregator output a single, final result for this window. For example, consider an aggregation for an event stream of customer payments, where we want to compute the number of payments per hour. By using a window size of 1 hour, we can emit a final count for the hourly number of payments once the respective 1-hour window closes. Ideally, the aggregator is able to handle out-of-order or \"late\" events, which is a common situation to deal with in an event streaming platform (e.g., an event created at 09:03 arrives only at 09:07). Here, a common technique is to let users define a so-called grace period for windows to give delayed events some extra time to arrive. Events that arrive within the grace period of a window will be processed, whereas any later events will be not be processed (e.g., if the grace period is 3 minutes, then the 09:03 event arriving at 09:07 would be included in the 09:00-09:05 window; any events arriving at or after 09:08 would be ignored by this window). Note that the use of a grace period increases the processing latency, because the aggregator has to wait for an additional period of time before it knows the input for a given window is complete and thus before it can output the single, final result for that window. Implementation For Apache Kafka\u00ae, the Kafka Streams client library provides a suppress operator in its DSL, which we can apply to windowed aggregations. In the following example we compute hourly aggregations on a stream of orders, using a grace period of five minutes to wait for any orders arriving with a slight delay. The suppress operator ensures that there's only a single result event for each hourly window. KStream<String, OrderEvent> orderStream = builder.stream(...); orderStream.groupByKey() .windowedBy(TimeWindows.of(Duration.ofHours(1)).grace(Duration.ofMinutes(5))) .aggregate(() -> 0.0 /* initial value of `total`, per window */, (key, order, total) -> total + order.getPrice(), Materialized.with(Serdes.String(), Serdes.Double())) .suppress(untilWindowCloses(unbounded())) .toStream() .map((windowKey, value) -> KeyValue.pair(windowKey.key(),value)) .to(outputTopic, Produced.with(Serdes.String(), Serdes.Double())); Considerations To honor the contract of outputting only a single result per window, the suppressed aggregator typically buffers events in some way until the window closes. If its implementation uses an in-memory buffer, then, depending on the number of events per window and their payload sizes, there's the risk to run into out-of-memory errors. References The tutorial Emit a final result from a time window with Kafka Streams provides more details about the suppress operator.","title":"Suppressed Event Aggregator"},{"location":"stream-processing/suppressed-event-aggregator/#suppressed-event-aggregator","text":"An Event Streaming Application can perform continuous aggregation operations like an Event Aggregator . Normally, however, the aggregator will emit \"intermediate\" processing results. That's because an event stream is potentially infinite, so generally we do not know when the input is considered \"complete\". So, with few exceptions, there's not really a point where we can have a \"final\" result. Also, this emit strategy has the benefit of resulting in low latency between the time when a new input event is received and the time when updated processing results are available. In certain situations, however, a single, final result is what we prefer to receive, rather than multiple intermediate results. For example, when we have to feed aggregation results into a system that is natively not compatible with a streaming approach. Here, we need an aggregator that is able to \"suppress\" intermediate results so that only a single, final result is being produced.","title":"Suppressed Event Aggregator"},{"location":"stream-processing/suppressed-event-aggregator/#problem","text":"How can an event aggregator provide a final aggregation result, rather than \"intermediate\" results that keep being updated?","title":"Problem"},{"location":"stream-processing/suppressed-event-aggregator/#solution","text":"Generally speaking, this is possible only for windowed aggregations. That's because, in this case, the aggregator does know when the input for a given window (e.g., a 5-minute window in order to compute 5-minute averages) is considered complete, and thus it can be configured to suppress \"intermediate\" results until the window time passes. How do we do this? First, the input events of the aggregator must be windowed via an Event Grouper , i.e., the events are being grouped into \"windows\" based on their timestamps. Depending on the configured grouping, an event is placed exclusively into a single window, or it can be placed into multiple windows. Then, the event aggregator performs its operation on each window. Only once the window is considered to have \"passed\" (e.g., a 5-minute window starting at 09:00am and ending at 09:05am) will the aggregator output a single, final result for this window. For example, consider an aggregation for an event stream of customer payments, where we want to compute the number of payments per hour. By using a window size of 1 hour, we can emit a final count for the hourly number of payments once the respective 1-hour window closes. Ideally, the aggregator is able to handle out-of-order or \"late\" events, which is a common situation to deal with in an event streaming platform (e.g., an event created at 09:03 arrives only at 09:07). Here, a common technique is to let users define a so-called grace period for windows to give delayed events some extra time to arrive. Events that arrive within the grace period of a window will be processed, whereas any later events will be not be processed (e.g., if the grace period is 3 minutes, then the 09:03 event arriving at 09:07 would be included in the 09:00-09:05 window; any events arriving at or after 09:08 would be ignored by this window). Note that the use of a grace period increases the processing latency, because the aggregator has to wait for an additional period of time before it knows the input for a given window is complete and thus before it can output the single, final result for that window.","title":"Solution"},{"location":"stream-processing/suppressed-event-aggregator/#implementation","text":"For Apache Kafka\u00ae, the Kafka Streams client library provides a suppress operator in its DSL, which we can apply to windowed aggregations. In the following example we compute hourly aggregations on a stream of orders, using a grace period of five minutes to wait for any orders arriving with a slight delay. The suppress operator ensures that there's only a single result event for each hourly window. KStream<String, OrderEvent> orderStream = builder.stream(...); orderStream.groupByKey() .windowedBy(TimeWindows.of(Duration.ofHours(1)).grace(Duration.ofMinutes(5))) .aggregate(() -> 0.0 /* initial value of `total`, per window */, (key, order, total) -> total + order.getPrice(), Materialized.with(Serdes.String(), Serdes.Double())) .suppress(untilWindowCloses(unbounded())) .toStream() .map((windowKey, value) -> KeyValue.pair(windowKey.key(),value)) .to(outputTopic, Produced.with(Serdes.String(), Serdes.Double()));","title":"Implementation"},{"location":"stream-processing/suppressed-event-aggregator/#considerations","text":"To honor the contract of outputting only a single result per window, the suppressed aggregator typically buffers events in some way until the window closes. If its implementation uses an in-memory buffer, then, depending on the number of events per window and their payload sizes, there's the risk to run into out-of-memory errors.","title":"Considerations"},{"location":"stream-processing/suppressed-event-aggregator/#references","text":"The tutorial Emit a final result from a time window with Kafka Streams provides more details about the suppress operator.","title":"References"},{"location":"stream-processing/wait-for-n-events/","text":"Wait For N Events Sometimes Events become significant after they've happened several times. A user can try to log in five times, but after that we'll lock their account. A parcel delivery will be attempted three times before we ask the customer to collect it from the depot. A gamer gets a trophy after they've killed their hundredth Blarg. How can we efficiently watch for logically similar Events? Problem How can an application wait for a certain number of Events to occur before performing processing? Solution To consider related Events as a group, we need to group them by a given key, and then count the occurrences of that key. Implementation In the streaming database ksqlDB , we can easily create a Projection Table that groups and counts Events by a particular key. As an example, imagine that we are handling very large financial transactions. We only want to process these transactions after they've been reviewed and approved by two managers. We'll start with a stream of signed Events from managers: CREATE OR REPLACE STREAM trade_reviews ( trade_id BIGINT, manager_id VARCHAR, signature VARCHAR, approved BOOLEAN ) WITH ( KAFKA_TOPIC = 'trade_reviews_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 2 ); We'll group reviews by their trade_id , and then COUNT() how many approvals ( approved = TRUE ) we see for each: CREATE OR REPLACE TABLE trade_approval AS SELECT trade_id, COUNT(*) AS approvals FROM trade_reviews WHERE approved = TRUE GROUP BY trade_id; Query that stream in one terminal: SELECT * FROM trade_approval WHERE approvals = 2 EMIT CHANGES; Insert some data in another terminal: INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (1, 'alice', '6f797a', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (2, 'alice', 'b523af', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (3, 'alice', 'fe1aaf', FALSE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (4, 'alice', 'f41bf3', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (2, 'bob', '0441ed', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (4, 'bob', '50f237', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (1, 'carol', 'ee52f5', FALSE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (3, 'carol', '4adb7c', TRUE); This produces a stream of trades that are ready to process: +----------+-----------+ |TRADE_ID |APPROVALS | +----------+-----------+ |2 |2 | |4 |2 | Considerations Note that in the example above, we queried for an exact number of approvals ( WHERE approvals = 2 ). We could have used a greater-than-or-equal check ( WHERE approvals >= 2 ), but that would have emitted a new Event for a third approval, and then a fourth, and so on. In this case, that would be the wrong behavior, but it might be a useful feature in a system where we wanted to reward loyal customers and send out a discount email for every order after their first 10. References See also the Event Grouping pattern, for a more general discussion of GROUP BY operations. See chapter 15, \"Building Streaming Services\", of Designing Event Driven Systems for further discussion.","title":"Wait For N Events"},{"location":"stream-processing/wait-for-n-events/#wait-for-n-events","text":"Sometimes Events become significant after they've happened several times. A user can try to log in five times, but after that we'll lock their account. A parcel delivery will be attempted three times before we ask the customer to collect it from the depot. A gamer gets a trophy after they've killed their hundredth Blarg. How can we efficiently watch for logically similar Events?","title":"Wait For N Events"},{"location":"stream-processing/wait-for-n-events/#problem","text":"How can an application wait for a certain number of Events to occur before performing processing?","title":"Problem"},{"location":"stream-processing/wait-for-n-events/#solution","text":"To consider related Events as a group, we need to group them by a given key, and then count the occurrences of that key.","title":"Solution"},{"location":"stream-processing/wait-for-n-events/#implementation","text":"In the streaming database ksqlDB , we can easily create a Projection Table that groups and counts Events by a particular key. As an example, imagine that we are handling very large financial transactions. We only want to process these transactions after they've been reviewed and approved by two managers. We'll start with a stream of signed Events from managers: CREATE OR REPLACE STREAM trade_reviews ( trade_id BIGINT, manager_id VARCHAR, signature VARCHAR, approved BOOLEAN ) WITH ( KAFKA_TOPIC = 'trade_reviews_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 2 ); We'll group reviews by their trade_id , and then COUNT() how many approvals ( approved = TRUE ) we see for each: CREATE OR REPLACE TABLE trade_approval AS SELECT trade_id, COUNT(*) AS approvals FROM trade_reviews WHERE approved = TRUE GROUP BY trade_id; Query that stream in one terminal: SELECT * FROM trade_approval WHERE approvals = 2 EMIT CHANGES; Insert some data in another terminal: INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (1, 'alice', '6f797a', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (2, 'alice', 'b523af', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (3, 'alice', 'fe1aaf', FALSE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (4, 'alice', 'f41bf3', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (2, 'bob', '0441ed', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (4, 'bob', '50f237', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (1, 'carol', 'ee52f5', FALSE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (3, 'carol', '4adb7c', TRUE); This produces a stream of trades that are ready to process: +----------+-----------+ |TRADE_ID |APPROVALS | +----------+-----------+ |2 |2 | |4 |2 |","title":"Implementation"},{"location":"stream-processing/wait-for-n-events/#considerations","text":"Note that in the example above, we queried for an exact number of approvals ( WHERE approvals = 2 ). We could have used a greater-than-or-equal check ( WHERE approvals >= 2 ), but that would have emitted a new Event for a third approval, and then a fourth, and so on. In this case, that would be the wrong behavior, but it might be a useful feature in a system where we wanted to reward loyal customers and send out a discount email for every order after their first 10.","title":"Considerations"},{"location":"stream-processing/wait-for-n-events/#references","text":"See also the Event Grouping pattern, for a more general discussion of GROUP BY operations. See chapter 15, \"Building Streaming Services\", of Designing Event Driven Systems for further discussion.","title":"References"},{"location":"stream-processing/wallclock-time/","text":"Wall-Clock-Time Processing Consistent time semantics are of particular importance in stream processing. Many operations in an Event Processor are dependent on time, such as joins, aggregations when computed over a window of time (e.g., five-minute averages), and handling out-of-order and \"late\" data. In many systems, developers have a choice between different variants of time for an Event : Event-time, which captures the time at which an Event was originally created by its Event Source. Ingestion-time, which captures the time at which an Event was received on the Event Stream in an Event Streaming Platform . Wall-clock-time or processing-time, which is the time at which a downstream Event Processor happens to process an Event (potentially milliseconds, hours, months, or more after event-time). Depending on the use case, developers need to pick one variant over the others. Problem How can Events from an Event Source be processed irrespective of the timestamps from when they were originally created by the Event Source? Solution Depending on the use case, Event Processors may use the time when the Event was originally created by its Event Source , the time when it was received on the Event Stream in the Event Streaming Platform , or a time derived from one or more data fields provided by the Event itself (i.e., from the Event payload). Implementation As an example, the streaming database ksqlDB maintains a system column called ROWTIME , which tracks the timestamp of an Event . By default, ROWTIME is inherited from the timestamp in the underlying Apache Kafka\u00ae record metadata, but it can also be pulled from a field in the Event . See Time semantics in the ksqlDB documentation for more information. CREATE STREAM TEMPERATURE_READINGS_EVENTTIME WITH (KAFKA_TOPIC='deviceEvents', VALUE_FORMAT='avro', TIMESTAMP='eventTime'); References The Event-Time Processing pattern provides basic information about time semantics in stream processing. See the tutorial Event-time semantics for a full example.","title":"Wall-Clock-Time Processing"},{"location":"stream-processing/wallclock-time/#wall-clock-time-processing","text":"Consistent time semantics are of particular importance in stream processing. Many operations in an Event Processor are dependent on time, such as joins, aggregations when computed over a window of time (e.g., five-minute averages), and handling out-of-order and \"late\" data. In many systems, developers have a choice between different variants of time for an Event : Event-time, which captures the time at which an Event was originally created by its Event Source. Ingestion-time, which captures the time at which an Event was received on the Event Stream in an Event Streaming Platform . Wall-clock-time or processing-time, which is the time at which a downstream Event Processor happens to process an Event (potentially milliseconds, hours, months, or more after event-time). Depending on the use case, developers need to pick one variant over the others.","title":"Wall-Clock-Time Processing"},{"location":"stream-processing/wallclock-time/#problem","text":"How can Events from an Event Source be processed irrespective of the timestamps from when they were originally created by the Event Source?","title":"Problem"},{"location":"stream-processing/wallclock-time/#solution","text":"Depending on the use case, Event Processors may use the time when the Event was originally created by its Event Source , the time when it was received on the Event Stream in the Event Streaming Platform , or a time derived from one or more data fields provided by the Event itself (i.e., from the Event payload).","title":"Solution"},{"location":"stream-processing/wallclock-time/#implementation","text":"As an example, the streaming database ksqlDB maintains a system column called ROWTIME , which tracks the timestamp of an Event . By default, ROWTIME is inherited from the timestamp in the underlying Apache Kafka\u00ae record metadata, but it can also be pulled from a field in the Event . See Time semantics in the ksqlDB documentation for more information. CREATE STREAM TEMPERATURE_READINGS_EVENTTIME WITH (KAFKA_TOPIC='deviceEvents', VALUE_FORMAT='avro', TIMESTAMP='eventTime');","title":"Implementation"},{"location":"stream-processing/wallclock-time/#references","text":"The Event-Time Processing pattern provides basic information about time semantics in stream processing. See the tutorial Event-time semantics for a full example.","title":"References"},{"location":"table/projection-table/","text":"Projection Table One of the first questions we want to ask of a stream of events is, \"Where are we now?\" If we have a stream of sales events, we'd like to have the total sales figures at our fingertips. If we have a stream of login events, we'd like to know when each user last logged in. If our trucks send GPS data every minute, we'd like to know where each truck is right now. How do we efficiently roll up data? How do we preserve a complete event log and enjoy the fast queries of an \"update-in-place\" style database? Problem How can a stream of change events be efficiently summarized to give the current state of the world? Solution We can maintain a projection table that behaves just like a materialized view in a traditional database. As new events come in, the table is automatically updated, constantly giving us a live picture of the system. Events with the same key are considered related; newer events are interpreted, depending on their contents, as updates to or deletions of older events. As with a materialized view, projection tables are read-only. To change a projection table, we change the underlying data by recording new events to the table's underlying stream. Implementation ksqlDB supports easy creation of summary tables and materialized views. We declare them once, and the server will maintain their data as new events stream in. As an example, imagine that we are shipping packages around the world. As a package reaches each point on its journey, it is logged with its current location. Let's start with a stream of package check-in events: CREATE OR REPLACE STREAM package_checkins ( package_id BIGINT KEY, location VARCHAR, processed_by VARCHAR ) WITH ( KAFKA_TOPIC = 'package_checkins_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 3 ); Then we'll create a projection table, tracking each package_id and its most recent location : CREATE OR REPLACE TABLE package_locations AS SELECT package_id, LATEST_BY_OFFSET(location) AS current_location FROM package_checkins GROUP BY package_id; Query that stream in one terminal: SELECT * FROM package_locations EMIT CHANGES; ...and insert some data in another terminal: INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'New York' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'Rome' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'Rome' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'Washington' ); These are the results, in a table of each package's last-known location: +------------+------------------+ |PACKAGE_ID |CURRENT_LOCATION | +------------+------------------+ |1 |Rome | |2 |Rome | |3 |Washington | As new data is inserted, the package_locations table is updated, so we can see the current location of each package without scanning through the event history every time. Considerations In the example above, it's important to consider partitioning. When we declared the package_checkins stream, we marked the package_id as the KEY . This ensures that all events with the same package_id will be stored in the same partition, which in turn means that for a given package_id , newer events will always have a higher offset value. Thus, when we query for the LATEST_BY_OFFSET , we always get the newest event for each package. If we had chosen a different partitioning key, or not specified a key at all, we would get very different results. LATEST_BY_OFFSET is only one of the many summary functions supported by ksqlDB . Others range from simple sums and averages to time-aware functions and histograms. And beyond those, we can easily define custom functions , or look to Kafka Streams for complete control. References Aggregate functions in the ksqlDB documentation. Creating custom ksqlDB functions in the ksqlDB documentation. See also the State Table pattern.","title":"Projection Table"},{"location":"table/projection-table/#projection-table","text":"One of the first questions we want to ask of a stream of events is, \"Where are we now?\" If we have a stream of sales events, we'd like to have the total sales figures at our fingertips. If we have a stream of login events, we'd like to know when each user last logged in. If our trucks send GPS data every minute, we'd like to know where each truck is right now. How do we efficiently roll up data? How do we preserve a complete event log and enjoy the fast queries of an \"update-in-place\" style database?","title":"Projection Table"},{"location":"table/projection-table/#problem","text":"How can a stream of change events be efficiently summarized to give the current state of the world?","title":"Problem"},{"location":"table/projection-table/#solution","text":"We can maintain a projection table that behaves just like a materialized view in a traditional database. As new events come in, the table is automatically updated, constantly giving us a live picture of the system. Events with the same key are considered related; newer events are interpreted, depending on their contents, as updates to or deletions of older events. As with a materialized view, projection tables are read-only. To change a projection table, we change the underlying data by recording new events to the table's underlying stream.","title":"Solution"},{"location":"table/projection-table/#implementation","text":"ksqlDB supports easy creation of summary tables and materialized views. We declare them once, and the server will maintain their data as new events stream in. As an example, imagine that we are shipping packages around the world. As a package reaches each point on its journey, it is logged with its current location. Let's start with a stream of package check-in events: CREATE OR REPLACE STREAM package_checkins ( package_id BIGINT KEY, location VARCHAR, processed_by VARCHAR ) WITH ( KAFKA_TOPIC = 'package_checkins_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 3 ); Then we'll create a projection table, tracking each package_id and its most recent location : CREATE OR REPLACE TABLE package_locations AS SELECT package_id, LATEST_BY_OFFSET(location) AS current_location FROM package_checkins GROUP BY package_id; Query that stream in one terminal: SELECT * FROM package_locations EMIT CHANGES; ...and insert some data in another terminal: INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'New York' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'Rome' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'Rome' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'Washington' ); These are the results, in a table of each package's last-known location: +------------+------------------+ |PACKAGE_ID |CURRENT_LOCATION | +------------+------------------+ |1 |Rome | |2 |Rome | |3 |Washington | As new data is inserted, the package_locations table is updated, so we can see the current location of each package without scanning through the event history every time.","title":"Implementation"},{"location":"table/projection-table/#considerations","text":"In the example above, it's important to consider partitioning. When we declared the package_checkins stream, we marked the package_id as the KEY . This ensures that all events with the same package_id will be stored in the same partition, which in turn means that for a given package_id , newer events will always have a higher offset value. Thus, when we query for the LATEST_BY_OFFSET , we always get the newest event for each package. If we had chosen a different partitioning key, or not specified a key at all, we would get very different results. LATEST_BY_OFFSET is only one of the many summary functions supported by ksqlDB . Others range from simple sums and averages to time-aware functions and histograms. And beyond those, we can easily define custom functions , or look to Kafka Streams for complete control.","title":"Considerations"},{"location":"table/projection-table/#references","text":"Aggregate functions in the ksqlDB documentation. Creating custom ksqlDB functions in the ksqlDB documentation. See also the State Table pattern.","title":"References"},{"location":"table/state-table/","text":"State Table Event Processors often need to perform stateful operations, such as an aggregation (for example, counting the number of events). The state is similar to a table in a relational database, and is mutable: it allows for read and write operations. It is essential that the event processor has an efficient and fault-tolerant mechanism for state management--for recording and updating the state while processing input events--to ensure correctness of the computations and to prevent data loss and data duplication. Problem How can an Event Processor manage mutable state, similar to how a table does in a relational database? Solution We need to implement a mutable state table that allows the Event Processor to record and update state. For example, to count the number of payments per customer, a state table provides a mapping between the customer (for example, a customer ID) and the current count of payments. The state's storage backend can vary by implementation: options include local state stores (such as RocksDB), remote state stores (such as Amazon DynamoDB or a NoSQL database), and in-memory caches. Local state stores are usually recommended, as they do not incur additional latency for network round trips, and this improves the end-to-end performance of the Event Processor. Regardless of backend choice, the state table should be fault-tolerant to ensure strong processing guarantees, such as exactly-once semantics. Fault tolerance can be achieved, for example, by attaching an Event Source Connector to the state table to perform change data capture (CDC). This allows the Event Processor to continuously back up state changes into an Event Stream and to restore the state table in the case of failure or similar scenarios. There is no network latency to contend with. This also provides an approach for restoring the state after a crash that destroys the local store. Implementation The streaming database ksqlDB provides state tables out of the box with its TABLE data collection. The implementation uses local, fault-tolerant state stores that are continuously backed up into ksqlDB's distributed storage layer -- Kafka -- so that the data is durable. For example, we can maintain a stateful count of all sales by aggregating the movie_ticket_sales stream into a movie_tickets_sold table: CREATE TABLE movie_tickets_sold AS SELECT title, COUNT(ticket_total_value) AS tickets_sold FROM movie_ticket_sales GROUP BY title EMIT CHANGES; References The blog post The Curious Incident of the State Store in Recovery in ksqlDB explains the fault tolerance of ksqlDB's state management in more detail. See also the Projection Table pattern.","title":"State Table"},{"location":"table/state-table/#state-table","text":"Event Processors often need to perform stateful operations, such as an aggregation (for example, counting the number of events). The state is similar to a table in a relational database, and is mutable: it allows for read and write operations. It is essential that the event processor has an efficient and fault-tolerant mechanism for state management--for recording and updating the state while processing input events--to ensure correctness of the computations and to prevent data loss and data duplication.","title":"State Table"},{"location":"table/state-table/#problem","text":"How can an Event Processor manage mutable state, similar to how a table does in a relational database?","title":"Problem"},{"location":"table/state-table/#solution","text":"We need to implement a mutable state table that allows the Event Processor to record and update state. For example, to count the number of payments per customer, a state table provides a mapping between the customer (for example, a customer ID) and the current count of payments. The state's storage backend can vary by implementation: options include local state stores (such as RocksDB), remote state stores (such as Amazon DynamoDB or a NoSQL database), and in-memory caches. Local state stores are usually recommended, as they do not incur additional latency for network round trips, and this improves the end-to-end performance of the Event Processor. Regardless of backend choice, the state table should be fault-tolerant to ensure strong processing guarantees, such as exactly-once semantics. Fault tolerance can be achieved, for example, by attaching an Event Source Connector to the state table to perform change data capture (CDC). This allows the Event Processor to continuously back up state changes into an Event Stream and to restore the state table in the case of failure or similar scenarios. There is no network latency to contend with. This also provides an approach for restoring the state after a crash that destroys the local store.","title":"Solution"},{"location":"table/state-table/#implementation","text":"The streaming database ksqlDB provides state tables out of the box with its TABLE data collection. The implementation uses local, fault-tolerant state stores that are continuously backed up into ksqlDB's distributed storage layer -- Kafka -- so that the data is durable. For example, we can maintain a stateful count of all sales by aggregating the movie_ticket_sales stream into a movie_tickets_sold table: CREATE TABLE movie_tickets_sold AS SELECT title, COUNT(ticket_total_value) AS tickets_sold FROM movie_ticket_sales GROUP BY title EMIT CHANGES;","title":"Implementation"},{"location":"table/state-table/#references","text":"The blog post The Curious Incident of the State Store in Recovery in ksqlDB explains the fault tolerance of ksqlDB's state management in more detail. See also the Projection Table pattern.","title":"References"}]}
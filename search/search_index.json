{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ksqlDB Recipes","title":"ksqlDB Recipes"},{"location":"#welcome-to-ksqldb-recipes","text":"","title":"Welcome to ksqlDB Recipes"},{"location":"ai-ml/","text":"Description These recipes ...","title":"Description"},{"location":"ai-ml/#description","text":"These recipes ...","title":"Description"},{"location":"customer-360/","text":"Description These recipes ...","title":"Description"},{"location":"customer-360/#description","text":"These recipes ...","title":"Description"},{"location":"customer-360/clickstream/","text":"Clickstream Data Analysis What is it? It's like this and like that and like this and uh. Cut to the code -- Stream of HTTP codes CREATE SOURCE CONNECTOR datagen_clickstream_codes WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_codes', 'quickstart' = 'clickstream_codes', 'maxInterval' = '20', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of users CREATE SOURCE CONNECTOR datagen_clickstream_users WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_users', 'quickstart' = 'clickstream_users', 'maxInterval' = '10', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of per-user session information CREATE SOURCE CONNECTOR datagen_clickstream WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream', 'quickstart' = 'clickstream', 'maxInterval' = '30', 'format' = 'json'); --------------------------------------------------------------------------------------------------- -- Register sources: --------------------------------------------------------------------------------------------------- -- stream of user clicks: CREATE STREAM clickstream ( _time bigint, time varchar, ip varchar, request varchar, status int, userid int, bytes bigint, agent varchar ) with ( kafka_topic = 'clickstream', value_format = 'json' ); -- error code lookup table: CREATE TABLE clickstream_codes ( code int primary key, definition varchar ) with ( kafka_topic = 'clickstream_codes', value_format = 'json' ); -- users lookup table: CREATE TABLE WEB_USERS ( user_id int primary key, registered_At BIGINT, username varchar, first_name varchar, last_name varchar, city varchar, level varchar ) with ( kafka_topic = 'clickstream_users', value_format = 'json' ); --------------------------------------------------------------------------------------------------- -- Build materialized stream views: --------------------------------------------------------------------------------------------------- -- enrich click-stream with more user information: CREATE STREAM USER_CLICKSTREAM AS SELECT userid, u.username, ip, u.city, request, status, bytes FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id; -- Enrich click-stream with more information about error codes: CREATE STREAM ENRICHED_ERROR_CODES AS SELECT code, definition FROM clickstream LEFT JOIN clickstream_codes ON clickstream.status = clickstream_codes.code; --------------------------------------------------------------------------------------------------- -- Build materialized table views: --------------------------------------------------------------------------------------------------- -- Per-userId tables ----------------------------------------------------------------------------- -- Table of events per minute for each user: CREATE table events_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS events FROM clickstream window TUMBLING (size 60 second) GROUP BY userid; -- Table of html pages per minute for each user: CREATE TABLE pages_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS pages FROM clickstream WINDOW HOPPING (size 60 second, advance by 5 second) WHERE request like '%html%' GROUP BY userid; -- Per-username tables ---------------------------------------------------------------------------- -- User sessions table - 30 seconds of inactivity expires the session -- Table counts number of events within the session CREATE TABLE CLICK_USER_SESSIONS AS SELECT username as K, AS_VALUE(username) as username, WINDOWEND as EVENT_TS, count(*) AS events FROM USER_CLICKSTREAM window SESSION (30 second) GROUP BY username; -- Per-status tables ------------------------------------------------------------------------------ -- number of errors per min, using 'HAVING' Filter to show ERROR codes > 400 where count > 5 CREATE TABLE ERRORS_PER_MIN_ALERT AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 20 second) WHERE status > 400 GROUP BY status HAVING count(*) > 5 AND count(*) is not NULL; -- number of errors per min CREATE TABLE ERRORS_PER_MIN AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 5 second) WHERE status > 400 GROUP BY status; -- Per-error code tables -------------------------------------------------------------------------- -- Enriched error codes table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE ENRICHED_ERROR_CODES_COUNT WITH (key_format='json') AS SELECT code as k1, definition as K2, AS_VALUE(code) as code, WINDOWSTART as EVENT_TS, AS_VALUE(definition) as definition, COUNT(*) AS count FROM ENRICHED_ERROR_CODES WINDOW TUMBLING (size 30 second) GROUP BY code, definition HAVING COUNT(*) > 1; -- Enriched user details table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE USER_IP_ACTIVITY WITH (key_format='json') AS SELECT username as k1, ip as k2, city as k3, AS_VALUE(username) as username, WINDOWSTART as EVENT_TS, AS_VALUE(ip) as ip, AS_VALUE(city) as city, COUNT(*) AS count FROM USER_CLICKSTREAM WINDOW TUMBLING (size 60 second) GROUP BY username, ip, city HAVING COUNT(*) > 1; -- Send data to Elasticsearch CREATE SINK CONNECTOR analyzed_clickstream WITH ( 'connector.class' = 'ElasticsearchSink', 'name' = 'elasticsearch-connector', 'input.data.format' = 'JSON', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'USER_IP_ACTIVITY, ENRICHED_ERROR_CODES_COUNT', 'connection.url' = '<elasticsearch-URI>', 'connection.user' = '<elasticsearch-username>', 'connection.password' = '<elasticsearch-password>', 'type.name' = 'type.name=kafkaconnect', 'key.ignore' = 'true', 'schema.ignore' = 'true' ); Launch Step-by-Step Setup your Environment Sign up for Confluent Cloud , a fully-managed Apache Kafka service. After you log in to Confluent Cloud, click on Add cloud environment and name the environment learn-kafka . Using a new environment keeps your learning resources separate from your other Confluent Cloud resources. From the Billing & payment section in the Menu, apply the promo code C50INTEG to receive an additional $50 free usage on Confluent Cloud ( details ). Click on LEARN and follow the instructions to launch a Kafka cluster and to enable Schema Registry. Read the data in This recipe uses blah blah connector. It assumes you have setup foobar. -- Stream of HTTP codes CREATE SOURCE CONNECTOR datagen_clickstream_codes WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_codes', 'quickstart' = 'clickstream_codes', 'maxInterval' = '20', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of users CREATE SOURCE CONNECTOR datagen_clickstream_users WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_users', 'quickstart' = 'clickstream_users', 'maxInterval' = '10', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of per-user session information CREATE SOURCE CONNECTOR datagen_clickstream WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream', 'quickstart' = 'clickstream', 'maxInterval' = '30', 'format' = 'json'); Run stream processing app Translate and filter all the things. --------------------------------------------------------------------------------------------------- -- Register sources: --------------------------------------------------------------------------------------------------- -- stream of user clicks: CREATE STREAM clickstream ( _time bigint, time varchar, ip varchar, request varchar, status int, userid int, bytes bigint, agent varchar ) with ( kafka_topic = 'clickstream', value_format = 'json' ); -- error code lookup table: CREATE TABLE clickstream_codes ( code int primary key, definition varchar ) with ( kafka_topic = 'clickstream_codes', value_format = 'json' ); -- users lookup table: CREATE TABLE WEB_USERS ( user_id int primary key, registered_At BIGINT, username varchar, first_name varchar, last_name varchar, city varchar, level varchar ) with ( kafka_topic = 'clickstream_users', value_format = 'json' ); --------------------------------------------------------------------------------------------------- -- Build materialized stream views: --------------------------------------------------------------------------------------------------- -- enrich click-stream with more user information: CREATE STREAM USER_CLICKSTREAM AS SELECT userid, u.username, ip, u.city, request, status, bytes FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id; -- Enrich click-stream with more information about error codes: CREATE STREAM ENRICHED_ERROR_CODES AS SELECT code, definition FROM clickstream LEFT JOIN clickstream_codes ON clickstream.status = clickstream_codes.code; --------------------------------------------------------------------------------------------------- -- Build materialized table views: --------------------------------------------------------------------------------------------------- -- Per-userId tables ----------------------------------------------------------------------------- -- Table of events per minute for each user: CREATE table events_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS events FROM clickstream window TUMBLING (size 60 second) GROUP BY userid; -- Table of html pages per minute for each user: CREATE TABLE pages_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS pages FROM clickstream WINDOW HOPPING (size 60 second, advance by 5 second) WHERE request like '%html%' GROUP BY userid; -- Per-username tables ---------------------------------------------------------------------------- -- User sessions table - 30 seconds of inactivity expires the session -- Table counts number of events within the session CREATE TABLE CLICK_USER_SESSIONS AS SELECT username as K, AS_VALUE(username) as username, WINDOWEND as EVENT_TS, count(*) AS events FROM USER_CLICKSTREAM window SESSION (30 second) GROUP BY username; -- Per-status tables ------------------------------------------------------------------------------ -- number of errors per min, using 'HAVING' Filter to show ERROR codes > 400 where count > 5 CREATE TABLE ERRORS_PER_MIN_ALERT AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 20 second) WHERE status > 400 GROUP BY status HAVING count(*) > 5 AND count(*) is not NULL; -- number of errors per min CREATE TABLE ERRORS_PER_MIN AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 5 second) WHERE status > 400 GROUP BY status; -- Per-error code tables -------------------------------------------------------------------------- -- Enriched error codes table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE ENRICHED_ERROR_CODES_COUNT WITH (key_format='json') AS SELECT code as k1, definition as K2, AS_VALUE(code) as code, WINDOWSTART as EVENT_TS, AS_VALUE(definition) as definition, COUNT(*) AS count FROM ENRICHED_ERROR_CODES WINDOW TUMBLING (size 30 second) GROUP BY code, definition HAVING COUNT(*) > 1; -- Enriched user details table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE USER_IP_ACTIVITY WITH (key_format='json') AS SELECT username as k1, ip as k2, city as k3, AS_VALUE(username) as username, WINDOWSTART as EVENT_TS, AS_VALUE(ip) as ip, AS_VALUE(city) as city, COUNT(*) AS count FROM USER_CLICKSTREAM WINDOW TUMBLING (size 60 second) GROUP BY username, ip, city HAVING COUNT(*) > 1; Write the data out Post-processing, send the data to this DB. -- Send data to Elasticsearch CREATE SINK CONNECTOR analyzed_clickstream WITH ( 'connector.class' = 'ElasticsearchSink', 'name' = 'elasticsearch-connector', 'input.data.format' = 'JSON', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'USER_IP_ACTIVITY, ENRICHED_ERROR_CODES_COUNT', 'connection.url' = '<elasticsearch-URI>', 'connection.user' = '<elasticsearch-username>', 'connection.password' = '<elasticsearch-password>', 'type.name' = 'type.name=kafkaconnect', 'key.ignore' = 'true', 'schema.ignore' = 'true' );","title":"Clickstream Data Analysis"},{"location":"customer-360/clickstream/#clickstream-data-analysis","text":"","title":"Clickstream Data Analysis"},{"location":"customer-360/clickstream/#what-is-it","text":"It's like this and like that and like this and uh.","title":"What is it?"},{"location":"customer-360/clickstream/#cut-to-the-code","text":"-- Stream of HTTP codes CREATE SOURCE CONNECTOR datagen_clickstream_codes WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_codes', 'quickstart' = 'clickstream_codes', 'maxInterval' = '20', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of users CREATE SOURCE CONNECTOR datagen_clickstream_users WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_users', 'quickstart' = 'clickstream_users', 'maxInterval' = '10', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of per-user session information CREATE SOURCE CONNECTOR datagen_clickstream WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream', 'quickstart' = 'clickstream', 'maxInterval' = '30', 'format' = 'json'); --------------------------------------------------------------------------------------------------- -- Register sources: --------------------------------------------------------------------------------------------------- -- stream of user clicks: CREATE STREAM clickstream ( _time bigint, time varchar, ip varchar, request varchar, status int, userid int, bytes bigint, agent varchar ) with ( kafka_topic = 'clickstream', value_format = 'json' ); -- error code lookup table: CREATE TABLE clickstream_codes ( code int primary key, definition varchar ) with ( kafka_topic = 'clickstream_codes', value_format = 'json' ); -- users lookup table: CREATE TABLE WEB_USERS ( user_id int primary key, registered_At BIGINT, username varchar, first_name varchar, last_name varchar, city varchar, level varchar ) with ( kafka_topic = 'clickstream_users', value_format = 'json' ); --------------------------------------------------------------------------------------------------- -- Build materialized stream views: --------------------------------------------------------------------------------------------------- -- enrich click-stream with more user information: CREATE STREAM USER_CLICKSTREAM AS SELECT userid, u.username, ip, u.city, request, status, bytes FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id; -- Enrich click-stream with more information about error codes: CREATE STREAM ENRICHED_ERROR_CODES AS SELECT code, definition FROM clickstream LEFT JOIN clickstream_codes ON clickstream.status = clickstream_codes.code; --------------------------------------------------------------------------------------------------- -- Build materialized table views: --------------------------------------------------------------------------------------------------- -- Per-userId tables ----------------------------------------------------------------------------- -- Table of events per minute for each user: CREATE table events_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS events FROM clickstream window TUMBLING (size 60 second) GROUP BY userid; -- Table of html pages per minute for each user: CREATE TABLE pages_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS pages FROM clickstream WINDOW HOPPING (size 60 second, advance by 5 second) WHERE request like '%html%' GROUP BY userid; -- Per-username tables ---------------------------------------------------------------------------- -- User sessions table - 30 seconds of inactivity expires the session -- Table counts number of events within the session CREATE TABLE CLICK_USER_SESSIONS AS SELECT username as K, AS_VALUE(username) as username, WINDOWEND as EVENT_TS, count(*) AS events FROM USER_CLICKSTREAM window SESSION (30 second) GROUP BY username; -- Per-status tables ------------------------------------------------------------------------------ -- number of errors per min, using 'HAVING' Filter to show ERROR codes > 400 where count > 5 CREATE TABLE ERRORS_PER_MIN_ALERT AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 20 second) WHERE status > 400 GROUP BY status HAVING count(*) > 5 AND count(*) is not NULL; -- number of errors per min CREATE TABLE ERRORS_PER_MIN AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 5 second) WHERE status > 400 GROUP BY status; -- Per-error code tables -------------------------------------------------------------------------- -- Enriched error codes table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE ENRICHED_ERROR_CODES_COUNT WITH (key_format='json') AS SELECT code as k1, definition as K2, AS_VALUE(code) as code, WINDOWSTART as EVENT_TS, AS_VALUE(definition) as definition, COUNT(*) AS count FROM ENRICHED_ERROR_CODES WINDOW TUMBLING (size 30 second) GROUP BY code, definition HAVING COUNT(*) > 1; -- Enriched user details table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE USER_IP_ACTIVITY WITH (key_format='json') AS SELECT username as k1, ip as k2, city as k3, AS_VALUE(username) as username, WINDOWSTART as EVENT_TS, AS_VALUE(ip) as ip, AS_VALUE(city) as city, COUNT(*) AS count FROM USER_CLICKSTREAM WINDOW TUMBLING (size 60 second) GROUP BY username, ip, city HAVING COUNT(*) > 1; -- Send data to Elasticsearch CREATE SINK CONNECTOR analyzed_clickstream WITH ( 'connector.class' = 'ElasticsearchSink', 'name' = 'elasticsearch-connector', 'input.data.format' = 'JSON', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'USER_IP_ACTIVITY, ENRICHED_ERROR_CODES_COUNT', 'connection.url' = '<elasticsearch-URI>', 'connection.user' = '<elasticsearch-username>', 'connection.password' = '<elasticsearch-password>', 'type.name' = 'type.name=kafkaconnect', 'key.ignore' = 'true', 'schema.ignore' = 'true' );","title":"Cut to the code"},{"location":"customer-360/clickstream/#launch-step-by-step","text":"","title":"Launch Step-by-Step"},{"location":"customer-360/clickstream/#setup-your-environment","text":"Sign up for Confluent Cloud , a fully-managed Apache Kafka service. After you log in to Confluent Cloud, click on Add cloud environment and name the environment learn-kafka . Using a new environment keeps your learning resources separate from your other Confluent Cloud resources. From the Billing & payment section in the Menu, apply the promo code C50INTEG to receive an additional $50 free usage on Confluent Cloud ( details ). Click on LEARN and follow the instructions to launch a Kafka cluster and to enable Schema Registry.","title":"Setup your Environment"},{"location":"customer-360/clickstream/#read-the-data-in","text":"This recipe uses blah blah connector. It assumes you have setup foobar. -- Stream of HTTP codes CREATE SOURCE CONNECTOR datagen_clickstream_codes WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_codes', 'quickstart' = 'clickstream_codes', 'maxInterval' = '20', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of users CREATE SOURCE CONNECTOR datagen_clickstream_users WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_users', 'quickstart' = 'clickstream_users', 'maxInterval' = '10', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of per-user session information CREATE SOURCE CONNECTOR datagen_clickstream WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream', 'quickstart' = 'clickstream', 'maxInterval' = '30', 'format' = 'json');","title":"Read the data in"},{"location":"customer-360/clickstream/#run-stream-processing-app","text":"Translate and filter all the things. --------------------------------------------------------------------------------------------------- -- Register sources: --------------------------------------------------------------------------------------------------- -- stream of user clicks: CREATE STREAM clickstream ( _time bigint, time varchar, ip varchar, request varchar, status int, userid int, bytes bigint, agent varchar ) with ( kafka_topic = 'clickstream', value_format = 'json' ); -- error code lookup table: CREATE TABLE clickstream_codes ( code int primary key, definition varchar ) with ( kafka_topic = 'clickstream_codes', value_format = 'json' ); -- users lookup table: CREATE TABLE WEB_USERS ( user_id int primary key, registered_At BIGINT, username varchar, first_name varchar, last_name varchar, city varchar, level varchar ) with ( kafka_topic = 'clickstream_users', value_format = 'json' ); --------------------------------------------------------------------------------------------------- -- Build materialized stream views: --------------------------------------------------------------------------------------------------- -- enrich click-stream with more user information: CREATE STREAM USER_CLICKSTREAM AS SELECT userid, u.username, ip, u.city, request, status, bytes FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id; -- Enrich click-stream with more information about error codes: CREATE STREAM ENRICHED_ERROR_CODES AS SELECT code, definition FROM clickstream LEFT JOIN clickstream_codes ON clickstream.status = clickstream_codes.code; --------------------------------------------------------------------------------------------------- -- Build materialized table views: --------------------------------------------------------------------------------------------------- -- Per-userId tables ----------------------------------------------------------------------------- -- Table of events per minute for each user: CREATE table events_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS events FROM clickstream window TUMBLING (size 60 second) GROUP BY userid; -- Table of html pages per minute for each user: CREATE TABLE pages_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS pages FROM clickstream WINDOW HOPPING (size 60 second, advance by 5 second) WHERE request like '%html%' GROUP BY userid; -- Per-username tables ---------------------------------------------------------------------------- -- User sessions table - 30 seconds of inactivity expires the session -- Table counts number of events within the session CREATE TABLE CLICK_USER_SESSIONS AS SELECT username as K, AS_VALUE(username) as username, WINDOWEND as EVENT_TS, count(*) AS events FROM USER_CLICKSTREAM window SESSION (30 second) GROUP BY username; -- Per-status tables ------------------------------------------------------------------------------ -- number of errors per min, using 'HAVING' Filter to show ERROR codes > 400 where count > 5 CREATE TABLE ERRORS_PER_MIN_ALERT AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 20 second) WHERE status > 400 GROUP BY status HAVING count(*) > 5 AND count(*) is not NULL; -- number of errors per min CREATE TABLE ERRORS_PER_MIN AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 5 second) WHERE status > 400 GROUP BY status; -- Per-error code tables -------------------------------------------------------------------------- -- Enriched error codes table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE ENRICHED_ERROR_CODES_COUNT WITH (key_format='json') AS SELECT code as k1, definition as K2, AS_VALUE(code) as code, WINDOWSTART as EVENT_TS, AS_VALUE(definition) as definition, COUNT(*) AS count FROM ENRICHED_ERROR_CODES WINDOW TUMBLING (size 30 second) GROUP BY code, definition HAVING COUNT(*) > 1; -- Enriched user details table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE USER_IP_ACTIVITY WITH (key_format='json') AS SELECT username as k1, ip as k2, city as k3, AS_VALUE(username) as username, WINDOWSTART as EVENT_TS, AS_VALUE(ip) as ip, AS_VALUE(city) as city, COUNT(*) AS count FROM USER_CLICKSTREAM WINDOW TUMBLING (size 60 second) GROUP BY username, ip, city HAVING COUNT(*) > 1;","title":"Run stream processing app"},{"location":"customer-360/clickstream/#write-the-data-out","text":"Post-processing, send the data to this DB. -- Send data to Elasticsearch CREATE SINK CONNECTOR analyzed_clickstream WITH ( 'connector.class' = 'ElasticsearchSink', 'name' = 'elasticsearch-connector', 'input.data.format' = 'JSON', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'USER_IP_ACTIVITY, ENRICHED_ERROR_CODES_COUNT', 'connection.url' = '<elasticsearch-URI>', 'connection.user' = '<elasticsearch-username>', 'connection.password' = '<elasticsearch-password>', 'type.name' = 'type.name=kafkaconnect', 'key.ignore' = 'true', 'schema.ignore' = 'true' );","title":"Write the data out"},{"location":"fin-serv/","text":"Description These recipes ...","title":"Description"},{"location":"fin-serv/#description","text":"These recipes ...","title":"Description"},{"location":"healthcare/","text":"Description These recipes ...","title":"Description"},{"location":"healthcare/#description","text":"These recipes ...","title":"Description"},{"location":"retail/","text":"Description These recipes ...","title":"Description"},{"location":"retail/#description","text":"These recipes ...","title":"Description"},{"location":"security/","text":"Description These recipes ...","title":"Description"},{"location":"security/#description","text":"These recipes ...","title":"Description"},{"location":"shared/ccloud_setup/","text":"Sign up for Confluent Cloud , a fully-managed Apache Kafka service. After you log in to Confluent Cloud, click on Add cloud environment and name the environment learn-kafka . Using a new environment keeps your learning resources separate from your other Confluent Cloud resources. From the Billing & payment section in the Menu, apply the promo code C50INTEG to receive an additional $50 free usage on Confluent Cloud ( details ). Click on LEARN and follow the instructions to launch a Kafka cluster and to enable Schema Registry.","title":"Ccloud setup"}]}
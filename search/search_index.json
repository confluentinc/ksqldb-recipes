{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ksqlDB Recipes This is a Work in Progress! Click in the left nav bar to see one of the recipes. How to run: the recipe explains the use case and shows how to implement in Confluent Cloud. It can also be run on Confluent Platform, but you'll first have to make appropriate modifications to the provided configuration files. Note: these recipes will be launched on Confluent Developer https://developer.confluent.io.","title":"ksqlDB Recipes"},{"location":"#welcome-to-ksqldb-recipes","text":"This is a Work in Progress! Click in the left nav bar to see one of the recipes. How to run: the recipe explains the use case and shows how to implement in Confluent Cloud. It can also be run on Confluent Platform, but you'll first have to make appropriate modifications to the provided configuration files. Note: these recipes will be launched on Confluent Developer https://developer.confluent.io.","title":"Welcome to ksqlDB Recipes"},{"location":"customer-360/clickstream/","text":"Understand User Behavior with Clickstream Data What is it? Analyzing clickstream data enables businesses to understand the behavior of its online users, for example: User activity over a given time frame: how many webpages are users viewing Requests that end in error, over a given threshold (e.g., 404 HTTP codes) Where the requests are coming from geographically in a given window of time How long users are interacting with the site (user sessions) Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks. Read the data in Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). This recipe creates simulated data with the Datagen connector. -- Stream of users CREATE SOURCE CONNECTOR datagen_clickstream_users WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_users', 'quickstart' = 'clickstream_users', 'maxInterval' = '10', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of per-user session information CREATE SOURCE CONNECTOR datagen_clickstream WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream', 'quickstart' = 'clickstream', 'maxInterval' = '30', 'format' = 'json'); Optional: to simulate a real-world scenario where user sessions aren't just always open but do close after some time, you can pause and resume the DATAGEN_CLICKSTREAM connector. Run stream processing app Now you can process the data in a variety of ways, by enriching the clickstream data with user information, analyze errors, aggregate data into windows of time, etc. -- stream of user clicks: CREATE STREAM clickstream ( _time bigint, time varchar, ip varchar, request varchar, status int, userid int, bytes bigint, agent varchar ) WITH ( kafka_topic = 'clickstream', value_format = 'json', partitions = 6 ); -- users lookup table: CREATE TABLE WEB_USERS ( user_id int primary key, registered_At BIGINT, username varchar, first_name varchar, last_name varchar, city varchar, level varchar ) WITH ( kafka_topic = 'clickstream_users', value_format = 'json', partitions = 6 ); -- Build materialized stream views: -- enrich click-stream with more user information: CREATE STREAM USER_CLICKSTREAM AS SELECT userid, u.username, ip, u.city, request, status, bytes FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id; -- Build materialized table views: -- Table of html pages per minute for each user: CREATE TABLE pages_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS pages FROM clickstream WINDOW HOPPING (size 60 second, advance by 5 second) WHERE request like '%html%' GROUP BY userid; -- User sessions table - 30 seconds of inactivity expires the session -- Table counts number of events within the session CREATE TABLE CLICK_USER_SESSIONS AS SELECT username as K, AS_VALUE(username) as username, WINDOWEND as EVENT_TS, count(*) AS events FROM USER_CLICKSTREAM window SESSION (30 second) GROUP BY username; -- number of errors per min, using 'HAVING' Filter to show ERROR codes > 400 where count > 5 CREATE TABLE ERRORS_PER_MIN_ALERT AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 20 second) WHERE status > 400 GROUP BY status HAVING count(*) > 5 AND count(*) is not NULL; -- Enriched user details table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE USER_IP_ACTIVITY WITH (key_format='json') AS SELECT username as k1, ip as k2, city as k3, AS_VALUE(username) as username, WINDOWSTART as EVENT_TS, AS_VALUE(ip) as ip, AS_VALUE(city) as city, COUNT(*) AS count FROM USER_CLICKSTREAM WINDOW TUMBLING (size 60 second) GROUP BY username, ip, city HAVING COUNT(*) > 1; Write the data out After processing the data, send it to Elasticsearch. -- Send data to Elasticsearch CREATE SINK CONNECTOR analyzed_clickstream WITH ( 'connector.class' = 'ElasticsearchSink', 'name' = 'recipe-elasticsearch-analyzed_clickstream', 'input.data.format' = 'JSON', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'USER_IP_ACTIVITY, ERRORS_PER_MIN_ALERT', 'connection.url' = '<elasticsearch-URI>', 'connection.user' = '<elasticsearch-username>', 'connection.password' = '<elasticsearch-password>', 'type.name' = 'type.name=kafkaconnect', 'key.ignore' = 'true', 'schema.ignore' = 'true' ); Full ksqlDB Statements Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of users CREATE SOURCE CONNECTOR datagen_clickstream_users WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_users', 'quickstart' = 'clickstream_users', 'maxInterval' = '10', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of per-user session information CREATE SOURCE CONNECTOR datagen_clickstream WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream', 'quickstart' = 'clickstream', 'maxInterval' = '30', 'format' = 'json'); -- stream of user clicks: CREATE STREAM clickstream ( _time bigint, time varchar, ip varchar, request varchar, status int, userid int, bytes bigint, agent varchar ) WITH ( kafka_topic = 'clickstream', value_format = 'json', partitions = 6 ); -- users lookup table: CREATE TABLE WEB_USERS ( user_id int primary key, registered_At BIGINT, username varchar, first_name varchar, last_name varchar, city varchar, level varchar ) WITH ( kafka_topic = 'clickstream_users', value_format = 'json', partitions = 6 ); -- Build materialized stream views: -- enrich click-stream with more user information: CREATE STREAM USER_CLICKSTREAM AS SELECT userid, u.username, ip, u.city, request, status, bytes FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id; -- Build materialized table views: -- Table of html pages per minute for each user: CREATE TABLE pages_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS pages FROM clickstream WINDOW HOPPING (size 60 second, advance by 5 second) WHERE request like '%html%' GROUP BY userid; -- User sessions table - 30 seconds of inactivity expires the session -- Table counts number of events within the session CREATE TABLE CLICK_USER_SESSIONS AS SELECT username as K, AS_VALUE(username) as username, WINDOWEND as EVENT_TS, count(*) AS events FROM USER_CLICKSTREAM window SESSION (30 second) GROUP BY username; -- number of errors per min, using 'HAVING' Filter to show ERROR codes > 400 where count > 5 CREATE TABLE ERRORS_PER_MIN_ALERT AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 20 second) WHERE status > 400 GROUP BY status HAVING count(*) > 5 AND count(*) is not NULL; -- Enriched user details table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE USER_IP_ACTIVITY WITH (key_format='json') AS SELECT username as k1, ip as k2, city as k3, AS_VALUE(username) as username, WINDOWSTART as EVENT_TS, AS_VALUE(ip) as ip, AS_VALUE(city) as city, COUNT(*) AS count FROM USER_CLICKSTREAM WINDOW TUMBLING (size 60 second) GROUP BY username, ip, city HAVING COUNT(*) > 1; -- Send data to Elasticsearch CREATE SINK CONNECTOR analyzed_clickstream WITH ( 'connector.class' = 'ElasticsearchSink', 'name' = 'recipe-elasticsearch-analyzed_clickstream', 'input.data.format' = 'JSON', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'USER_IP_ACTIVITY, ERRORS_PER_MIN_ALERT', 'connection.url' = '<elasticsearch-URI>', 'connection.user' = '<elasticsearch-username>', 'connection.password' = '<elasticsearch-password>', 'type.name' = 'type.name=kafkaconnect', 'key.ignore' = 'true', 'schema.ignore' = 'true' );","title":"Understand User Behavior with Clickstream Data"},{"location":"customer-360/clickstream/#understand-user-behavior-with-clickstream-data","text":"","title":"Understand User Behavior with Clickstream Data"},{"location":"customer-360/clickstream/#what-is-it","text":"Analyzing clickstream data enables businesses to understand the behavior of its online users, for example: User activity over a given time frame: how many webpages are users viewing Requests that end in error, over a given threshold (e.g., 404 HTTP codes) Where the requests are coming from geographically in a given window of time How long users are interacting with the site (user sessions)","title":"What is it?"},{"location":"customer-360/clickstream/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"customer-360/clickstream/#step-by-step","text":"","title":"Step-by-step"},{"location":"customer-360/clickstream/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks.","title":"Setup your Environment"},{"location":"customer-360/clickstream/#read-the-data-in","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). This recipe creates simulated data with the Datagen connector. -- Stream of users CREATE SOURCE CONNECTOR datagen_clickstream_users WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_users', 'quickstart' = 'clickstream_users', 'maxInterval' = '10', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of per-user session information CREATE SOURCE CONNECTOR datagen_clickstream WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream', 'quickstart' = 'clickstream', 'maxInterval' = '30', 'format' = 'json'); Optional: to simulate a real-world scenario where user sessions aren't just always open but do close after some time, you can pause and resume the DATAGEN_CLICKSTREAM connector.","title":"Read the data in"},{"location":"customer-360/clickstream/#run-stream-processing-app","text":"Now you can process the data in a variety of ways, by enriching the clickstream data with user information, analyze errors, aggregate data into windows of time, etc. -- stream of user clicks: CREATE STREAM clickstream ( _time bigint, time varchar, ip varchar, request varchar, status int, userid int, bytes bigint, agent varchar ) WITH ( kafka_topic = 'clickstream', value_format = 'json', partitions = 6 ); -- users lookup table: CREATE TABLE WEB_USERS ( user_id int primary key, registered_At BIGINT, username varchar, first_name varchar, last_name varchar, city varchar, level varchar ) WITH ( kafka_topic = 'clickstream_users', value_format = 'json', partitions = 6 ); -- Build materialized stream views: -- enrich click-stream with more user information: CREATE STREAM USER_CLICKSTREAM AS SELECT userid, u.username, ip, u.city, request, status, bytes FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id; -- Build materialized table views: -- Table of html pages per minute for each user: CREATE TABLE pages_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS pages FROM clickstream WINDOW HOPPING (size 60 second, advance by 5 second) WHERE request like '%html%' GROUP BY userid; -- User sessions table - 30 seconds of inactivity expires the session -- Table counts number of events within the session CREATE TABLE CLICK_USER_SESSIONS AS SELECT username as K, AS_VALUE(username) as username, WINDOWEND as EVENT_TS, count(*) AS events FROM USER_CLICKSTREAM window SESSION (30 second) GROUP BY username; -- number of errors per min, using 'HAVING' Filter to show ERROR codes > 400 where count > 5 CREATE TABLE ERRORS_PER_MIN_ALERT AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 20 second) WHERE status > 400 GROUP BY status HAVING count(*) > 5 AND count(*) is not NULL; -- Enriched user details table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE USER_IP_ACTIVITY WITH (key_format='json') AS SELECT username as k1, ip as k2, city as k3, AS_VALUE(username) as username, WINDOWSTART as EVENT_TS, AS_VALUE(ip) as ip, AS_VALUE(city) as city, COUNT(*) AS count FROM USER_CLICKSTREAM WINDOW TUMBLING (size 60 second) GROUP BY username, ip, city HAVING COUNT(*) > 1;","title":"Run stream processing app"},{"location":"customer-360/clickstream/#write-the-data-out","text":"After processing the data, send it to Elasticsearch. -- Send data to Elasticsearch CREATE SINK CONNECTOR analyzed_clickstream WITH ( 'connector.class' = 'ElasticsearchSink', 'name' = 'recipe-elasticsearch-analyzed_clickstream', 'input.data.format' = 'JSON', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'USER_IP_ACTIVITY, ERRORS_PER_MIN_ALERT', 'connection.url' = '<elasticsearch-URI>', 'connection.user' = '<elasticsearch-username>', 'connection.password' = '<elasticsearch-password>', 'type.name' = 'type.name=kafkaconnect', 'key.ignore' = 'true', 'schema.ignore' = 'true' );","title":"Write the data out"},{"location":"customer-360/clickstream/#full-ksqldb-statements","text":"Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of users CREATE SOURCE CONNECTOR datagen_clickstream_users WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream_users', 'quickstart' = 'clickstream_users', 'maxInterval' = '10', 'format' = 'json', 'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'); -- Stream of per-user session information CREATE SOURCE CONNECTOR datagen_clickstream WITH ( 'connector.class' = 'DatagenSource', 'kafka.topic' = 'clickstream', 'quickstart' = 'clickstream', 'maxInterval' = '30', 'format' = 'json'); -- stream of user clicks: CREATE STREAM clickstream ( _time bigint, time varchar, ip varchar, request varchar, status int, userid int, bytes bigint, agent varchar ) WITH ( kafka_topic = 'clickstream', value_format = 'json', partitions = 6 ); -- users lookup table: CREATE TABLE WEB_USERS ( user_id int primary key, registered_At BIGINT, username varchar, first_name varchar, last_name varchar, city varchar, level varchar ) WITH ( kafka_topic = 'clickstream_users', value_format = 'json', partitions = 6 ); -- Build materialized stream views: -- enrich click-stream with more user information: CREATE STREAM USER_CLICKSTREAM AS SELECT userid, u.username, ip, u.city, request, status, bytes FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id; -- Build materialized table views: -- Table of html pages per minute for each user: CREATE TABLE pages_per_min AS SELECT userid as k1, AS_VALUE(userid) as userid, WINDOWSTART as EVENT_TS, count(*) AS pages FROM clickstream WINDOW HOPPING (size 60 second, advance by 5 second) WHERE request like '%html%' GROUP BY userid; -- User sessions table - 30 seconds of inactivity expires the session -- Table counts number of events within the session CREATE TABLE CLICK_USER_SESSIONS AS SELECT username as K, AS_VALUE(username) as username, WINDOWEND as EVENT_TS, count(*) AS events FROM USER_CLICKSTREAM window SESSION (30 second) GROUP BY username; -- number of errors per min, using 'HAVING' Filter to show ERROR codes > 400 where count > 5 CREATE TABLE ERRORS_PER_MIN_ALERT AS SELECT status as k1, AS_VALUE(status) as status, WINDOWSTART as EVENT_TS, count(*) AS errors FROM clickstream window HOPPING (size 60 second, advance by 20 second) WHERE status > 400 GROUP BY status HAVING count(*) > 5 AND count(*) is not NULL; -- Enriched user details table: -- Aggregate (count&groupBy) using a TABLE-Window CREATE TABLE USER_IP_ACTIVITY WITH (key_format='json') AS SELECT username as k1, ip as k2, city as k3, AS_VALUE(username) as username, WINDOWSTART as EVENT_TS, AS_VALUE(ip) as ip, AS_VALUE(city) as city, COUNT(*) AS count FROM USER_CLICKSTREAM WINDOW TUMBLING (size 60 second) GROUP BY username, ip, city HAVING COUNT(*) > 1; -- Send data to Elasticsearch CREATE SINK CONNECTOR analyzed_clickstream WITH ( 'connector.class' = 'ElasticsearchSink', 'name' = 'recipe-elasticsearch-analyzed_clickstream', 'input.data.format' = 'JSON', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'USER_IP_ACTIVITY, ERRORS_PER_MIN_ALERT', 'connection.url' = '<elasticsearch-URI>', 'connection.user' = '<elasticsearch-username>', 'connection.password' = '<elasticsearch-password>', 'type.name' = 'type.name=kafkaconnect', 'key.ignore' = 'true', 'schema.ignore' = 'true' );","title":"Full ksqlDB Statements"},{"location":"fin-serv/credit-card-activity/","text":"Detect Unusual Credit Card Activity What is it? In banking, fraud can involve using stolen credit cards, forging checks, misleading accounting practices, etc. This recipe analyzes total credit card spend. If a customer exceeds their average spend, the account will be flagged as a possible case of credit card theft. Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks. Read the data in Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). -- Stream of transactions CREATE SOURCE CONNECTOR FD_transactions WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-transactions', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'TRANSACTIONS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR FD_customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams. Run stream processing app Now you can process the data in a variety of ways. -- Register the stream of transactions CREATE STREAM FD_TRANSACTIONS_RAW ( ACCOUNT_ID BIGINT, TIMESTAMP VARCHAR, CARD_TYPE VARCHAR, AMOUNT DOUBLE, IP_ADDRESS VARCHAR, TRANSACTION_ID VARCHAR ) WITH ( KAFKA_TOPIC='FD_transactions', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Repartition the stream on account_id in order to ensure that all the streams and tables are co-partitioned, which means that input records on both sides of the join have the same configuration settings for partitions. CREATE STREAM FD_TRANSACTIONS_SOURCE AS SELECT * FROM FD_TRANSACTIONS_RAW PARTITION BY ACCOUNT_ID; -- Register the existing stream of customer data CREATE STREAM FD_CUST_RAW_STREAM ( ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, AVG_CREDIT_SPEND DOUBLE ) WITH ( KAFKA_TOPIC='FD_customers', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Repartition the customer data stream by account_id to prepare for the join CREATE STREAM FD_CUSTOMER_REKEYED WITH (kafka_topic='FD_CUSTOMER_REKEYED') AS SELECT * FROM FD_CUST_RAW_STREAM PARTITION BY ID; -- Register the partitioned customer data topic as a table used for the join with the incoming stream of transactions: CREATE TABLE FD_customer ( ID BIGINT PRIMARY KEY, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, AVG_CREDIT_SPEND DOUBLE ) WITH ( KAFKA_TOPIC='FD_CUSTOMER_REKEYED', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Join the transactions to customer information: CREATE STREAM FD_TRANSACTIONS_ENRICHED AS SELECT T.ACCOUNT_ID, T.CARD_TYPE, T.AMOUNT, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME, C.AVG_CREDIT_SPEND FROM FD_TRANSACTIONS_SOURCE T INNER JOIN FD_CUSTOMER C ON T.ACCOUNT_ID = C.ID; -- Aggregate the stream of transactions for each account ID using a two-hour tumbling window, and filter for accounts in which the total spend in a two-hour period is greater than the customer\u2019s average: CREATE TABLE FD_POSSIBLE_STOLEN_CARD WITH (key_format='json') AS SELECT TIMESTAMPTOSTRING(WINDOWSTART, 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_START, T.ACCOUNT_ID, T.CARD_TYPE, SUM(T.AMOUNT) AS TOTAL_CREDIT_SPEND, T.FULL_NAME, MAX(T.AVG_CREDIT_SPEND) AS AVG_CREDIT_SPEND FROM FD_TRANSACTIONS_ENRICHED T WINDOW TUMBLING (SIZE 2 HOURS) GROUP BY T.ACCOUNT_ID, T.CARD_TYPE, T.FULL_NAME HAVING SUM(T.AMOUNT) > MAX(T.AVG_CREDIT_SPEND); In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO FD_transactions (account_id, timestamp, card_type, amount, ip_address, transaction_id) VALUES (6011000990139424, '2021-09-23T10:50:00.000Z', 'visa', 542.99, '192.168.44.1', '3985757'); INSERT INTO FD_transactions (account_id, timestamp, card_type, amount, ip_address, transaction_id) VALUES (6011000990139424, '2021-09-23T10:50:01.000Z', 'visa', 611.48, '192.168.44.1', '8028435'); INSERT INTO FD_transactions (account_id, timestamp, card_type, amount, ip_address, transaction_id) VALUES (3530111333300000, '2021-09-23T10:50:00.000Z', 'mastercard', 65.0, '192.168.101.3', '1695780'); INSERT INTO FD_customers (id, first_name, last_name, email, avg_credit_spend) VALUES (6011000990139424, 'Janice', 'Smith', 'jsmith@mycompany.com', 500.00); INSERT INTO FD_customers (id, first_name, last_name, email, avg_credit_spend) VALUES (3530111333300000, 'George', 'Mall', 'gmall@mycompany.com', 20.00); Full ksqlDB Statements Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of transactions CREATE SOURCE CONNECTOR FD_transactions WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-transactions', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'TRANSACTIONS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR FD_customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Register the stream of transactions CREATE STREAM FD_TRANSACTIONS_RAW ( ACCOUNT_ID BIGINT, TIMESTAMP VARCHAR, CARD_TYPE VARCHAR, AMOUNT DOUBLE, IP_ADDRESS VARCHAR, TRANSACTION_ID VARCHAR ) WITH ( KAFKA_TOPIC='FD_transactions', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Repartition the stream on account_id in order to ensure that all the streams and tables are co-partitioned, which means that input records on both sides of the join have the same configuration settings for partitions. CREATE STREAM FD_TRANSACTIONS_SOURCE AS SELECT * FROM FD_TRANSACTIONS_RAW PARTITION BY ACCOUNT_ID; -- Register the existing stream of customer data CREATE STREAM FD_CUST_RAW_STREAM ( ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, AVG_CREDIT_SPEND DOUBLE ) WITH ( KAFKA_TOPIC='FD_customers', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Repartition the customer data stream by account_id to prepare for the join CREATE STREAM FD_CUSTOMER_REKEYED WITH (kafka_topic='FD_CUSTOMER_REKEYED') AS SELECT * FROM FD_CUST_RAW_STREAM PARTITION BY ID; -- Register the partitioned customer data topic as a table used for the join with the incoming stream of transactions: CREATE TABLE FD_customer ( ID BIGINT PRIMARY KEY, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, AVG_CREDIT_SPEND DOUBLE ) WITH ( KAFKA_TOPIC='FD_CUSTOMER_REKEYED', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Join the transactions to customer information: CREATE STREAM FD_TRANSACTIONS_ENRICHED AS SELECT T.ACCOUNT_ID, T.CARD_TYPE, T.AMOUNT, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME, C.AVG_CREDIT_SPEND FROM FD_TRANSACTIONS_SOURCE T INNER JOIN FD_CUSTOMER C ON T.ACCOUNT_ID = C.ID; -- Aggregate the stream of transactions for each account ID using a two-hour tumbling window, and filter for accounts in which the total spend in a two-hour period is greater than the customer\u2019s average: CREATE TABLE FD_POSSIBLE_STOLEN_CARD WITH (key_format='json') AS SELECT TIMESTAMPTOSTRING(WINDOWSTART, 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_START, T.ACCOUNT_ID, T.CARD_TYPE, SUM(T.AMOUNT) AS TOTAL_CREDIT_SPEND, T.FULL_NAME, MAX(T.AVG_CREDIT_SPEND) AS AVG_CREDIT_SPEND FROM FD_TRANSACTIONS_ENRICHED T WINDOW TUMBLING (SIZE 2 HOURS) GROUP BY T.ACCOUNT_ID, T.CARD_TYPE, T.FULL_NAME HAVING SUM(T.AMOUNT) > MAX(T.AVG_CREDIT_SPEND);","title":"Detect Unusual Credit Card Activity"},{"location":"fin-serv/credit-card-activity/#detect-unusual-credit-card-activity","text":"","title":"Detect Unusual Credit Card Activity"},{"location":"fin-serv/credit-card-activity/#what-is-it","text":"In banking, fraud can involve using stolen credit cards, forging checks, misleading accounting practices, etc. This recipe analyzes total credit card spend. If a customer exceeds their average spend, the account will be flagged as a possible case of credit card theft.","title":"What is it?"},{"location":"fin-serv/credit-card-activity/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"fin-serv/credit-card-activity/#step-by-step","text":"","title":"Step-by-step"},{"location":"fin-serv/credit-card-activity/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks.","title":"Setup your Environment"},{"location":"fin-serv/credit-card-activity/#read-the-data-in","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). -- Stream of transactions CREATE SOURCE CONNECTOR FD_transactions WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-transactions', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'TRANSACTIONS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR FD_customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams.","title":"Read the data in"},{"location":"fin-serv/credit-card-activity/#run-stream-processing-app","text":"Now you can process the data in a variety of ways. -- Register the stream of transactions CREATE STREAM FD_TRANSACTIONS_RAW ( ACCOUNT_ID BIGINT, TIMESTAMP VARCHAR, CARD_TYPE VARCHAR, AMOUNT DOUBLE, IP_ADDRESS VARCHAR, TRANSACTION_ID VARCHAR ) WITH ( KAFKA_TOPIC='FD_transactions', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Repartition the stream on account_id in order to ensure that all the streams and tables are co-partitioned, which means that input records on both sides of the join have the same configuration settings for partitions. CREATE STREAM FD_TRANSACTIONS_SOURCE AS SELECT * FROM FD_TRANSACTIONS_RAW PARTITION BY ACCOUNT_ID; -- Register the existing stream of customer data CREATE STREAM FD_CUST_RAW_STREAM ( ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, AVG_CREDIT_SPEND DOUBLE ) WITH ( KAFKA_TOPIC='FD_customers', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Repartition the customer data stream by account_id to prepare for the join CREATE STREAM FD_CUSTOMER_REKEYED WITH (kafka_topic='FD_CUSTOMER_REKEYED') AS SELECT * FROM FD_CUST_RAW_STREAM PARTITION BY ID; -- Register the partitioned customer data topic as a table used for the join with the incoming stream of transactions: CREATE TABLE FD_customer ( ID BIGINT PRIMARY KEY, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, AVG_CREDIT_SPEND DOUBLE ) WITH ( KAFKA_TOPIC='FD_CUSTOMER_REKEYED', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Join the transactions to customer information: CREATE STREAM FD_TRANSACTIONS_ENRICHED AS SELECT T.ACCOUNT_ID, T.CARD_TYPE, T.AMOUNT, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME, C.AVG_CREDIT_SPEND FROM FD_TRANSACTIONS_SOURCE T INNER JOIN FD_CUSTOMER C ON T.ACCOUNT_ID = C.ID; -- Aggregate the stream of transactions for each account ID using a two-hour tumbling window, and filter for accounts in which the total spend in a two-hour period is greater than the customer\u2019s average: CREATE TABLE FD_POSSIBLE_STOLEN_CARD WITH (key_format='json') AS SELECT TIMESTAMPTOSTRING(WINDOWSTART, 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_START, T.ACCOUNT_ID, T.CARD_TYPE, SUM(T.AMOUNT) AS TOTAL_CREDIT_SPEND, T.FULL_NAME, MAX(T.AVG_CREDIT_SPEND) AS AVG_CREDIT_SPEND FROM FD_TRANSACTIONS_ENRICHED T WINDOW TUMBLING (SIZE 2 HOURS) GROUP BY T.ACCOUNT_ID, T.CARD_TYPE, T.FULL_NAME HAVING SUM(T.AMOUNT) > MAX(T.AVG_CREDIT_SPEND); In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO FD_transactions (account_id, timestamp, card_type, amount, ip_address, transaction_id) VALUES (6011000990139424, '2021-09-23T10:50:00.000Z', 'visa', 542.99, '192.168.44.1', '3985757'); INSERT INTO FD_transactions (account_id, timestamp, card_type, amount, ip_address, transaction_id) VALUES (6011000990139424, '2021-09-23T10:50:01.000Z', 'visa', 611.48, '192.168.44.1', '8028435'); INSERT INTO FD_transactions (account_id, timestamp, card_type, amount, ip_address, transaction_id) VALUES (3530111333300000, '2021-09-23T10:50:00.000Z', 'mastercard', 65.0, '192.168.101.3', '1695780'); INSERT INTO FD_customers (id, first_name, last_name, email, avg_credit_spend) VALUES (6011000990139424, 'Janice', 'Smith', 'jsmith@mycompany.com', 500.00); INSERT INTO FD_customers (id, first_name, last_name, email, avg_credit_spend) VALUES (3530111333300000, 'George', 'Mall', 'gmall@mycompany.com', 20.00);","title":"Run stream processing app"},{"location":"fin-serv/credit-card-activity/#full-ksqldb-statements","text":"Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of transactions CREATE SOURCE CONNECTOR FD_transactions WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-transactions', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'TRANSACTIONS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR FD_customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Register the stream of transactions CREATE STREAM FD_TRANSACTIONS_RAW ( ACCOUNT_ID BIGINT, TIMESTAMP VARCHAR, CARD_TYPE VARCHAR, AMOUNT DOUBLE, IP_ADDRESS VARCHAR, TRANSACTION_ID VARCHAR ) WITH ( KAFKA_TOPIC='FD_transactions', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Repartition the stream on account_id in order to ensure that all the streams and tables are co-partitioned, which means that input records on both sides of the join have the same configuration settings for partitions. CREATE STREAM FD_TRANSACTIONS_SOURCE AS SELECT * FROM FD_TRANSACTIONS_RAW PARTITION BY ACCOUNT_ID; -- Register the existing stream of customer data CREATE STREAM FD_CUST_RAW_STREAM ( ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, AVG_CREDIT_SPEND DOUBLE ) WITH ( KAFKA_TOPIC='FD_customers', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Repartition the customer data stream by account_id to prepare for the join CREATE STREAM FD_CUSTOMER_REKEYED WITH (kafka_topic='FD_CUSTOMER_REKEYED') AS SELECT * FROM FD_CUST_RAW_STREAM PARTITION BY ID; -- Register the partitioned customer data topic as a table used for the join with the incoming stream of transactions: CREATE TABLE FD_customer ( ID BIGINT PRIMARY KEY, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, AVG_CREDIT_SPEND DOUBLE ) WITH ( KAFKA_TOPIC='FD_CUSTOMER_REKEYED', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Join the transactions to customer information: CREATE STREAM FD_TRANSACTIONS_ENRICHED AS SELECT T.ACCOUNT_ID, T.CARD_TYPE, T.AMOUNT, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME, C.AVG_CREDIT_SPEND FROM FD_TRANSACTIONS_SOURCE T INNER JOIN FD_CUSTOMER C ON T.ACCOUNT_ID = C.ID; -- Aggregate the stream of transactions for each account ID using a two-hour tumbling window, and filter for accounts in which the total spend in a two-hour period is greater than the customer\u2019s average: CREATE TABLE FD_POSSIBLE_STOLEN_CARD WITH (key_format='json') AS SELECT TIMESTAMPTOSTRING(WINDOWSTART, 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_START, T.ACCOUNT_ID, T.CARD_TYPE, SUM(T.AMOUNT) AS TOTAL_CREDIT_SPEND, T.FULL_NAME, MAX(T.AVG_CREDIT_SPEND) AS AVG_CREDIT_SPEND FROM FD_TRANSACTIONS_ENRICHED T WINDOW TUMBLING (SIZE 2 HOURS) GROUP BY T.ACCOUNT_ID, T.CARD_TYPE, T.FULL_NAME HAVING SUM(T.AMOUNT) > MAX(T.AVG_CREDIT_SPEND);","title":"Full ksqlDB Statements"},{"location":"fin-serv/denormalization/","text":"Denormalize Change Data Capture (CDC) for Orders What is it? If you have transactional events for orders in a marketplace, you can stream the Change Data Capture (CDC) and denormalize the events. Denormalization is a well-established pattern for performance because querying a single table of enriched data will often perform better than querying across multiple at runtime. You can consume the denormalized events from downstream applications in your business, or stream them to another destination. This recipe demonstrates this principle by streaming from a SQL Server, denormalizing the data, and writing to Snowflake. Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks. Read the data in Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). Change Data Capture (CDC) for orders is being written to a SQL Server database, and there is an Oracle database with customer data. -- Stream of orders CREATE SOURCE CONNECTOR orders WITH ( 'connector.class' = 'SqlServerCdcSource', 'name' = 'recipe-sqlservercdc-orders', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'database.hostname' = '<db-name>', 'database.port' = '1433', 'database.user' = '<database-username>', 'database.password' = '<database-password>', 'database.dbname' = 'database-name', 'database.server.name' = 'sql', 'table.include.list' ='<table_name>', 'snapshot.mode' = 'initial', 'output.data.format' = 'JSON', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topic.prefix' = 'oracle_', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams. Run stream processing app This streams the user orders and denormalizes the data by joining facts (orders) with the dimension (customer). -- Stream of user orders: CREATE STREAM orders ( ORDER_ID BIGINT, CUSTOMER_ID BIGINT, ITEM VARCHAR, ORDER_TOTAL_USD DOUBLE ) WITH ( kafka_topic = 'orders', value_format = 'json', partitions = 6 ); -- Register the existing stream of customer data CREATE STREAM CUST_RAW_STREAM ( ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR ) WITH ( KAFKA_TOPIC='customers', VALUE_FORMAT='JSON', PARTITIONS = 6 ); -- Register the customer data topic as a table CREATE TABLE customer ( ID BIGINT PRIMARY KEY ) WITH ( KAFKA_TOPIC='CUST_RAW_STREAM', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Denormalize data: joining facts (orders) with the dimension (customer) CREATE STREAM ORDERS_ENRICHED AS SELECT O.order_id AS order_id, O.item AS item, O.order_total_usd AS order_total_usd, C.first_name || ' ' || C.last_name AS full_name, C.email AS email FROM ORDERS O LEFT JOIN CUSTOMERS C ON O.customer_id = C.id; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697328, 375, 'book', 29.99); INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697329, 375, 'guitar', 215.99); INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697330, 983, 'thermometer', 12.99); INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697331, 983, 'scarf', 32.99); INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697332, 375, 'doormat', 15.99); INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697333, 983, 'clippers', 65.99); INSERT INTO customers (id, first_name, last_name, email) VALUES (375, 'Janice', 'Smith', 'jsmith@mycompany.com'); INSERT INTO customers (id, first_name, last_name, email) VALUES (983, 'George', 'Mall', 'gmall@mycompany.com'); Write the data out Any downstream application or database can receive the denormalized data. -- Send data to Snowflake CREATE SINK CONNECTOR orders_enriched WITH ( 'connector.class' = 'SnowflakeSink', 'name' = 'recipe-snowflake-analyzed_clickstream', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'ORDERS_ENRICHED', 'input.data.format' = 'JSON', 'snowflake.url.name' = 'https=//wm83168.us-central1.gcp.snowflakecomputing.com=443', 'snowflake.user.name' = '<login-username>', 'snowflake.private.key' = '<private-key>', 'snowflake.database.name' = '<database-name>', 'snowflake.schema.name' = '<schema-name>', 'tasks.max' = '1'); Full ksqlDB Statements Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of orders CREATE SOURCE CONNECTOR orders WITH ( 'connector.class' = 'SqlServerCdcSource', 'name' = 'recipe-sqlservercdc-orders', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'database.hostname' = '<db-name>', 'database.port' = '1433', 'database.user' = '<database-username>', 'database.password' = '<database-password>', 'database.dbname' = 'database-name', 'database.server.name' = 'sql', 'table.include.list' ='<table_name>', 'snapshot.mode' = 'initial', 'output.data.format' = 'JSON', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topic.prefix' = 'oracle_', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Stream of user orders: CREATE STREAM orders ( ORDER_ID BIGINT, CUSTOMER_ID BIGINT, ITEM VARCHAR, ORDER_TOTAL_USD DOUBLE ) WITH ( kafka_topic = 'orders', value_format = 'json', partitions = 6 ); -- Register the existing stream of customer data CREATE STREAM CUST_RAW_STREAM ( ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR ) WITH ( KAFKA_TOPIC='customers', VALUE_FORMAT='JSON', PARTITIONS = 6 ); -- Register the customer data topic as a table CREATE TABLE customer ( ID BIGINT PRIMARY KEY ) WITH ( KAFKA_TOPIC='CUST_RAW_STREAM', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Denormalize data: joining facts (orders) with the dimension (customer) CREATE STREAM ORDERS_ENRICHED AS SELECT O.order_id AS order_id, O.item AS item, O.order_total_usd AS order_total_usd, C.first_name || ' ' || C.last_name AS full_name, C.email AS email FROM ORDERS O LEFT JOIN CUSTOMERS C ON O.customer_id = C.id; -- Send data to Snowflake CREATE SINK CONNECTOR orders_enriched WITH ( 'connector.class' = 'SnowflakeSink', 'name' = 'recipe-snowflake-analyzed_clickstream', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'ORDERS_ENRICHED', 'input.data.format' = 'JSON', 'snowflake.url.name' = 'https=//wm83168.us-central1.gcp.snowflakecomputing.com=443', 'snowflake.user.name' = '<login-username>', 'snowflake.private.key' = '<private-key>', 'snowflake.database.name' = '<database-name>', 'snowflake.schema.name' = '<schema-name>', 'tasks.max' = '1');","title":"Denormalize Change Data Capture (CDC) for Orders"},{"location":"fin-serv/denormalization/#denormalize-change-data-capture-cdc-for-orders","text":"","title":"Denormalize Change Data Capture (CDC) for Orders"},{"location":"fin-serv/denormalization/#what-is-it","text":"If you have transactional events for orders in a marketplace, you can stream the Change Data Capture (CDC) and denormalize the events. Denormalization is a well-established pattern for performance because querying a single table of enriched data will often perform better than querying across multiple at runtime. You can consume the denormalized events from downstream applications in your business, or stream them to another destination. This recipe demonstrates this principle by streaming from a SQL Server, denormalizing the data, and writing to Snowflake.","title":"What is it?"},{"location":"fin-serv/denormalization/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"fin-serv/denormalization/#step-by-step","text":"","title":"Step-by-step"},{"location":"fin-serv/denormalization/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks.","title":"Setup your Environment"},{"location":"fin-serv/denormalization/#read-the-data-in","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). Change Data Capture (CDC) for orders is being written to a SQL Server database, and there is an Oracle database with customer data. -- Stream of orders CREATE SOURCE CONNECTOR orders WITH ( 'connector.class' = 'SqlServerCdcSource', 'name' = 'recipe-sqlservercdc-orders', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'database.hostname' = '<db-name>', 'database.port' = '1433', 'database.user' = '<database-username>', 'database.password' = '<database-password>', 'database.dbname' = 'database-name', 'database.server.name' = 'sql', 'table.include.list' ='<table_name>', 'snapshot.mode' = 'initial', 'output.data.format' = 'JSON', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topic.prefix' = 'oracle_', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams.","title":"Read the data in"},{"location":"fin-serv/denormalization/#run-stream-processing-app","text":"This streams the user orders and denormalizes the data by joining facts (orders) with the dimension (customer). -- Stream of user orders: CREATE STREAM orders ( ORDER_ID BIGINT, CUSTOMER_ID BIGINT, ITEM VARCHAR, ORDER_TOTAL_USD DOUBLE ) WITH ( kafka_topic = 'orders', value_format = 'json', partitions = 6 ); -- Register the existing stream of customer data CREATE STREAM CUST_RAW_STREAM ( ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR ) WITH ( KAFKA_TOPIC='customers', VALUE_FORMAT='JSON', PARTITIONS = 6 ); -- Register the customer data topic as a table CREATE TABLE customer ( ID BIGINT PRIMARY KEY ) WITH ( KAFKA_TOPIC='CUST_RAW_STREAM', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Denormalize data: joining facts (orders) with the dimension (customer) CREATE STREAM ORDERS_ENRICHED AS SELECT O.order_id AS order_id, O.item AS item, O.order_total_usd AS order_total_usd, C.first_name || ' ' || C.last_name AS full_name, C.email AS email FROM ORDERS O LEFT JOIN CUSTOMERS C ON O.customer_id = C.id; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697328, 375, 'book', 29.99); INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697329, 375, 'guitar', 215.99); INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697330, 983, 'thermometer', 12.99); INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697331, 983, 'scarf', 32.99); INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697332, 375, 'doormat', 15.99); INSERT INTO orders (order_id, customer_id, item, order_total_usd) VALUES (44697333, 983, 'clippers', 65.99); INSERT INTO customers (id, first_name, last_name, email) VALUES (375, 'Janice', 'Smith', 'jsmith@mycompany.com'); INSERT INTO customers (id, first_name, last_name, email) VALUES (983, 'George', 'Mall', 'gmall@mycompany.com');","title":"Run stream processing app"},{"location":"fin-serv/denormalization/#write-the-data-out","text":"Any downstream application or database can receive the denormalized data. -- Send data to Snowflake CREATE SINK CONNECTOR orders_enriched WITH ( 'connector.class' = 'SnowflakeSink', 'name' = 'recipe-snowflake-analyzed_clickstream', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'ORDERS_ENRICHED', 'input.data.format' = 'JSON', 'snowflake.url.name' = 'https=//wm83168.us-central1.gcp.snowflakecomputing.com=443', 'snowflake.user.name' = '<login-username>', 'snowflake.private.key' = '<private-key>', 'snowflake.database.name' = '<database-name>', 'snowflake.schema.name' = '<schema-name>', 'tasks.max' = '1');","title":"Write the data out"},{"location":"fin-serv/denormalization/#full-ksqldb-statements","text":"Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of orders CREATE SOURCE CONNECTOR orders WITH ( 'connector.class' = 'SqlServerCdcSource', 'name' = 'recipe-sqlservercdc-orders', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'database.hostname' = '<db-name>', 'database.port' = '1433', 'database.user' = '<database-username>', 'database.password' = '<database-password>', 'database.dbname' = 'database-name', 'database.server.name' = 'sql', 'table.include.list' ='<table_name>', 'snapshot.mode' = 'initial', 'output.data.format' = 'JSON', 'tasks.max' = '1'); -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'OracleDatabaseSource', 'name' = 'recipe-oracle-customers', 'connector.class' = 'OracleDatabaseSource', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topic.prefix' = 'oracle_', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '1521', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'CUSTOMERS', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UCT', 'tasks.max' = '1'); -- Stream of user orders: CREATE STREAM orders ( ORDER_ID BIGINT, CUSTOMER_ID BIGINT, ITEM VARCHAR, ORDER_TOTAL_USD DOUBLE ) WITH ( kafka_topic = 'orders', value_format = 'json', partitions = 6 ); -- Register the existing stream of customer data CREATE STREAM CUST_RAW_STREAM ( ID BIGINT, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR ) WITH ( KAFKA_TOPIC='customers', VALUE_FORMAT='JSON', PARTITIONS = 6 ); -- Register the customer data topic as a table CREATE TABLE customer ( ID BIGINT PRIMARY KEY ) WITH ( KAFKA_TOPIC='CUST_RAW_STREAM', VALUE_FORMAT='JSON', PARTITIONS=6 ); -- Denormalize data: joining facts (orders) with the dimension (customer) CREATE STREAM ORDERS_ENRICHED AS SELECT O.order_id AS order_id, O.item AS item, O.order_total_usd AS order_total_usd, C.first_name || ' ' || C.last_name AS full_name, C.email AS email FROM ORDERS O LEFT JOIN CUSTOMERS C ON O.customer_id = C.id; -- Send data to Snowflake CREATE SINK CONNECTOR orders_enriched WITH ( 'connector.class' = 'SnowflakeSink', 'name' = 'recipe-snowflake-analyzed_clickstream', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'topics' = 'ORDERS_ENRICHED', 'input.data.format' = 'JSON', 'snowflake.url.name' = 'https=//wm83168.us-central1.gcp.snowflakecomputing.com=443', 'snowflake.user.name' = '<login-username>', 'snowflake.private.key' = '<private-key>', 'snowflake.database.name' = '<database-name>', 'snowflake.schema.name' = '<schema-name>', 'tasks.max' = '1');","title":"Full ksqlDB Statements"},{"location":"fin-serv/payment-status-check/","text":"Check Payment Requests What is it? With financial services, it is useful to do real-time checking of customer payment requests. This recipe shows you how to validate payments against available funds and anti-money-laundering (AML) policies. Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks. Read the data in Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-customers', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'customers', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of payments CREATE SOURCE CONNECTOR payments WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-payments', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'payments', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of aml_status CREATE SOURCE CONNECTOR aml_status WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-aml_status', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'aml_status', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of funds_status CREATE SOURCE CONNECTOR funds_status WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-funds_status', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'funds_status', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams. Run stream processing app Now you can process the data in a variety of ways. -- Register the initial streams and tables from the Kafka topics CREATE STREAM PAYMENTS ( PAYMENT_ID INTEGER KEY, CUSTID INTEGER, ACCOUNTID INTEGER, AMOUNT INTEGER, BANK VARCHAR ) WITH ( kafka_topic='payments', value_format='json', PARTITIONS=6 ); create stream aml_status ( PAYMENT_ID INTEGER, BANK VARCHAR, STATUS VARCHAR ) with ( kafka_topic='aml_status', value_format='json', PARTITIONS=6 ); create stream funds_status ( PAYMENT_ID INTEGER, REASON_CODE VARCHAR, STATUS VARCHAR ) with ( kafka_topic='funds_status', value_format='json', PARTITIONS=6 ); create table customers ( ID INTEGER PRIMARY KEY, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, GENDER VARCHAR, STATUS360 VARCHAR ) WITH ( kafka_topic='customers', value_format='JSON', PARTITIONS=6 ); -- Enrich Payments stream with Customers table CREATE STREAM enriched_payments AS SELECT p.payment_id as payment_id, p.custid as customer_id, p.accountid, p.amount, p.bank, c.first_name, c.last_name, c.email, c.status360 from payments p left join customers c on p.custid = c.id; -- Combine the status streams CREATE STREAM payment_statuses AS SELECT payment_id, status, 'AML' as source_system FROM aml_status; INSERT INTO payment_statuses SELECT payment_id, status, 'FUNDS' as source_system FROM funds_status; -- Combine payment and status events in 1 hour window. Why we need a timing window for stream-stream join? CREATE STREAM payments_with_status AS SELECT ep.payment_id as payment_id, ep.accountid, ep.amount, ep.bank, ep.first_name, ep.last_name, ep.email, ep.status360, ps.status, ps.source_system FROM enriched_payments ep LEFT JOIN payment_statuses ps WITHIN 1 HOUR ON ep.payment_id = ps.payment_id ; -- Aggregate data to the final table CREATE TABLE payments_final AS SELECT payment_id, histogram(status) as status_counts, collect_list('{ \"system\" : \"' + source_system + '\", \"status\" : \"' + STATUS + '\"}') as service_status_list FROM payments_with_status WHERE status is not null GROUP BY payment_id; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: -- Customer Data INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (10,'Brena','Tollerton','btollerton9@furl.net','Female','silver'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (9,'Even','Tinham','etinham8@facebook.com','Male','silver'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (8,'Patti','Rosten','prosten7@ihg.com','Female','silver'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (7,'Fay','Huc','fhuc6@quantcast.com','Female','bronze'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (6,'Robinet','Leheude','rleheude5@reddit.com','Female','platinum'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (5,'Hansiain','Coda','hcoda4@senate.gov','Male','platinum'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (4,'Hashim','Rumke','hrumke3@sohu.com','Male','platinum'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (3,'Mariejeanne','Cocci','mcocci2@techcrunch.com','Female','bronze'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (2,'Ruthie','Brockherst','rbrockherst1@ow.ly','Female','platinum'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (1,'Rica','Blaisdell','rblaisdell0@rambler.ru','Female','bronze'); -- Payment Instruction Data INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (1,1,1234000,100,'DBS'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (3,2,1234100,200,'Barclays Bank'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (5,3,1234200,300,'BNP Paribas'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (7,4,1234300,400,'Wells Fargo'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (9,5,1234400,500,'DBS'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (11,6,1234500,600,'Royal Bank of Canada'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (13,7,1234600,700,'DBS'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (15,8,1234700,800,'DBS'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (17,9,1234800,900,'DBS'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (19,10,1234900,1000,'United Overseas Bank'); -- AML Status Data INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (1,'Wells Fargo','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (3,'Commonwealth Bank of Australia','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (5,'Deutsche Bank','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (7,'DBS','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (9,'United Overseas Bank','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (11,'Citi','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (13,'Commonwealth Bank of Australia','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (15,'Barclays Bank','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (17,'United Overseas Bank','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (19,'Royal Bank of Canada','OK'); -- Funds Status Data INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (1,'00','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (3,'99','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (5,'30','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (7,'00','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (9,'00','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (11,'00','NOT OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (13,'30','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (15,'00','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (17,'10','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (19,'10','OK'); Full ksqlDB Statements Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-customers', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'customers', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of payments CREATE SOURCE CONNECTOR payments WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-payments', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'payments', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of aml_status CREATE SOURCE CONNECTOR aml_status WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-aml_status', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'aml_status', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of funds_status CREATE SOURCE CONNECTOR funds_status WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-funds_status', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'funds_status', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Register the initial streams and tables from the Kafka topics CREATE STREAM PAYMENTS ( PAYMENT_ID INTEGER KEY, CUSTID INTEGER, ACCOUNTID INTEGER, AMOUNT INTEGER, BANK VARCHAR ) WITH ( kafka_topic='payments', value_format='json', PARTITIONS=6 ); create stream aml_status ( PAYMENT_ID INTEGER, BANK VARCHAR, STATUS VARCHAR ) with ( kafka_topic='aml_status', value_format='json', PARTITIONS=6 ); create stream funds_status ( PAYMENT_ID INTEGER, REASON_CODE VARCHAR, STATUS VARCHAR ) with ( kafka_topic='funds_status', value_format='json', PARTITIONS=6 ); create table customers ( ID INTEGER PRIMARY KEY, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, GENDER VARCHAR, STATUS360 VARCHAR ) WITH ( kafka_topic='customers', value_format='JSON', PARTITIONS=6 ); -- Enrich Payments stream with Customers table CREATE STREAM enriched_payments AS SELECT p.payment_id as payment_id, p.custid as customer_id, p.accountid, p.amount, p.bank, c.first_name, c.last_name, c.email, c.status360 from payments p left join customers c on p.custid = c.id; -- Combine the status streams CREATE STREAM payment_statuses AS SELECT payment_id, status, 'AML' as source_system FROM aml_status; INSERT INTO payment_statuses SELECT payment_id, status, 'FUNDS' as source_system FROM funds_status; -- Combine payment and status events in 1 hour window. Why we need a timing window for stream-stream join? CREATE STREAM payments_with_status AS SELECT ep.payment_id as payment_id, ep.accountid, ep.amount, ep.bank, ep.first_name, ep.last_name, ep.email, ep.status360, ps.status, ps.source_system FROM enriched_payments ep LEFT JOIN payment_statuses ps WITHIN 1 HOUR ON ep.payment_id = ps.payment_id ; -- Aggregate data to the final table CREATE TABLE payments_final AS SELECT payment_id, histogram(status) as status_counts, collect_list('{ \"system\" : \"' + source_system + '\", \"status\" : \"' + STATUS + '\"}') as service_status_list FROM payments_with_status WHERE status is not null GROUP BY payment_id;","title":"Check Payment Requests"},{"location":"fin-serv/payment-status-check/#check-payment-requests","text":"","title":"Check Payment Requests"},{"location":"fin-serv/payment-status-check/#what-is-it","text":"With financial services, it is useful to do real-time checking of customer payment requests. This recipe shows you how to validate payments against available funds and anti-money-laundering (AML) policies.","title":"What is it?"},{"location":"fin-serv/payment-status-check/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"fin-serv/payment-status-check/#step-by-step","text":"","title":"Step-by-step"},{"location":"fin-serv/payment-status-check/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks.","title":"Setup your Environment"},{"location":"fin-serv/payment-status-check/#read-the-data-in","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-customers', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'customers', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of payments CREATE SOURCE CONNECTOR payments WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-payments', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'payments', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of aml_status CREATE SOURCE CONNECTOR aml_status WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-aml_status', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'aml_status', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of funds_status CREATE SOURCE CONNECTOR funds_status WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-funds_status', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'funds_status', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams.","title":"Read the data in"},{"location":"fin-serv/payment-status-check/#run-stream-processing-app","text":"Now you can process the data in a variety of ways. -- Register the initial streams and tables from the Kafka topics CREATE STREAM PAYMENTS ( PAYMENT_ID INTEGER KEY, CUSTID INTEGER, ACCOUNTID INTEGER, AMOUNT INTEGER, BANK VARCHAR ) WITH ( kafka_topic='payments', value_format='json', PARTITIONS=6 ); create stream aml_status ( PAYMENT_ID INTEGER, BANK VARCHAR, STATUS VARCHAR ) with ( kafka_topic='aml_status', value_format='json', PARTITIONS=6 ); create stream funds_status ( PAYMENT_ID INTEGER, REASON_CODE VARCHAR, STATUS VARCHAR ) with ( kafka_topic='funds_status', value_format='json', PARTITIONS=6 ); create table customers ( ID INTEGER PRIMARY KEY, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, GENDER VARCHAR, STATUS360 VARCHAR ) WITH ( kafka_topic='customers', value_format='JSON', PARTITIONS=6 ); -- Enrich Payments stream with Customers table CREATE STREAM enriched_payments AS SELECT p.payment_id as payment_id, p.custid as customer_id, p.accountid, p.amount, p.bank, c.first_name, c.last_name, c.email, c.status360 from payments p left join customers c on p.custid = c.id; -- Combine the status streams CREATE STREAM payment_statuses AS SELECT payment_id, status, 'AML' as source_system FROM aml_status; INSERT INTO payment_statuses SELECT payment_id, status, 'FUNDS' as source_system FROM funds_status; -- Combine payment and status events in 1 hour window. Why we need a timing window for stream-stream join? CREATE STREAM payments_with_status AS SELECT ep.payment_id as payment_id, ep.accountid, ep.amount, ep.bank, ep.first_name, ep.last_name, ep.email, ep.status360, ps.status, ps.source_system FROM enriched_payments ep LEFT JOIN payment_statuses ps WITHIN 1 HOUR ON ep.payment_id = ps.payment_id ; -- Aggregate data to the final table CREATE TABLE payments_final AS SELECT payment_id, histogram(status) as status_counts, collect_list('{ \"system\" : \"' + source_system + '\", \"status\" : \"' + STATUS + '\"}') as service_status_list FROM payments_with_status WHERE status is not null GROUP BY payment_id; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: -- Customer Data INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (10,'Brena','Tollerton','btollerton9@furl.net','Female','silver'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (9,'Even','Tinham','etinham8@facebook.com','Male','silver'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (8,'Patti','Rosten','prosten7@ihg.com','Female','silver'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (7,'Fay','Huc','fhuc6@quantcast.com','Female','bronze'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (6,'Robinet','Leheude','rleheude5@reddit.com','Female','platinum'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (5,'Hansiain','Coda','hcoda4@senate.gov','Male','platinum'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (4,'Hashim','Rumke','hrumke3@sohu.com','Male','platinum'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (3,'Mariejeanne','Cocci','mcocci2@techcrunch.com','Female','bronze'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (2,'Ruthie','Brockherst','rbrockherst1@ow.ly','Female','platinum'); INSERT INTO customers (id, FIRST_NAME, LAST_NAME, EMAIL, GENDER, STATUS360) VALUES (1,'Rica','Blaisdell','rblaisdell0@rambler.ru','Female','bronze'); -- Payment Instruction Data INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (1,1,1234000,100,'DBS'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (3,2,1234100,200,'Barclays Bank'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (5,3,1234200,300,'BNP Paribas'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (7,4,1234300,400,'Wells Fargo'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (9,5,1234400,500,'DBS'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (11,6,1234500,600,'Royal Bank of Canada'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (13,7,1234600,700,'DBS'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (15,8,1234700,800,'DBS'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (17,9,1234800,900,'DBS'); INSERT INTO payments (PAYMENT_ID, CUSTID, ACCOUNTID, AMOUNT, BANK) VALUES (19,10,1234900,1000,'United Overseas Bank'); -- AML Status Data INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (1,'Wells Fargo','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (3,'Commonwealth Bank of Australia','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (5,'Deutsche Bank','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (7,'DBS','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (9,'United Overseas Bank','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (11,'Citi','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (13,'Commonwealth Bank of Australia','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (15,'Barclays Bank','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (17,'United Overseas Bank','OK'); INSERT INTO aml_status(PAYMENT_ID,BANK,STATUS) VALUES (19,'Royal Bank of Canada','OK'); -- Funds Status Data INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (1,'00','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (3,'99','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (5,'30','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (7,'00','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (9,'00','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (11,'00','NOT OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (13,'30','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (15,'00','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (17,'10','OK'); INSERT INTO funds_status(PAYMENT_ID,REASON_CODE,STATUS) VALUES (19,'10','OK');","title":"Run stream processing app"},{"location":"fin-serv/payment-status-check/#full-ksqldb-statements","text":"Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of customers CREATE SOURCE CONNECTOR customers WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-customers', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'customers', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of payments CREATE SOURCE CONNECTOR payments WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-payments', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'payments', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of aml_status CREATE SOURCE CONNECTOR aml_status WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-aml_status', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'aml_status', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Stream of funds_status CREATE SOURCE CONNECTOR funds_status WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-payment-status-check-funds_status', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'funds_status', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Register the initial streams and tables from the Kafka topics CREATE STREAM PAYMENTS ( PAYMENT_ID INTEGER KEY, CUSTID INTEGER, ACCOUNTID INTEGER, AMOUNT INTEGER, BANK VARCHAR ) WITH ( kafka_topic='payments', value_format='json', PARTITIONS=6 ); create stream aml_status ( PAYMENT_ID INTEGER, BANK VARCHAR, STATUS VARCHAR ) with ( kafka_topic='aml_status', value_format='json', PARTITIONS=6 ); create stream funds_status ( PAYMENT_ID INTEGER, REASON_CODE VARCHAR, STATUS VARCHAR ) with ( kafka_topic='funds_status', value_format='json', PARTITIONS=6 ); create table customers ( ID INTEGER PRIMARY KEY, FIRST_NAME VARCHAR, LAST_NAME VARCHAR, EMAIL VARCHAR, GENDER VARCHAR, STATUS360 VARCHAR ) WITH ( kafka_topic='customers', value_format='JSON', PARTITIONS=6 ); -- Enrich Payments stream with Customers table CREATE STREAM enriched_payments AS SELECT p.payment_id as payment_id, p.custid as customer_id, p.accountid, p.amount, p.bank, c.first_name, c.last_name, c.email, c.status360 from payments p left join customers c on p.custid = c.id; -- Combine the status streams CREATE STREAM payment_statuses AS SELECT payment_id, status, 'AML' as source_system FROM aml_status; INSERT INTO payment_statuses SELECT payment_id, status, 'FUNDS' as source_system FROM funds_status; -- Combine payment and status events in 1 hour window. Why we need a timing window for stream-stream join? CREATE STREAM payments_with_status AS SELECT ep.payment_id as payment_id, ep.accountid, ep.amount, ep.bank, ep.first_name, ep.last_name, ep.email, ep.status360, ps.status, ps.source_system FROM enriched_payments ep LEFT JOIN payment_statuses ps WITHIN 1 HOUR ON ep.payment_id = ps.payment_id ; -- Aggregate data to the final table CREATE TABLE payments_final AS SELECT payment_id, histogram(status) as status_counts, collect_list('{ \"system\" : \"' + source_system + '\", \"status\" : \"' + STATUS + '\"}') as service_status_list FROM payments_with_status WHERE status is not null GROUP BY payment_id;","title":"Full ksqlDB Statements"},{"location":"internet-of-things/coalesce/","text":"Coalesce Telemetry What is it? With Internet of Things (IoT), devices can emit a lot of telemetry, and it may be difficult to analyze that information to determine if something is \"wrong\". This recipe shows you how to process and coalesce that telemetry using ksqlDB and flag devices that warrant more investigation. Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-Step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks. Read the data in Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). In this example, the telemetry is stored in two tables in a database and is read into 2 Kafak topics in Confluent Cloud. CREATE SOURCE CONNECTOR telemetry WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-iot', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = '<table>', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams. Run stream processing app In this example, there is one stream of data reporting device threshold values and another reporting alarms. The following stream processing app identifies which set of devices need to be investigated where threshold is insufficient and alarm code is not zero. -- Create table with latest state of alarms CREATE TABLE alarms ( device_id STRING PRIMARY key, alarm_name STRING, code INT ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='alarms', PARTITIONS = 6); -- Create stream of throughputs CREATE STREAM throughputs ( device_id STRING key, throughput DOUBLE ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='throughputs', PARTITIONS = 6); -- Filter throughputs below threshold 1000.0 CREATE STREAM throughput_insufficient AS SELECT * FROM throughputs WHERE throughput < 1000.0 EMIT CHANGES; -- Create new stream where threshold is insufficient and alarm code is not 0 CREATE STREAM critical_issues_to_investigate AS SELECT t.device_id, t.throughput, a.alarm_name, a.code FROM throughput_insufficient t LEFT JOIN alarms a ON t.device_id = a.device_id WHERE a.code != 0; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO alarms (device_id, alarm_name, code) VALUES ('d1', 'CHANNEL_CREATE', 0); INSERT INTO alarms (device_id, alarm_name, code) VALUES ('d2', 'CHANNEL_CREATE', 42); INSERT INTO alarms (device_id, alarm_name, code) VALUES ('d3', 'CHANNEL_CREATE', 0); INSERT INTO throughputs (device_id, throughput) VALUES ('d1', 2000.0); INSERT INTO throughputs (device_id, throughput) VALUES ('d2', 900.0); INSERT INTO throughputs (device_id, throughput) VALUES ('d3', 500.0); Full ksqlDB Statements Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. CREATE SOURCE CONNECTOR telemetry WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-iot', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = '<table>', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Create table with latest state of alarms CREATE TABLE alarms ( device_id STRING PRIMARY key, alarm_name STRING, code INT ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='alarms', PARTITIONS = 6); -- Create stream of throughputs CREATE STREAM throughputs ( device_id STRING key, throughput DOUBLE ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='throughputs', PARTITIONS = 6); -- Filter throughputs below threshold 1000.0 CREATE STREAM throughput_insufficient AS SELECT * FROM throughputs WHERE throughput < 1000.0 EMIT CHANGES; -- Create new stream where threshold is insufficient and alarm code is not 0 CREATE STREAM critical_issues_to_investigate AS SELECT t.device_id, t.throughput, a.alarm_name, a.code FROM throughput_insufficient t LEFT JOIN alarms a ON t.device_id = a.device_id WHERE a.code != 0;","title":"Coalesce Telemetry"},{"location":"internet-of-things/coalesce/#coalesce-telemetry","text":"","title":"Coalesce Telemetry"},{"location":"internet-of-things/coalesce/#what-is-it","text":"With Internet of Things (IoT), devices can emit a lot of telemetry, and it may be difficult to analyze that information to determine if something is \"wrong\". This recipe shows you how to process and coalesce that telemetry using ksqlDB and flag devices that warrant more investigation.","title":"What is it?"},{"location":"internet-of-things/coalesce/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"internet-of-things/coalesce/#step-by-step","text":"","title":"Step-by-Step"},{"location":"internet-of-things/coalesce/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks.","title":"Setup your Environment"},{"location":"internet-of-things/coalesce/#read-the-data-in","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). In this example, the telemetry is stored in two tables in a database and is read into 2 Kafak topics in Confluent Cloud. CREATE SOURCE CONNECTOR telemetry WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-iot', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = '<table>', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams.","title":"Read the data in"},{"location":"internet-of-things/coalesce/#run-stream-processing-app","text":"In this example, there is one stream of data reporting device threshold values and another reporting alarms. The following stream processing app identifies which set of devices need to be investigated where threshold is insufficient and alarm code is not zero. -- Create table with latest state of alarms CREATE TABLE alarms ( device_id STRING PRIMARY key, alarm_name STRING, code INT ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='alarms', PARTITIONS = 6); -- Create stream of throughputs CREATE STREAM throughputs ( device_id STRING key, throughput DOUBLE ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='throughputs', PARTITIONS = 6); -- Filter throughputs below threshold 1000.0 CREATE STREAM throughput_insufficient AS SELECT * FROM throughputs WHERE throughput < 1000.0 EMIT CHANGES; -- Create new stream where threshold is insufficient and alarm code is not 0 CREATE STREAM critical_issues_to_investigate AS SELECT t.device_id, t.throughput, a.alarm_name, a.code FROM throughput_insufficient t LEFT JOIN alarms a ON t.device_id = a.device_id WHERE a.code != 0; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO alarms (device_id, alarm_name, code) VALUES ('d1', 'CHANNEL_CREATE', 0); INSERT INTO alarms (device_id, alarm_name, code) VALUES ('d2', 'CHANNEL_CREATE', 42); INSERT INTO alarms (device_id, alarm_name, code) VALUES ('d3', 'CHANNEL_CREATE', 0); INSERT INTO throughputs (device_id, throughput) VALUES ('d1', 2000.0); INSERT INTO throughputs (device_id, throughput) VALUES ('d2', 900.0); INSERT INTO throughputs (device_id, throughput) VALUES ('d3', 500.0);","title":"Run stream processing app"},{"location":"internet-of-things/coalesce/#full-ksqldb-statements","text":"Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. CREATE SOURCE CONNECTOR telemetry WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-iot', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = '<table>', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Create table with latest state of alarms CREATE TABLE alarms ( device_id STRING PRIMARY key, alarm_name STRING, code INT ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='alarms', PARTITIONS = 6); -- Create stream of throughputs CREATE STREAM throughputs ( device_id STRING key, throughput DOUBLE ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='throughputs', PARTITIONS = 6); -- Filter throughputs below threshold 1000.0 CREATE STREAM throughput_insufficient AS SELECT * FROM throughputs WHERE throughput < 1000.0 EMIT CHANGES; -- Create new stream where threshold is insufficient and alarm code is not 0 CREATE STREAM critical_issues_to_investigate AS SELECT t.device_id, t.throughput, a.alarm_name, a.code FROM throughput_insufficient t LEFT JOIN alarms a ON t.device_id = a.device_id WHERE a.code != 0;","title":"Full ksqlDB Statements"},{"location":"operations/salesforce/","text":"Handle Corrupted Data From Salesforce What is it? Salesforce sends a notification when a change to a Salesforce record occurs as part of a create, update, delete, or undelete operation. However, if there is corrupted data in Salesforce, it sends a gap event instead of a change event, which contains information about the change in the header, such as the change type and record ID. These gap events need to be identified and then handled by calling a SFDC API to reconcile the events in real time. TODO--diagram Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks. Read the data in Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). Use Avro so ksqlDB can automatically detect the schema. -- Stream of changes to Salesforce records CREATE SOURCE CONNECTOR sfdc_cdc WITH ( 'connector.class' = 'SalesforceCdcSource', 'name' = 'SalesforceCdcSourceConnector', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'kafka.topic' = 'sfdc.cdc.raw', 'salesforce.username' = '<my-sfdc-username>', 'salesforce.password' = '<my-sfdc-password>', 'salesforce.password.token' = '<sfdc-password-token>', 'salesforce.consumer.key' = '<sfdc-consumer-key>', 'salesforce.consumer.secret' = '<sfdc-consumer-secret>', 'salesforce.cdc.name' = 'AccountChangeEvent', 'output.data.format' = 'AVRO', 'tasks.max' = '1' } Run stream processing app -- Register the stream of SFDC CDC Opportunities CREATE STREAM STREAM_SFDC_CDC_OPPORTUNITY_RAW WITH ( KAFKA_TOPIC='sfdc.cdc.raw', VALUE_FORMAT='AVRO', PARTITIONS=6 ); -- Create a new stream with Replay ID and Change Event Header for just Gap Events CREATE STREAM STREAM_SFDC_CDC_OPPORTUNITY_CHANGE_LOG AS SELECT STREAM_SFDC_CDC_OPPORTUNITY_RAW.REPLAYID AS REPLAYID, STREAM_SFDC_CDC_OPPORTUNITY_RAW.CHANGEEVENTHEADER AS CHANGEEVENTHEADER FROM STREAM_SFDC_CDC_OPPORTUNITY_RAW WHERE (UCASE(STREAM_SFDC_CDC_OPPORTUNITY_RAW.CHANGEEVENTHEADER->CHANGETYPE) LIKE 'GAP%') EMIT CHANGES; Full ksqlDB Statements Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of changes to Salesforce records CREATE SOURCE CONNECTOR sfdc_cdc WITH ( 'connector.class' = 'SalesforceCdcSource', 'name' = 'SalesforceCdcSourceConnector', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'kafka.topic' = 'sfdc.cdc.raw', 'salesforce.username' = '<my-sfdc-username>', 'salesforce.password' = '<my-sfdc-password>', 'salesforce.password.token' = '<sfdc-password-token>', 'salesforce.consumer.key' = '<sfdc-consumer-key>', 'salesforce.consumer.secret' = '<sfdc-consumer-secret>', 'salesforce.cdc.name' = 'AccountChangeEvent', 'output.data.format' = 'AVRO', 'tasks.max' = '1' } -- Register the stream of SFDC CDC Opportunities CREATE STREAM STREAM_SFDC_CDC_OPPORTUNITY_RAW WITH ( KAFKA_TOPIC='sfdc.cdc.raw', VALUE_FORMAT='AVRO', PARTITIONS=6 ); -- Create a new stream with Replay ID and Change Event Header for just Gap Events CREATE STREAM STREAM_SFDC_CDC_OPPORTUNITY_CHANGE_LOG AS SELECT STREAM_SFDC_CDC_OPPORTUNITY_RAW.REPLAYID AS REPLAYID, STREAM_SFDC_CDC_OPPORTUNITY_RAW.CHANGEEVENTHEADER AS CHANGEEVENTHEADER FROM STREAM_SFDC_CDC_OPPORTUNITY_RAW WHERE (UCASE(STREAM_SFDC_CDC_OPPORTUNITY_RAW.CHANGEEVENTHEADER->CHANGETYPE) LIKE 'GAP%') EMIT CHANGES;","title":"Handle Corrupted Data From Salesforce"},{"location":"operations/salesforce/#handle-corrupted-data-from-salesforce","text":"","title":"Handle Corrupted Data From Salesforce"},{"location":"operations/salesforce/#what-is-it","text":"Salesforce sends a notification when a change to a Salesforce record occurs as part of a create, update, delete, or undelete operation. However, if there is corrupted data in Salesforce, it sends a gap event instead of a change event, which contains information about the change in the header, such as the change type and record ID. These gap events need to be identified and then handled by calling a SFDC API to reconcile the events in real time. TODO--diagram","title":"What is it?"},{"location":"operations/salesforce/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"operations/salesforce/#step-by-step","text":"","title":"Step-by-step"},{"location":"operations/salesforce/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks.","title":"Setup your Environment"},{"location":"operations/salesforce/#read-the-data-in","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). Use Avro so ksqlDB can automatically detect the schema. -- Stream of changes to Salesforce records CREATE SOURCE CONNECTOR sfdc_cdc WITH ( 'connector.class' = 'SalesforceCdcSource', 'name' = 'SalesforceCdcSourceConnector', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'kafka.topic' = 'sfdc.cdc.raw', 'salesforce.username' = '<my-sfdc-username>', 'salesforce.password' = '<my-sfdc-password>', 'salesforce.password.token' = '<sfdc-password-token>', 'salesforce.consumer.key' = '<sfdc-consumer-key>', 'salesforce.consumer.secret' = '<sfdc-consumer-secret>', 'salesforce.cdc.name' = 'AccountChangeEvent', 'output.data.format' = 'AVRO', 'tasks.max' = '1' }","title":"Read the data in"},{"location":"operations/salesforce/#run-stream-processing-app","text":"-- Register the stream of SFDC CDC Opportunities CREATE STREAM STREAM_SFDC_CDC_OPPORTUNITY_RAW WITH ( KAFKA_TOPIC='sfdc.cdc.raw', VALUE_FORMAT='AVRO', PARTITIONS=6 ); -- Create a new stream with Replay ID and Change Event Header for just Gap Events CREATE STREAM STREAM_SFDC_CDC_OPPORTUNITY_CHANGE_LOG AS SELECT STREAM_SFDC_CDC_OPPORTUNITY_RAW.REPLAYID AS REPLAYID, STREAM_SFDC_CDC_OPPORTUNITY_RAW.CHANGEEVENTHEADER AS CHANGEEVENTHEADER FROM STREAM_SFDC_CDC_OPPORTUNITY_RAW WHERE (UCASE(STREAM_SFDC_CDC_OPPORTUNITY_RAW.CHANGEEVENTHEADER->CHANGETYPE) LIKE 'GAP%') EMIT CHANGES;","title":"Run stream processing app"},{"location":"operations/salesforce/#full-ksqldb-statements","text":"Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of changes to Salesforce records CREATE SOURCE CONNECTOR sfdc_cdc WITH ( 'connector.class' = 'SalesforceCdcSource', 'name' = 'SalesforceCdcSourceConnector', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'kafka.topic' = 'sfdc.cdc.raw', 'salesforce.username' = '<my-sfdc-username>', 'salesforce.password' = '<my-sfdc-password>', 'salesforce.password.token' = '<sfdc-password-token>', 'salesforce.consumer.key' = '<sfdc-consumer-key>', 'salesforce.consumer.secret' = '<sfdc-consumer-secret>', 'salesforce.cdc.name' = 'AccountChangeEvent', 'output.data.format' = 'AVRO', 'tasks.max' = '1' } -- Register the stream of SFDC CDC Opportunities CREATE STREAM STREAM_SFDC_CDC_OPPORTUNITY_RAW WITH ( KAFKA_TOPIC='sfdc.cdc.raw', VALUE_FORMAT='AVRO', PARTITIONS=6 ); -- Create a new stream with Replay ID and Change Event Header for just Gap Events CREATE STREAM STREAM_SFDC_CDC_OPPORTUNITY_CHANGE_LOG AS SELECT STREAM_SFDC_CDC_OPPORTUNITY_RAW.REPLAYID AS REPLAYID, STREAM_SFDC_CDC_OPPORTUNITY_RAW.CHANGEEVENTHEADER AS CHANGEEVENTHEADER FROM STREAM_SFDC_CDC_OPPORTUNITY_RAW WHERE (UCASE(STREAM_SFDC_CDC_OPPORTUNITY_RAW.CHANGEEVENTHEADER->CHANGETYPE) LIKE 'GAP%') EMIT CHANGES;","title":"Full ksqlDB Statements"},{"location":"retail/dynamic_pricing/","text":"Set Dynamic Pricing What is it? An online marketplace keeps track of statistics regarding pricing: lowest, median, etc. These statistics enable buyers and sellers to make dynamic offers based on historical sales events. Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-Step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks. Read the data in Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). For this recipe, we are interested in knowing each marketplace event for an item, specifically its pricing. This creates a stream of events, upon which real-time stream processing can keep state and calculate pricing statistics. CREATE SOURCE CONNECTOR sales WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-pricing', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'sales', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); CREATE SOURCE CONNECTOR items WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-items', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'items', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams. Run stream processing app -- Create stream of sales CREATE STREAM sales ( item_id INT key, seller_id STRING, price DOUBLE ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='sales', PARTITIONS = 6); -- Create table of items CREATE TABLE items ( item_id INT PRIMARY key, item_name STRING ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='items', PARTITIONS = 6); -- Calculate minimum, maximum, and average price, per item, and join with item name CREATE TABLE sales_stats AS SELECT S.item_id, I.item_name, MIN(price) AS price_min, MAX(price) AS price_max, AVG(price) AS price_avg FROM sales S INNER JOIN items I ON S.item_id = I.item_id GROUP BY S.item_id EMIT CHANGES; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO items (item_id, item_name) VALUES (1, 'Pikachu card'); INSERT INTO items (item_id, item_name) VALUES (2, 'Charizard card'); INSERT INTO items (item_id, item_name) VALUES (3, 'Mew card'); INSERT INTO sales_stream (item_id, price) VALUES (1, 10.00); INSERT INTO sales_stream (item_id, price) VALUES (2, 20.00); INSERT INTO sales_stream (item_id, price) VALUES (3, 30.00); INSERT INTO sales_stream (item_id, price) VALUES (1, 12.00); INSERT INTO sales_stream (item_id, price) VALUES (1, 17.00); INSERT INTO sales_stream (item_id, price) VALUES (3, 26.00); Full ksqlDB Statements Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. CREATE SOURCE CONNECTOR sales WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-pricing', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'sales', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); CREATE SOURCE CONNECTOR items WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-items', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'items', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Create stream of sales CREATE STREAM sales ( item_id INT key, seller_id STRING, price DOUBLE ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='sales', PARTITIONS = 6); -- Create table of items CREATE TABLE items ( item_id INT PRIMARY key, item_name STRING ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='items', PARTITIONS = 6); -- Calculate minimum, maximum, and average price, per item, and join with item name CREATE TABLE sales_stats AS SELECT S.item_id, I.item_name, MIN(price) AS price_min, MAX(price) AS price_max, AVG(price) AS price_avg FROM sales S INNER JOIN items I ON S.item_id = I.item_id GROUP BY S.item_id EMIT CHANGES;","title":"Set Dynamic Pricing"},{"location":"retail/dynamic_pricing/#set-dynamic-pricing","text":"","title":"Set Dynamic Pricing"},{"location":"retail/dynamic_pricing/#what-is-it","text":"An online marketplace keeps track of statistics regarding pricing: lowest, median, etc. These statistics enable buyers and sellers to make dynamic offers based on historical sales events.","title":"What is it?"},{"location":"retail/dynamic_pricing/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"retail/dynamic_pricing/#step-by-step","text":"","title":"Step-by-Step"},{"location":"retail/dynamic_pricing/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks.","title":"Setup your Environment"},{"location":"retail/dynamic_pricing/#read-the-data-in","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). For this recipe, we are interested in knowing each marketplace event for an item, specifically its pricing. This creates a stream of events, upon which real-time stream processing can keep state and calculate pricing statistics. CREATE SOURCE CONNECTOR sales WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-pricing', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'sales', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); CREATE SOURCE CONNECTOR items WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-items', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'items', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams.","title":"Read the data in"},{"location":"retail/dynamic_pricing/#run-stream-processing-app","text":"-- Create stream of sales CREATE STREAM sales ( item_id INT key, seller_id STRING, price DOUBLE ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='sales', PARTITIONS = 6); -- Create table of items CREATE TABLE items ( item_id INT PRIMARY key, item_name STRING ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='items', PARTITIONS = 6); -- Calculate minimum, maximum, and average price, per item, and join with item name CREATE TABLE sales_stats AS SELECT S.item_id, I.item_name, MIN(price) AS price_min, MAX(price) AS price_max, AVG(price) AS price_avg FROM sales S INNER JOIN items I ON S.item_id = I.item_id GROUP BY S.item_id EMIT CHANGES; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO items (item_id, item_name) VALUES (1, 'Pikachu card'); INSERT INTO items (item_id, item_name) VALUES (2, 'Charizard card'); INSERT INTO items (item_id, item_name) VALUES (3, 'Mew card'); INSERT INTO sales_stream (item_id, price) VALUES (1, 10.00); INSERT INTO sales_stream (item_id, price) VALUES (2, 20.00); INSERT INTO sales_stream (item_id, price) VALUES (3, 30.00); INSERT INTO sales_stream (item_id, price) VALUES (1, 12.00); INSERT INTO sales_stream (item_id, price) VALUES (1, 17.00); INSERT INTO sales_stream (item_id, price) VALUES (3, 26.00);","title":"Run stream processing app"},{"location":"retail/dynamic_pricing/#full-ksqldb-statements","text":"Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. CREATE SOURCE CONNECTOR sales WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-pricing', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'sales', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); CREATE SOURCE CONNECTOR items WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-items', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'items', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Create stream of sales CREATE STREAM sales ( item_id INT key, seller_id STRING, price DOUBLE ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='sales', PARTITIONS = 6); -- Create table of items CREATE TABLE items ( item_id INT PRIMARY key, item_name STRING ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='items', PARTITIONS = 6); -- Calculate minimum, maximum, and average price, per item, and join with item name CREATE TABLE sales_stats AS SELECT S.item_id, I.item_name, MIN(price) AS price_min, MAX(price) AS price_max, AVG(price) AS price_avg FROM sales S INNER JOIN items I ON S.item_id = I.item_id GROUP BY S.item_id EMIT CHANGES;","title":"Full ksqlDB Statements"},{"location":"retail/fleet_management/","text":"Build a Real-time Fleet Management System What is it? More and more, fleet management relies on knowing real-time information on vehicles, their locations, and vehicle telemetry. This enables businesses to improve operational efficiency by optimizing travel routes, lowering fuel consumption, and automating service schedules. This recipe enriches fleet location with individual vehicle information, so organizations can have a real-time consolidated view on the entire fleet. TODO--add diagram Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks. Read the data in Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). -- Stream of fleet descriptions CREATE SOURCE CONNECTOR fleet_description WITH ( 'connector.class' = 'MongoDbAtlasSource', 'name' = 'recipe-mongodb-fleet_description', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<database-host-address>', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'database' = '<database-name>', 'collection' = '<database-collection-name>', 'poll.await.time.ms' = '5000', 'poll.max.batch.size' = '1000', 'copy.existing' = 'true', 'output.data.format' = 'JSON' 'tasks.max' = '1'); -- Stream of current location of each vehicle in the fleet CREATE SOURCE CONNECTOR fleet_location WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-fleet_location', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'fleet_location', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams. Run stream processing app Now you can process the data in a variety of ways. In this case, the original fleet telemetry is enriched with details about the vehicle. -- create stream of locations CREATE STREAM locations ( vehicle_id int, latitude double, longitude double, timestamp varchar ) WITH ( kafka_topic = 'locations', value_format = 'json', partitions = 6 ); -- fleet lookup table CREATE TABLE fleet ( vehicle_id int primary key, driver_id int, license bigint ) WITH ( kafka_topic = 'descriptions', value_format = 'json', partitions = 6 ); -- enrich fleet location stream with more fleet information CREATE STREAM fleet_location_enhanced AS SELECT l.vehicle_id, latitude, longitude, timestamp, f.driver_id, f.license FROM locations l LEFT JOIN fleet f ON l.vehicle_id = f.vehicle_id; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO locations (vehicle_id, latitude, longitude, timestamp) VALUES (5401, 16.7587537, 96.2482149, '2021-09-23T10:50:00.000Z'); INSERT INTO locations (vehicle_id, latitude, longitude, timestamp) VALUES (5401, 16.004175, 120.7806412, '2021-09-27T06:39:00.000Z'); INSERT INTO locations (vehicle_id, latitude, longitude, timestamp) VALUES (5402, 32.755613, 22.6377432, '2021-09-25T20:22:00.000Z'); INSERT INTO fleet (vehicle_id, driver_id, license) VALUES (5401, 847383, 8852693196); INSERT INTO fleet (vehicle_id, driver_id, license) VALUES (5402, 922947, 1255144201); INSERT INTO fleet (vehicle_id, driver_id, license) VALUES (5403, 435309, 2132311746); Full ksqlDB Statements Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of fleet descriptions CREATE SOURCE CONNECTOR fleet_description WITH ( 'connector.class' = 'MongoDbAtlasSource', 'name' = 'recipe-mongodb-fleet_description', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<database-host-address>', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'database' = '<database-name>', 'collection' = '<database-collection-name>', 'poll.await.time.ms' = '5000', 'poll.max.batch.size' = '1000', 'copy.existing' = 'true', 'output.data.format' = 'JSON' 'tasks.max' = '1'); -- Stream of current location of each vehicle in the fleet CREATE SOURCE CONNECTOR fleet_location WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-fleet_location', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'fleet_location', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- create stream of locations CREATE STREAM locations ( vehicle_id int, latitude double, longitude double, timestamp varchar ) WITH ( kafka_topic = 'locations', value_format = 'json', partitions = 6 ); -- fleet lookup table CREATE TABLE fleet ( vehicle_id int primary key, driver_id int, license bigint ) WITH ( kafka_topic = 'descriptions', value_format = 'json', partitions = 6 ); -- enrich fleet location stream with more fleet information CREATE STREAM fleet_location_enhanced AS SELECT l.vehicle_id, latitude, longitude, timestamp, f.driver_id, f.license FROM locations l LEFT JOIN fleet f ON l.vehicle_id = f.vehicle_id;","title":"Build a Real-time Fleet Management System"},{"location":"retail/fleet_management/#build-a-real-time-fleet-management-system","text":"","title":"Build a Real-time Fleet Management System"},{"location":"retail/fleet_management/#what-is-it","text":"More and more, fleet management relies on knowing real-time information on vehicles, their locations, and vehicle telemetry. This enables businesses to improve operational efficiency by optimizing travel routes, lowering fuel consumption, and automating service schedules. This recipe enriches fleet location with individual vehicle information, so organizations can have a real-time consolidated view on the entire fleet. TODO--add diagram","title":"What is it?"},{"location":"retail/fleet_management/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"retail/fleet_management/#step-by-step","text":"","title":"Step-by-step"},{"location":"retail/fleet_management/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks.","title":"Setup your Environment"},{"location":"retail/fleet_management/#read-the-data-in","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). -- Stream of fleet descriptions CREATE SOURCE CONNECTOR fleet_description WITH ( 'connector.class' = 'MongoDbAtlasSource', 'name' = 'recipe-mongodb-fleet_description', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<database-host-address>', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'database' = '<database-name>', 'collection' = '<database-collection-name>', 'poll.await.time.ms' = '5000', 'poll.max.batch.size' = '1000', 'copy.existing' = 'true', 'output.data.format' = 'JSON' 'tasks.max' = '1'); -- Stream of current location of each vehicle in the fleet CREATE SOURCE CONNECTOR fleet_location WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-fleet_location', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'fleet_location', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams.","title":"Read the data in"},{"location":"retail/fleet_management/#run-stream-processing-app","text":"Now you can process the data in a variety of ways. In this case, the original fleet telemetry is enriched with details about the vehicle. -- create stream of locations CREATE STREAM locations ( vehicle_id int, latitude double, longitude double, timestamp varchar ) WITH ( kafka_topic = 'locations', value_format = 'json', partitions = 6 ); -- fleet lookup table CREATE TABLE fleet ( vehicle_id int primary key, driver_id int, license bigint ) WITH ( kafka_topic = 'descriptions', value_format = 'json', partitions = 6 ); -- enrich fleet location stream with more fleet information CREATE STREAM fleet_location_enhanced AS SELECT l.vehicle_id, latitude, longitude, timestamp, f.driver_id, f.license FROM locations l LEFT JOIN fleet f ON l.vehicle_id = f.vehicle_id; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO locations (vehicle_id, latitude, longitude, timestamp) VALUES (5401, 16.7587537, 96.2482149, '2021-09-23T10:50:00.000Z'); INSERT INTO locations (vehicle_id, latitude, longitude, timestamp) VALUES (5401, 16.004175, 120.7806412, '2021-09-27T06:39:00.000Z'); INSERT INTO locations (vehicle_id, latitude, longitude, timestamp) VALUES (5402, 32.755613, 22.6377432, '2021-09-25T20:22:00.000Z'); INSERT INTO fleet (vehicle_id, driver_id, license) VALUES (5401, 847383, 8852693196); INSERT INTO fleet (vehicle_id, driver_id, license) VALUES (5402, 922947, 1255144201); INSERT INTO fleet (vehicle_id, driver_id, license) VALUES (5403, 435309, 2132311746);","title":"Run stream processing app"},{"location":"retail/fleet_management/#full-ksqldb-statements","text":"Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. -- Stream of fleet descriptions CREATE SOURCE CONNECTOR fleet_description WITH ( 'connector.class' = 'MongoDbAtlasSource', 'name' = 'recipe-mongodb-fleet_description', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<database-host-address>', 'connection.user' = '<database-username>', 'connection.password' = '<database-password>', 'database' = '<database-name>', 'collection' = '<database-collection-name>', 'poll.await.time.ms' = '5000', 'poll.max.batch.size' = '1000', 'copy.existing' = 'true', 'output.data.format' = 'JSON' 'tasks.max' = '1'); -- Stream of current location of each vehicle in the fleet CREATE SOURCE CONNECTOR fleet_location WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-fleet_location', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = 'fleet_location', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- create stream of locations CREATE STREAM locations ( vehicle_id int, latitude double, longitude double, timestamp varchar ) WITH ( kafka_topic = 'locations', value_format = 'json', partitions = 6 ); -- fleet lookup table CREATE TABLE fleet ( vehicle_id int primary key, driver_id int, license bigint ) WITH ( kafka_topic = 'descriptions', value_format = 'json', partitions = 6 ); -- enrich fleet location stream with more fleet information CREATE STREAM fleet_location_enhanced AS SELECT l.vehicle_id, latitude, longitude, timestamp, f.driver_id, f.license FROM locations l LEFT JOIN fleet f ON l.vehicle_id = f.vehicle_id;","title":"Full ksqlDB Statements"},{"location":"retail/inventory/","text":"View Real-time Inventory What is it? Having an up-to-date view of inventory on every item is essential in today's online marketplaces. This helps businesses maintain the optimum level of inventory\u2014not too much and not too little\u2014so that they can meet demand while minimizing costs. This recipe demonstrates how to see your updated inventory in real-time, so you always have an up-to-date snapshot of your stock. Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-Step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks. Read the data in Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). For this recipe, we are interested in knowing each event for an item that affects its quantity. This creates a stream of events, where each event results in the addition or removal of inventory. CREATE SOURCE CONNECTOR inventory WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-inventory', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = '<table>', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams. Run stream processing app Create a ksqlDB TABLE , which is a mutable, partitioned collection that models change over time that represents what is true as of \"now\". -- Create stream of inventory CREATE STREAM inventory_stream ( id STRING key, item STRING, quantity INTEGER ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='inventory', PARTITIONS = 6); -- Create stateful table with up-to-date information of inventory availability CREATE TABLE inventory_stream_table WITH (kafka_topic='inventory_table') AS SELECT item, SUM(quantity) AS item_quantity FROM inventory_stream GROUP BY item EMIT CHANGES; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO inventory_stream (id, item, quantity) VALUES ('1', 'Apple Magic Mouse 2', 10); INSERT INTO inventory_stream (id, item, quantity) VALUES ('2', 'iPhoneX', 25); INSERT INTO inventory_stream (id, item, quantity) VALUES ('3', 'MacBookPro13', 100); INSERT INTO inventory_stream (id, item, quantity) VALUES ('4', 'iPad4', 20); INSERT INTO inventory_stream (id, item, quantity) VALUES ('5', 'Apple Pencil', 10); INSERT INTO inventory_stream (id, item, quantity) VALUES ('5', 'PhoneX', 10); INSERT INTO inventory_stream (id, item, quantity) VALUES ('4', 'iPad4', -20); INSERT INTO inventory_stream (id, item, quantity) VALUES ('3', 'MacBookPro13', 10); INSERT INTO inventory_stream (id, item, quantity) VALUES ('4', 'iPad4', 20); Full ksqlDB Statements Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. CREATE SOURCE CONNECTOR inventory WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-inventory', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = '<table>', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Create stream of inventory CREATE STREAM inventory_stream ( id STRING key, item STRING, quantity INTEGER ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='inventory', PARTITIONS = 6); -- Create stateful table with up-to-date information of inventory availability CREATE TABLE inventory_stream_table WITH (kafka_topic='inventory_table') AS SELECT item, SUM(quantity) AS item_quantity FROM inventory_stream GROUP BY item EMIT CHANGES;","title":"View Real-time Inventory"},{"location":"retail/inventory/#view-real-time-inventory","text":"","title":"View Real-time Inventory"},{"location":"retail/inventory/#what-is-it","text":"Having an up-to-date view of inventory on every item is essential in today's online marketplaces. This helps businesses maintain the optimum level of inventory\u2014not too much and not too little\u2014so that they can meet demand while minimizing costs. This recipe demonstrates how to see your updated inventory in real-time, so you always have an up-to-date snapshot of your stock.","title":"What is it?"},{"location":"retail/inventory/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"retail/inventory/#step-by-step","text":"","title":"Step-by-Step"},{"location":"retail/inventory/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks.","title":"Setup your Environment"},{"location":"retail/inventory/#read-the-data-in","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ). For this recipe, we are interested in knowing each event for an item that affects its quantity. This creates a stream of events, where each event results in the addition or removal of inventory. CREATE SOURCE CONNECTOR inventory WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-inventory', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = '<table>', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); Optional: If you do not have a real data source to connect to with properly formatted data, or you just want to execute this recipe without external dependencies, no worries! In the next section, we'll show you how to insert values into the streams.","title":"Read the data in"},{"location":"retail/inventory/#run-stream-processing-app","text":"Create a ksqlDB TABLE , which is a mutable, partitioned collection that models change over time that represents what is true as of \"now\". -- Create stream of inventory CREATE STREAM inventory_stream ( id STRING key, item STRING, quantity INTEGER ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='inventory', PARTITIONS = 6); -- Create stateful table with up-to-date information of inventory availability CREATE TABLE inventory_stream_table WITH (kafka_topic='inventory_table') AS SELECT item, SUM(quantity) AS item_quantity FROM inventory_stream GROUP BY item EMIT CHANGES; In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements: INSERT INTO inventory_stream (id, item, quantity) VALUES ('1', 'Apple Magic Mouse 2', 10); INSERT INTO inventory_stream (id, item, quantity) VALUES ('2', 'iPhoneX', 25); INSERT INTO inventory_stream (id, item, quantity) VALUES ('3', 'MacBookPro13', 100); INSERT INTO inventory_stream (id, item, quantity) VALUES ('4', 'iPad4', 20); INSERT INTO inventory_stream (id, item, quantity) VALUES ('5', 'Apple Pencil', 10); INSERT INTO inventory_stream (id, item, quantity) VALUES ('5', 'PhoneX', 10); INSERT INTO inventory_stream (id, item, quantity) VALUES ('4', 'iPad4', -20); INSERT INTO inventory_stream (id, item, quantity) VALUES ('3', 'MacBookPro13', 10); INSERT INTO inventory_stream (id, item, quantity) VALUES ('4', 'iPad4', 20);","title":"Run stream processing app"},{"location":"retail/inventory/#full-ksqldb-statements","text":"Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. CREATE SOURCE CONNECTOR inventory WITH ( 'connector.class' = 'PostgresSource', 'name' = 'recipe-postgres-inventory', 'kafka.api.key' = '<my-kafka-api-key>', 'kafka.api.secret' = '<my-kafka-api-secret>', 'connection.host' = '<my-database-endpoint>', 'connection.port' = '5432', 'connection.user' = 'postgres', 'connection.password' = '<my-database-password>', 'db.name' = '<db-name>', 'table.whitelist' = '<table>', 'timestamp.column.name' = 'created_at', 'output.data.format' = 'JSON', 'db.timezone' = 'UTC', 'tasks.max' = '1'); -- Create stream of inventory CREATE STREAM inventory_stream ( id STRING key, item STRING, quantity INTEGER ) WITH ( VALUE_FORMAT='json', KAFKA_TOPIC='inventory', PARTITIONS = 6); -- Create stateful table with up-to-date information of inventory availability CREATE TABLE inventory_stream_table WITH (kafka_topic='inventory_table') AS SELECT item, SUM(quantity) AS item_quantity FROM inventory_stream GROUP BY item EMIT CHANGES;","title":"Full ksqlDB Statements"},{"location":"security/ssh-attack/","text":"Detect and Analyze SSH Attacks What is it? There are lots of ways SSH can be abused but one of the most straightforward ways to detect a problem is to monitor for rejected logins. This recipe processes Syslog data to detect bad logins and streams out those pairs of usernames and IP addresses. With ksqlDB, you can filter and react to events in real time rather than performing historical analysis of Syslog data from cold storage. Get Started Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow). Step-by-Step Setup your Environment Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks. Read the data in This recipe is a great demonstration on how to run a self-managed connector, to push syslog data into Confluent Cloud into a Kafka topic called syslog . Create a file called Dockerfile to bundle a connect worker with kafka-connect-syslog : FROM confluentinc/cp-server-connect-base:6.2.0 ENV CONNECT_PLUGIN_PATH=\"/usr/share/java,/usr/share/confluent-hub-components\" USER root COPY --chown=appuser:appuser include/etc/confluent/docker /etc/confluent/docker USER appuser ARG CONNECTOR_OWNER=confluentinc ARG CONNECTOR_NAME ARG CONNECTOR_VERSION RUN confluent-hub install --no-prompt confluent-hub install confluentinc/kafka-connect-syslog:1.3.4 CMD [\"/etc/confluent/docker/run\"] Build the custom Docker image with the command: docker build \\ -t localbuild/connect_distributed_with_syslog:1.3.4 \\ -f Dockerfile . Create a file called docker-compose.yml with the following content, substituting your Confluent Cloud connection information: --- version: '2' services: connect: image: localbuild/connect_distributed_with_syslog:1.3.4 hostname: connect container_name: connect ports: - \"8083:8083\" environment: CONNECT_BOOTSTRAP_SERVERS: $BOOTSTRAP_SERVERS CONNECT_REST_PORT: 8083 CONNECT_GROUP_ID: \"connect\" CONNECT_CONFIG_STORAGE_TOPIC: recipe-connect-configs CONNECT_OFFSET_STORAGE_TOPIC: recipe-connect-offsets CONNECT_STATUS_STORAGE_TOPIC: recipe-connect-status CONNECT_REPLICATION_FACTOR: 3 CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3 CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3 CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3 CONNECT_KEY_CONVERTER: \"org.apache.kafka.connect.storage.StringConverter\" CONNECT_VALUE_CONVERTER: \"org.apache.kafka.connect.json.JsonConverter\" CONNECT_REST_ADVERTISED_HOST_NAME: \"connect\" CONNECT_PLUGIN_PATH: \"/usr/share/java,/usr/share/confluent-hub-components\" CONNECT_LOG4J_ROOT_LOGLEVEL: INFO CONNECT_LOG4J_LOGGERS: org.reflections=ERROR # Connect worker CONNECT_SECURITY_PROTOCOL: SASL_SSL CONNECT_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_SASL_MECHANISM: PLAIN # Connect producer CONNECT_PRODUCER_SECURITY_PROTOCOL: SASL_SSL CONNECT_PRODUCER_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_PRODUCER_SASL_MECHANISM: PLAIN # Connect consumer CONNECT_CONSUMER_SECURITY_PROTOCOL: SASL_SSL CONNECT_CONSUMER_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_CONSUMER_SASL_MECHANISM: PLAIN Run the container with: docker-compose up -d Now you should have Syslog messages being written to the topic syslog in Confluent Cloud. Run stream processing app Process the syslog events by flagging events with invalid users, stripping out all the other unnecessary fields, and creating just a stream of relevant information. There are many ways to customize the resulting stream to fit the business needs: this example also demonstrates how to enrich the stream with a new field FACILITY_DESCRIPTION with human-readable content. -- Define Kafka parameters DEFINE topic = 'syslog'; -- Extract relevant fields from log messages CREATE OR REPLACE STREAM `syslog` WITH ( KAFKA_TOPIC = '${topic}', VALUE_FORMAT = 'json', PARTITIONS = 6 ); CREATE STREAM `by_facility` AS SELECT TIMESTAMPTOSTRING(TIMESTAMP, 'yyyy-MM-dd HH:mm:ss') AS SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS FROM `syslog` PARTITION BY FACILITY EMIT CHANGES; -- Flag events with invalid users, and enrich with a new field 'FACILITY_DESCRIPTION' CREATE STREAM `invalid_users` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS, CASE WHEN FACILITY = 0 THEN 'kernel messages' WHEN FACILITY = 1 THEN 'user-level messages' WHEN FACILITY = 2 THEN 'mail system' WHEN FACILITY = 3 THEN 'system daemons' WHEN FACILITY = 4 THEN 'security/authorization messages' WHEN FACILITY = 5 THEN 'messages generated internally by syslogd' WHEN FACILITY = 6 THEN 'line printer subsystem' ELSE '<unknown>' END AS FACILITY_DESCRIPTION FROM `by_facility` WHERE MESSAGE LIKE 'Invalid user%' EMIT CHANGES; -- Create actionable stream of SSH attacks, enriched with user and IP CREATE STREAM `ssh_attacks` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY_DESCRIPTION, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[1] AS ATTACK_USER, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[2] AS ATTACK_IP FROM `invalid_users` EMIT CHANGES; Full ksqlDB Statements Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. Run the Syslog source connector locally, then proceed with ksqlDB to process the Syslog messages. -- Define Kafka parameters DEFINE topic = 'syslog'; -- Extract relevant fields from log messages CREATE OR REPLACE STREAM `syslog` WITH ( KAFKA_TOPIC = '${topic}', VALUE_FORMAT = 'json', PARTITIONS = 6 ); CREATE STREAM `by_facility` AS SELECT TIMESTAMPTOSTRING(TIMESTAMP, 'yyyy-MM-dd HH:mm:ss') AS SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS FROM `syslog` PARTITION BY FACILITY EMIT CHANGES; -- Flag events with invalid users, and enrich with a new field 'FACILITY_DESCRIPTION' CREATE STREAM `invalid_users` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS, CASE WHEN FACILITY = 0 THEN 'kernel messages' WHEN FACILITY = 1 THEN 'user-level messages' WHEN FACILITY = 2 THEN 'mail system' WHEN FACILITY = 3 THEN 'system daemons' WHEN FACILITY = 4 THEN 'security/authorization messages' WHEN FACILITY = 5 THEN 'messages generated internally by syslogd' WHEN FACILITY = 6 THEN 'line printer subsystem' ELSE '<unknown>' END AS FACILITY_DESCRIPTION FROM `by_facility` WHERE MESSAGE LIKE 'Invalid user%' EMIT CHANGES; -- Create actionable stream of SSH attacks, enriched with user and IP CREATE STREAM `ssh_attacks` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY_DESCRIPTION, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[1] AS ATTACK_USER, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[2] AS ATTACK_IP FROM `invalid_users` EMIT CHANGES;","title":"Detect and Analyze SSH Attacks"},{"location":"security/ssh-attack/#detect-and-analyze-ssh-attacks","text":"","title":"Detect and Analyze SSH Attacks"},{"location":"security/ssh-attack/#what-is-it","text":"There are lots of ways SSH can be abused but one of the most straightforward ways to detect a problem is to monitor for rejected logins. This recipe processes Syslog data to detect bad logins and streams out those pairs of usernames and IP addresses. With ksqlDB, you can filter and react to events in real time rather than performing historical analysis of Syslog data from cold storage.","title":"What is it?"},{"location":"security/ssh-attack/#get-started","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Get Started"},{"location":"security/ssh-attack/#step-by-step","text":"","title":"Step-by-Step"},{"location":"security/ssh-attack/#setup-your-environment","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks.","title":"Setup your Environment"},{"location":"security/ssh-attack/#read-the-data-in","text":"This recipe is a great demonstration on how to run a self-managed connector, to push syslog data into Confluent Cloud into a Kafka topic called syslog . Create a file called Dockerfile to bundle a connect worker with kafka-connect-syslog : FROM confluentinc/cp-server-connect-base:6.2.0 ENV CONNECT_PLUGIN_PATH=\"/usr/share/java,/usr/share/confluent-hub-components\" USER root COPY --chown=appuser:appuser include/etc/confluent/docker /etc/confluent/docker USER appuser ARG CONNECTOR_OWNER=confluentinc ARG CONNECTOR_NAME ARG CONNECTOR_VERSION RUN confluent-hub install --no-prompt confluent-hub install confluentinc/kafka-connect-syslog:1.3.4 CMD [\"/etc/confluent/docker/run\"] Build the custom Docker image with the command: docker build \\ -t localbuild/connect_distributed_with_syslog:1.3.4 \\ -f Dockerfile . Create a file called docker-compose.yml with the following content, substituting your Confluent Cloud connection information: --- version: '2' services: connect: image: localbuild/connect_distributed_with_syslog:1.3.4 hostname: connect container_name: connect ports: - \"8083:8083\" environment: CONNECT_BOOTSTRAP_SERVERS: $BOOTSTRAP_SERVERS CONNECT_REST_PORT: 8083 CONNECT_GROUP_ID: \"connect\" CONNECT_CONFIG_STORAGE_TOPIC: recipe-connect-configs CONNECT_OFFSET_STORAGE_TOPIC: recipe-connect-offsets CONNECT_STATUS_STORAGE_TOPIC: recipe-connect-status CONNECT_REPLICATION_FACTOR: 3 CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3 CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3 CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3 CONNECT_KEY_CONVERTER: \"org.apache.kafka.connect.storage.StringConverter\" CONNECT_VALUE_CONVERTER: \"org.apache.kafka.connect.json.JsonConverter\" CONNECT_REST_ADVERTISED_HOST_NAME: \"connect\" CONNECT_PLUGIN_PATH: \"/usr/share/java,/usr/share/confluent-hub-components\" CONNECT_LOG4J_ROOT_LOGLEVEL: INFO CONNECT_LOG4J_LOGGERS: org.reflections=ERROR # Connect worker CONNECT_SECURITY_PROTOCOL: SASL_SSL CONNECT_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_SASL_MECHANISM: PLAIN # Connect producer CONNECT_PRODUCER_SECURITY_PROTOCOL: SASL_SSL CONNECT_PRODUCER_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_PRODUCER_SASL_MECHANISM: PLAIN # Connect consumer CONNECT_CONSUMER_SECURITY_PROTOCOL: SASL_SSL CONNECT_CONSUMER_SASL_JAAS_CONFIG: $SASL_JAAS_CONFIG CONNECT_CONSUMER_SASL_MECHANISM: PLAIN Run the container with: docker-compose up -d Now you should have Syslog messages being written to the topic syslog in Confluent Cloud.","title":"Read the data in"},{"location":"security/ssh-attack/#run-stream-processing-app","text":"Process the syslog events by flagging events with invalid users, stripping out all the other unnecessary fields, and creating just a stream of relevant information. There are many ways to customize the resulting stream to fit the business needs: this example also demonstrates how to enrich the stream with a new field FACILITY_DESCRIPTION with human-readable content. -- Define Kafka parameters DEFINE topic = 'syslog'; -- Extract relevant fields from log messages CREATE OR REPLACE STREAM `syslog` WITH ( KAFKA_TOPIC = '${topic}', VALUE_FORMAT = 'json', PARTITIONS = 6 ); CREATE STREAM `by_facility` AS SELECT TIMESTAMPTOSTRING(TIMESTAMP, 'yyyy-MM-dd HH:mm:ss') AS SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS FROM `syslog` PARTITION BY FACILITY EMIT CHANGES; -- Flag events with invalid users, and enrich with a new field 'FACILITY_DESCRIPTION' CREATE STREAM `invalid_users` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS, CASE WHEN FACILITY = 0 THEN 'kernel messages' WHEN FACILITY = 1 THEN 'user-level messages' WHEN FACILITY = 2 THEN 'mail system' WHEN FACILITY = 3 THEN 'system daemons' WHEN FACILITY = 4 THEN 'security/authorization messages' WHEN FACILITY = 5 THEN 'messages generated internally by syslogd' WHEN FACILITY = 6 THEN 'line printer subsystem' ELSE '<unknown>' END AS FACILITY_DESCRIPTION FROM `by_facility` WHERE MESSAGE LIKE 'Invalid user%' EMIT CHANGES; -- Create actionable stream of SSH attacks, enriched with user and IP CREATE STREAM `ssh_attacks` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY_DESCRIPTION, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[1] AS ATTACK_USER, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[2] AS ATTACK_IP FROM `invalid_users` EMIT CHANGES;","title":"Run stream processing app"},{"location":"security/ssh-attack/#full-ksqldb-statements","text":"Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above. Run the Syslog source connector locally, then proceed with ksqlDB to process the Syslog messages. -- Define Kafka parameters DEFINE topic = 'syslog'; -- Extract relevant fields from log messages CREATE OR REPLACE STREAM `syslog` WITH ( KAFKA_TOPIC = '${topic}', VALUE_FORMAT = 'json', PARTITIONS = 6 ); CREATE STREAM `by_facility` AS SELECT TIMESTAMPTOSTRING(TIMESTAMP, 'yyyy-MM-dd HH:mm:ss') AS SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS FROM `syslog` PARTITION BY FACILITY EMIT CHANGES; -- Flag events with invalid users, and enrich with a new field 'FACILITY_DESCRIPTION' CREATE STREAM `invalid_users` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY, MESSAGE, REMOTEADDRESS, CASE WHEN FACILITY = 0 THEN 'kernel messages' WHEN FACILITY = 1 THEN 'user-level messages' WHEN FACILITY = 2 THEN 'mail system' WHEN FACILITY = 3 THEN 'system daemons' WHEN FACILITY = 4 THEN 'security/authorization messages' WHEN FACILITY = 5 THEN 'messages generated internally by syslogd' WHEN FACILITY = 6 THEN 'line printer subsystem' ELSE '<unknown>' END AS FACILITY_DESCRIPTION FROM `by_facility` WHERE MESSAGE LIKE 'Invalid user%' EMIT CHANGES; -- Create actionable stream of SSH attacks, enriched with user and IP CREATE STREAM `ssh_attacks` AS SELECT SYSLOG_TIMESTAMP, HOST, FACILITY_DESCRIPTION, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[1] AS ATTACK_USER, SPLIT(REPLACE(MESSAGE,'Invalid user ',''),' from ')[2] AS ATTACK_IP FROM `invalid_users` EMIT CHANGES;","title":"Full ksqlDB Statements"},{"location":"shared/ccloud_launch/","text":"Click below to automatically launch this recipe in Confluent Cloud (alternatively: follow the step-by-step instructions that follow).","title":"Ccloud launch"},{"location":"shared/ccloud_setup/","text":"Setup your environment in Confluent Cloud , a fully-managed Apache Kafka service. Then proceed with the recipe, which can be executed with SQL syntax using Confluent Cloud ksqlDB, which has language for processing the data in real-time and will soon support connector integration for reading and writing data to other data sources and sinks.","title":"Ccloud setup"},{"location":"shared/code_summary/","text":"Here is the full recipe that you can run with ksqlDB, a consolidation of all the the steps shown above.","title":"Code summary"},{"location":"shared/connect/","text":"Confluent Cloud offers pre-built, fully managed connectors that make it easy to instantly connect to popular data sources and end systems in the cloud. This recipe is shown with one example of a data source, but you can substitute your own preferred connector to use any data source. The principles are the same, just modify the connector configuration shown below to fit your deployment (see documentation ).","title":"Connect"},{"location":"shared/manual_cue/","text":"In the previous section Read the data in , if you did not have a real data source to connect to, you can now use ksqlDB to insert mock data into source topics with the following statements:","title":"Manual cue"},{"location":"shared/manual_insert/","text":"","title":"Manual insert"}]}